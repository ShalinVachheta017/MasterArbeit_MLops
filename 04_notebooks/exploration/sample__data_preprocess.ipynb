{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb695d3",
   "metadata": {},
   "source": [
    "### I — Ingest\n",
    "\n",
    "##### Goal \n",
    "\n",
    "Load raw accel & gyro files, keep everything as UTC time, and prepare to expand batched samples (Garmin usually stores a base timestamp_ms plus list columns for offsets and x/y/z values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "CONFIG = {\n",
    "    \"accel_path\": \"2025-03-23-15-23-10-accelerometer_data.xlsx\",\n",
    "    \"gyro_path\":  \"2025-03-23-15-23-10-gyroscope_data.xlsx\",\n",
    "    \"target_hz\":  50,         # target sampling rate\n",
    "    \"merge_tolerance_ms\": 1,  # max allowed accel↔gyro timestamp gap for alignment\n",
    "    \"tz\": \"UTC\",              # keep everything in UTC\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34b927",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370da84",
   "metadata": {},
   "source": [
    "## 2) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18332f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_dt\n",
    "from datetime import timezone, datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131d0a1",
   "metadata": {},
   "source": [
    "## 3) Load raw Excel files (no parsing yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec42798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accel shape: (14536, 6)\n",
      "Gyro  shape: (14536, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                 timestamp  timestamp_ms  \\\n",
       " 0  03/24/2025, 07:52:19 AM           766   \n",
       " 1  03/24/2025, 07:52:20 AM            23   \n",
       " 2  03/24/2025, 07:52:20 AM           271   \n",
       " \n",
       "                                   sample_time_offset  \\\n",
       " 0  [\"0\",\"10\",\"29\",\"38\",\"48\",\"57\",\"66\",\"77\",\"85\",\"...   \n",
       " 1  [\"0\",\"9\",\"19\",\"28\",\"38\",\"48\",\"57\",\"67\",\"77\",\"8...   \n",
       " 2  [\"0\",\"10\",\"19\",\"29\",\"38\",\"47\",\"58\",\"66\",\"76\",\"...   \n",
       " \n",
       "                                   calibrated_accel_x  \\\n",
       " 0  [\"-62.48186\",\"-27.60828\",\"-95.41802\",\"-155.478...   \n",
       " 1  [\"-178.7271\",\"-161.2903\",\"-133.1977\",\"-124.479...   \n",
       " 2  [\"-81.85607\",\"-92.51189\",\"-101.2303\",\"-112.854...   \n",
       " \n",
       "                                   calibrated_accel_y  \\\n",
       " 0  [\"-758.6971\",\"-984.1958\",\"-1240.532\",\"-1338.82...   \n",
       " 1  [\"-1360.991\",\"-1342.681\",\"-1269.442\",\"-1171.14...   \n",
       " 2  [\"-1140.310\",\"-1056.471\",\"-967.8134\",\"-805.917...   \n",
       " \n",
       "                                   calibrated_accel_z  \n",
       " 0  [\"-1097.585\",\"-1221.784\",\"-1086.742\",\"-957.614...  \n",
       " 1  [\"-777.2302\",\"-619.5170\",\"-460.8181\",\"-345.490...  \n",
       " 2  [\"-254.8053\",\"-147.3632\",\"-53.72105\",\"-31.0497...  ,\n",
       "                  timestamp  timestamp_ms  \\\n",
       " 0  03/24/2025, 07:52:19 AM           795   \n",
       " 1  03/24/2025, 07:52:20 AM            52   \n",
       " 2  03/24/2025, 07:52:20 AM           300   \n",
       " \n",
       "                                   sample_time_offset  \\\n",
       " 0  [\"0\",\"9\",\"19\",\"38\",\"48\",\"57\",\"67\",\"76\",\"85\",\"9...   \n",
       " 1  [\"0\",\"9\",\"19\",\"28\",\"38\",\"48\",\"57\",\"67\",\"76\",\"8...   \n",
       " 2  [\"0\",\"9\",\"19\",\"29\",\"38\",\"48\",\"57\",\"66\",\"76\",\"8...   \n",
       " \n",
       "                                    calibrated_gyro_x  \\\n",
       " 0  [\"39.15614\",\"32.96125\",\"22.04125\",\"1.041086\",\"...   \n",
       " 1  [\"-24.26378\",\"-35.91878\",\"-44.66883\",\"-50.3738...   \n",
       " 2  [\"-49.46386\",\"-51.45886\",\"-49.60395\",\"-43.0239...   \n",
       " \n",
       "                                    calibrated_gyro_y  \\\n",
       " 0  [\"-7.520049\",\"-10.77502\",\"-9.620019\",\"-4.79006...   \n",
       " 1  [\"-12.10503\",\"-11.82503\",\"-11.30004\",\"-9.90004...   \n",
       " 2  [\"-9.515048\",\"-7.205049\",\"-4.720075\",\"-2.02507...   \n",
       " \n",
       "                                    calibrated_gyro_z  \n",
       " 0  [\"7.881836\",\"7.742001\",\"12.78200\",\"2.981754\",\"...  \n",
       " 1  [\"-0.3430405\",\"-1.078040\",\"-1.533123\",\"-2.1631...  \n",
       " 2  [\"-1.743164\",\"-2.723164\",\"-3.773308\",\"-4.08830...  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel_raw = pd.read_excel(\"2025-03-23-15-23-10-accelerometer_data.xlsx\")\n",
    "gyro_raw  = pd.read_excel(\"2025-03-23-15-23-10-gyroscope_data.xlsx\")\n",
    "\n",
    "print(\"Accel shape:\", accel_raw.shape)\n",
    "print(\"Gyro  shape:\", gyro_raw.shape)\n",
    "accel_raw.head(3), gyro_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835b021",
   "metadata": {},
   "source": [
    "## P — Process\n",
    "#### Goal \n",
    "\n",
    "Turn batched rows into one row per sample, compute absolute timestamps for each sub‑sample, align accel & gyro, resample to exactly 50 Hz, and standardize column names/units.\n",
    "\n",
    "##### We’ll assume typical Garmin columns:\n",
    "\n",
    "timestamp_ms (int, base time per row),\n",
    "\n",
    "sample_time_offset (list[int] ms),\n",
    "\n",
    "x, y, z (each list[float] per row).\n",
    "If your column names differ, update the mapping once in the code block below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f232ec",
   "metadata": {},
   "source": [
    "### 4) Helpers for exploding batched rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6694a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that converts each batched row into per‑sample rows with absolute timestamps\n",
    "def explode_sensor(df, ts_col=\"timestamp_ms\", off_col=\"sample_time_offset\",\n",
    "                   x_col=\"x\", y_col=\"y\", z_col=\"z\", prefix=\"A\"):\n",
    "    \"\"\"\n",
    "    df: raw sensor table with base timestamp + lists of offsets and x/y/z.\n",
    "    prefix: 'A' for accel, 'G' for gyro (used to prefix columns in output).\n",
    "    \"\"\"\n",
    "    # Ensure list-like cols are truly Python lists\n",
    "    for c in [off_col, x_col, y_col, z_col]:\n",
    "        if df[c].dtype == object:\n",
    "            # If the cell is a JSON string, parse it; if it’s already list, keep it\n",
    "            df[c] = df[c].apply(lambda v: v if isinstance(v, (list, tuple, np.ndarray)) \n",
    "                                else json.loads(v) if isinstance(v, str) else v)\n",
    "\n",
    "    # Length sanity: offsets, x, y, z must be same length per row\n",
    "    def _check_lengths(row):\n",
    "        lens = list(map(len, [row[off_col], row[x_col], row[y_col], row[z_col]]))\n",
    "        return len(set(lens)) == 1\n",
    "\n",
    "    bad = df[~df.apply(_check_lengths, axis=1)]\n",
    "    if len(bad):\n",
    "        print(f\"[WARN] {len(bad)} rows with mismatched list lengths. They will be dropped.\")\n",
    "        df = df[df.apply(_check_lengths, axis=1)].copy()\n",
    "\n",
    "    # Build long-form rows\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        base = int(r[ts_col])\n",
    "        offs = r[off_col]\n",
    "        xs, ys, zs = r[x_col], r[y_col], r[z_col]\n",
    "        for o, xv, yv, zv in zip(offs, xs, ys, zs):\n",
    "            abs_ms = base + int(o)\n",
    "            rows.append({\n",
    "                \"timestamp_ms\": abs_ms,\n",
    "                f\"{prefix}x\": float(xv),\n",
    "                f\"{prefix}y\": float(yv),\n",
    "                f\"{prefix}z\": float(zv),\n",
    "            })\n",
    "    out = pd.DataFrame(rows).sort_values(\"timestamp_ms\").reset_index(drop=True)\n",
    "    # to datetime (UTC)\n",
    "    out[\"timestamp\"] = pd.to_datetime(out[\"timestamp_ms\"], unit=\"ms\", utc=True)\n",
    "    return out[[\"timestamp\", \"timestamp_ms\", f\"{prefix}x\", f\"{prefix}y\", f\"{prefix}z\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0516bf2e",
   "metadata": {},
   "source": [
    "## 5) Expand accel & gyro to per‑sample rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d1d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: rename once, keep your existing explode_sensor(..., x_col=\"x\", ...)\n",
    "accel = accel_raw.rename(columns={\n",
    "    \"calibrated_accel_x\":\"x\", \"calibrated_accel_y\":\"y\", \"calibrated_accel_z\":\"z\"\n",
    "})\n",
    "gyro  = gyro_raw.rename(columns={\n",
    "    \"calibrated_gyro_x\":\"x\", \"calibrated_gyro_y\":\"y\", \"calibrated_gyro_z\":\"z\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96fc96d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                 timestamp  timestamp_ms  \\\n",
       " 0  03/24/2025, 07:52:19 AM           766   \n",
       " 1  03/24/2025, 07:52:20 AM            23   \n",
       " 2  03/24/2025, 07:52:20 AM           271   \n",
       " \n",
       "                                   sample_time_offset  \\\n",
       " 0  [0, 10, 29, 38, 48, 57, 66, 77, 85, 95, 104, 1...   \n",
       " 1  [0, 9, 19, 28, 38, 48, 57, 67, 77, 86, 96, 105...   \n",
       " 2  [0, 10, 19, 29, 38, 47, 58, 66, 76, 86, 95, 10...   \n",
       " \n",
       "                                                    x  \\\n",
       " 0  [\"-62.48186\",\"-27.60828\",\"-95.41802\",\"-155.478...   \n",
       " 1  [\"-178.7271\",\"-161.2903\",\"-133.1977\",\"-124.479...   \n",
       " 2  [\"-81.85607\",\"-92.51189\",\"-101.2303\",\"-112.854...   \n",
       " \n",
       "                                                    y  \\\n",
       " 0  [\"-758.6971\",\"-984.1958\",\"-1240.532\",\"-1338.82...   \n",
       " 1  [\"-1360.991\",\"-1342.681\",\"-1269.442\",\"-1171.14...   \n",
       " 2  [\"-1140.310\",\"-1056.471\",\"-967.8134\",\"-805.917...   \n",
       " \n",
       "                                                    z  \n",
       " 0  [\"-1097.585\",\"-1221.784\",\"-1086.742\",\"-957.614...  \n",
       " 1  [\"-777.2302\",\"-619.5170\",\"-460.8181\",\"-345.490...  \n",
       " 2  [\"-254.8053\",\"-147.3632\",\"-53.72105\",\"-31.0497...  ,\n",
       "                  timestamp  timestamp_ms  \\\n",
       " 0  03/24/2025, 07:52:19 AM           795   \n",
       " 1  03/24/2025, 07:52:20 AM            52   \n",
       " 2  03/24/2025, 07:52:20 AM           300   \n",
       " \n",
       "                                   sample_time_offset  \\\n",
       " 0  [\"0\",\"9\",\"19\",\"38\",\"48\",\"57\",\"67\",\"76\",\"85\",\"9...   \n",
       " 1  [\"0\",\"9\",\"19\",\"28\",\"38\",\"48\",\"57\",\"67\",\"76\",\"8...   \n",
       " 2  [\"0\",\"9\",\"19\",\"29\",\"38\",\"48\",\"57\",\"66\",\"76\",\"8...   \n",
       " \n",
       "                                                    x  \\\n",
       " 0  [\"39.15614\",\"32.96125\",\"22.04125\",\"1.041086\",\"...   \n",
       " 1  [\"-24.26378\",\"-35.91878\",\"-44.66883\",\"-50.3738...   \n",
       " 2  [\"-49.46386\",\"-51.45886\",\"-49.60395\",\"-43.0239...   \n",
       " \n",
       "                                                    y  \\\n",
       " 0  [\"-7.520049\",\"-10.77502\",\"-9.620019\",\"-4.79006...   \n",
       " 1  [\"-12.10503\",\"-11.82503\",\"-11.30004\",\"-9.90004...   \n",
       " 2  [\"-9.515048\",\"-7.205049\",\"-4.720075\",\"-2.02507...   \n",
       " \n",
       "                                                    z  \n",
       " 0  [\"7.881836\",\"7.742001\",\"12.78200\",\"2.981754\",\"...  \n",
       " 1  [\"-0.3430405\",\"-1.078040\",\"-1.533123\",\"-2.1631...  \n",
       " 2  [\"-1.743164\",\"-2.723164\",\"-3.773308\",\"-4.08830...  )"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accel.head(3), gyro.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9a283",
   "metadata": {},
   "source": [
    "## 6) Align accelerometer & gyroscope (nearest timestamp with tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be00125",
   "metadata": {},
   "source": [
    "\n",
    "We align accel and gyro by the nearest timestamp using merge_asof, with a tolerance so mismatched samples are skipped. We also compute diagnostics: the match ratio and the alignment lag distribution in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed36c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 6] Matched within 1 ms: 100.0%\n",
      "[Step 6] Lag(ms): mean=0.000, std=0.000, max_abs=0.000\n",
      "[Step 6] merged shape: (14536, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shali\\AppData\\Local\\Temp\\ipykernel_68520\\4049600805.py:19: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  delta_ms = ((merged.loc[have_gyro, \"timestamp\"].view(\"int64\") - merged.loc[have_gyro, \"gyro_ts\"].view(\"int64\")) / 1e6)\n",
      "C:\\Users\\shali\\AppData\\Local\\Temp\\ipykernel_68520\\4049600805.py:19: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  delta_ms = ((merged.loc[have_gyro, \"timestamp\"].view(\"int64\") - merged.loc[have_gyro, \"gyro_ts\"].view(\"int64\")) / 1e6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Az</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gy</th>\n",
       "      <th>Gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-24 07:52:19+00:00</td>\n",
       "      <td>[\"-62.48186\",\"-27.60828\",\"-95.41802\",\"-155.478...</td>\n",
       "      <td>[\"-758.6971\",\"-984.1958\",\"-1240.532\",\"-1338.82...</td>\n",
       "      <td>[\"-1097.585\",\"-1221.784\",\"-1086.742\",\"-957.614...</td>\n",
       "      <td>[\"39.15614\",\"32.96125\",\"22.04125\",\"1.041086\",\"...</td>\n",
       "      <td>[\"-7.520049\",\"-10.77502\",\"-9.620019\",\"-4.79006...</td>\n",
       "      <td>[\"7.881836\",\"7.742001\",\"12.78200\",\"2.981754\",\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-24 07:52:20+00:00</td>\n",
       "      <td>[\"-178.7271\",\"-161.2903\",\"-133.1977\",\"-124.479...</td>\n",
       "      <td>[\"-1360.991\",\"-1342.681\",\"-1269.442\",\"-1171.14...</td>\n",
       "      <td>[\"-777.2302\",\"-619.5170\",\"-460.8181\",\"-345.490...</td>\n",
       "      <td>[\"2.160977\",\"1.005977\",\"1.531018\",\"1.986018\",\"...</td>\n",
       "      <td>[\"3.259907\",\"3.924907\",\"3.679918\",\"4.589918\",\"...</td>\n",
       "      <td>[\"-10.80841\",\"-11.50841\",\"-10.73835\",\"-9.47834...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-24 07:52:20+00:00</td>\n",
       "      <td>[\"-81.85607\",\"-92.51189\",\"-101.2303\",\"-112.854...</td>\n",
       "      <td>[\"-1140.310\",\"-1056.471\",\"-967.8134\",\"-805.917...</td>\n",
       "      <td>[\"-254.8053\",\"-147.3632\",\"-53.72105\",\"-31.0497...</td>\n",
       "      <td>[\"2.160977\",\"1.005977\",\"1.531018\",\"1.986018\",\"...</td>\n",
       "      <td>[\"3.259907\",\"3.924907\",\"3.679918\",\"4.589918\",\"...</td>\n",
       "      <td>[\"-10.80841\",\"-11.50841\",\"-10.73835\",\"-9.47834...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  \\\n",
       "0 2025-03-24 07:52:19+00:00   \n",
       "1 2025-03-24 07:52:20+00:00   \n",
       "2 2025-03-24 07:52:20+00:00   \n",
       "\n",
       "                                                  Ax  \\\n",
       "0  [\"-62.48186\",\"-27.60828\",\"-95.41802\",\"-155.478...   \n",
       "1  [\"-178.7271\",\"-161.2903\",\"-133.1977\",\"-124.479...   \n",
       "2  [\"-81.85607\",\"-92.51189\",\"-101.2303\",\"-112.854...   \n",
       "\n",
       "                                                  Ay  \\\n",
       "0  [\"-758.6971\",\"-984.1958\",\"-1240.532\",\"-1338.82...   \n",
       "1  [\"-1360.991\",\"-1342.681\",\"-1269.442\",\"-1171.14...   \n",
       "2  [\"-1140.310\",\"-1056.471\",\"-967.8134\",\"-805.917...   \n",
       "\n",
       "                                                  Az  \\\n",
       "0  [\"-1097.585\",\"-1221.784\",\"-1086.742\",\"-957.614...   \n",
       "1  [\"-777.2302\",\"-619.5170\",\"-460.8181\",\"-345.490...   \n",
       "2  [\"-254.8053\",\"-147.3632\",\"-53.72105\",\"-31.0497...   \n",
       "\n",
       "                                                  Gx  \\\n",
       "0  [\"39.15614\",\"32.96125\",\"22.04125\",\"1.041086\",\"...   \n",
       "1  [\"2.160977\",\"1.005977\",\"1.531018\",\"1.986018\",\"...   \n",
       "2  [\"2.160977\",\"1.005977\",\"1.531018\",\"1.986018\",\"...   \n",
       "\n",
       "                                                  Gy  \\\n",
       "0  [\"-7.520049\",\"-10.77502\",\"-9.620019\",\"-4.79006...   \n",
       "1  [\"3.259907\",\"3.924907\",\"3.679918\",\"4.589918\",\"...   \n",
       "2  [\"3.259907\",\"3.924907\",\"3.679918\",\"4.589918\",\"...   \n",
       "\n",
       "                                                  Gz  \n",
       "0  [\"7.881836\",\"7.742001\",\"12.78200\",\"2.981754\",\"...  \n",
       "1  [\"-10.80841\",\"-11.50841\",\"-10.73835\",\"-9.47834...  \n",
       "2  [\"-10.80841\",\"-11.50841\",\"-10.73835\",\"-9.47834...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- edit here if needed ----\n",
    "MERGE_TOLERANCE_MS = 1\n",
    "# -----------------------------\n",
    "\n",
    "tol = pd.Timedelta(milliseconds=MERGE_TOLERANCE_MS)\n",
    "\n",
    "# Keep a copy of gyro ts to compute lag\n",
    "gyro2 = gyro.rename(columns={\"timestamp\":\"gyro_ts\"}).copy()\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    accel.sort_values(\"timestamp\"), \n",
    "    gyro2.sort_values(\"gyro_ts\"),\n",
    "    left_on=\"timestamp\", right_on=\"gyro_ts\",\n",
    "    direction=\"nearest\", tolerance=tol\n",
    ")\n",
    "\n",
    "have_gyro = merged[[\"Gx\",\"Gy\",\"Gz\"]].notna().all(axis=1)\n",
    "match_ratio = float(have_gyro.mean())\n",
    "delta_ms = ((merged.loc[have_gyro, \"timestamp\"].view(\"int64\") - merged.loc[have_gyro, \"gyro_ts\"].view(\"int64\")) / 1e6)\n",
    "\n",
    "print(f\"[Step 6] Matched within {MERGE_TOLERANCE_MS} ms: {match_ratio:.1%}\")\n",
    "if len(delta_ms):\n",
    "    print(f\"[Step 6] Lag(ms): mean={delta_ms.mean():.3f}, std={delta_ms.std():.3f}, max_abs={np.abs(delta_ms).max():.3f}\")\n",
    "else:\n",
    "    print(\"[Step 6] No matched rows. Try a larger MERGE_TOLERANCE_MS.\")\n",
    "\n",
    "merged = merged.loc[have_gyro, [\"timestamp\",\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]].reset_index(drop=True)\n",
    "print(\"[Step 6] merged shape:\", merged.shape)\n",
    "merged.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef5eb2",
   "metadata": {},
   "source": [
    "## 7) Resample to exact 50 Hz (gap-free)\n",
    "\n",
    "#### What/why\n",
    "Make a perfect 50 Hz grid (20 ms), use mean per bin, then interpolate tiny gaps. Recreate timestamp_ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1490cc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7] Fixed @ 50 Hz: (181701, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shali\\AppData\\Local\\Temp\\ipykernel_68520\\1704905874.py:19: FutureWarning: Series.view is deprecated and will be removed in a future version. Use ``astype`` as an alternative to change the dtype.\n",
      "  fixed[\"timestamp_ms\"] = (fixed[\"timestamp\"].view(\"int64\") // 10**6).astype(\"int64\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Az</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gy</th>\n",
       "      <th>Gz</th>\n",
       "      <th>timestamp_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-24 07:52:19+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742802739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-24 07:52:19.020000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742802739020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-24 07:52:19.040000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742802739040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-24 07:52:19.060000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742802739060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-24 07:52:19.080000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742802739080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp  Ax  Ay  Az  Gx  Gy  Gz   timestamp_ms\n",
       "0        2025-03-24 07:52:19+00:00 NaN NaN NaN NaN NaN NaN  1742802739000\n",
       "1 2025-03-24 07:52:19.020000+00:00 NaN NaN NaN NaN NaN NaN  1742802739020\n",
       "2 2025-03-24 07:52:19.040000+00:00 NaN NaN NaN NaN NaN NaN  1742802739040\n",
       "3 2025-03-24 07:52:19.060000+00:00 NaN NaN NaN NaN NaN NaN  1742802739060\n",
       "4 2025-03-24 07:52:19.080000+00:00 NaN NaN NaN NaN NaN NaN  1742802739080"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- edit here if needed ----\n",
    "TARGET_HZ = 50\n",
    "# -----------------------------\n",
    "\n",
    "step_ms = int(round(1000 / TARGET_HZ))\n",
    "freq_str = f\"{step_ms}ms\"\n",
    "\n",
    "# Ensure numeric dtypes for sensor cols\n",
    "for c in [\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]:\n",
    "    merged[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n",
    "\n",
    "fixed = (\n",
    "    merged.set_index(\"timestamp\")[[\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]]  # numeric only\n",
    "          .resample(freq_str).mean()\n",
    "          .interpolate(limit_direction=\"both\")\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "fixed[\"timestamp_ms\"] = (fixed[\"timestamp\"].view(\"int64\") // 10**6).astype(\"int64\")\n",
    "\n",
    "print(f\"[Step 7] Fixed @ {TARGET_HZ} Hz:\", fixed.shape)\n",
    "fixed.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98834c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 8] Columns: ['timestamp', 'timestamp_ms', 'Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Az</th>\n",
       "      <th>Gx</th>\n",
       "      <th>Gy</th>\n",
       "      <th>Gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-24 07:52:19+00:00</td>\n",
       "      <td>1742802739000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-24 07:52:19.020000+00:00</td>\n",
       "      <td>1742802739020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-24 07:52:19.040000+00:00</td>\n",
       "      <td>1742802739040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp   timestamp_ms  Ax  Ay  Az  Gx  Gy  Gz\n",
       "0        2025-03-24 07:52:19+00:00  1742802739000 NaN NaN NaN NaN NaN NaN\n",
       "1 2025-03-24 07:52:19.020000+00:00  1742802739020 NaN NaN NaN NaN NaN NaN\n",
       "2 2025-03-24 07:52:19.040000+00:00  1742802739040 NaN NaN NaN NaN NaN NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- edit here if needed ----\n",
    "USE_RAD_PER_SEC = False\n",
    "# -----------------------------\n",
    "\n",
    "if USE_RAD_PER_SEC:\n",
    "    for c in [\"Gx\",\"Gy\",\"Gz\"]:\n",
    "        fixed[c] = np.deg2rad(fixed[c])\n",
    "\n",
    "fixed = fixed[[\"timestamp\",\"timestamp_ms\",\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]]\n",
    "print(\"[Step 8] Columns:\", list(fixed.columns))\n",
    "fixed.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6916bad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 9] Duplicate timestamps: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Found non-finite values!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m dupes = fixed[\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m].duplicated().sum()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Step 9] Duplicate timestamps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdupes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m np.isfinite(fixed[[\u001b[33m\"\u001b[39m\u001b[33mAx\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAy\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAz\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGx\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGy\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGz\u001b[39m\u001b[33m\"\u001b[39m]]).all().all(), \u001b[33m\"\u001b[39m\u001b[33mFound non-finite values!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m accel_ok = (fixed[[\u001b[33m\"\u001b[39m\u001b[33mAx\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAy\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAz\u001b[39m\u001b[33m\"\u001b[39m]].abs() < \u001b[32m50\u001b[39m).all().all()\n\u001b[32m      8\u001b[39m gyro_ok  = (fixed[[\u001b[33m\"\u001b[39m\u001b[33mGx\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGy\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGz\u001b[39m\u001b[33m\"\u001b[39m]].abs() < \u001b[32m2000\u001b[39m).all().all()\n",
      "\u001b[31mAssertionError\u001b[39m: Found non-finite values!"
     ]
    }
   ],
   "source": [
    "assert fixed[\"timestamp\"].is_monotonic_increasing, \"Timestamps are NOT sorted!\"\n",
    "dupes = fixed[\"timestamp\"].duplicated().sum()\n",
    "print(f\"[Step 9] Duplicate timestamps: {dupes}\")\n",
    "\n",
    "assert np.isfinite(fixed[[\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]]).all().all(), \"Found non-finite values!\"\n",
    "\n",
    "accel_ok = (fixed[[\"Ax\",\"Ay\",\"Az\"]].abs() < 50).all().all()\n",
    "gyro_ok  = (fixed[[\"Gx\",\"Gy\",\"Gz\"]].abs() < 2000).all().all()\n",
    "print(f\"[Step 9] Accel magnitude OK: {accel_ok} | Gyro magnitude OK: {gyro_ok}\")\n",
    "\n",
    "duration_s = (fixed[\"timestamp\"].iloc[-1] - fixed[\"timestamp\"].iloc[0]).total_seconds()\n",
    "print(f\"[Step 9] Duration: {duration_s:.2f}s, Rows: {len(fixed)}, Target Hz: {TARGET_HZ}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c141c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "807db654",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 0) Notebook title & overview\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> # Garmin Wearable — Simple Accel+Gyro Processing (Easy Mode)\n",
    ">\n",
    "> **Goal:** turn two raw Excel files (accelerometer & gyroscope) with batched samples into a single, clean **50 Hz** time series with columns:\n",
    "> `timestamp, timestamp_ms, Ax, Ay, Az, Gx, Gy, Gz`.\n",
    ">\n",
    "> **Key ideas:**\n",
    ">\n",
    "> 1. Parse list-like columns → 2) explode to one row per sample → 3) build true timestamps → 4) align accel↔gyro by nearest time → 5) resample to exact 50 Hz.\n",
    ">\n",
    "> We keep everything **UTC**, avoid clever abstractions, and comment every step.\n",
    "\n",
    "---\n",
    "\n",
    "# 1) Imports & file paths\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# 1) Imports & simple paths (edit the two paths below)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, ast\n",
    "\n",
    "# >>> EDIT THESE TWO LINES ONLY FOR YOUR FILES <<<\n",
    "ACCEL_PATH = \"2025-03-23-15-23-10-accelerometer_data.xlsx\"\n",
    "GYRO_PATH  = \"2025-03-23-15-23-10-gyroscope_data.xlsx\"\n",
    "\n",
    "# for resampling/alignment (edit if needed)\n",
    "TARGET_HZ = 50                 # resample target (Hz)\n",
    "MERGE_TOLERANCE_MS = 5         # max accel↔gyro time gap allowed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2) Load raw Excel & peek\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> We read both Excel files as-is to see the original column names.\n",
    "> Many Garmin exports have:\n",
    ">\n",
    "> * `timestamp` (date string)\n",
    "> * `timestamp_ms` (base ms offset)\n",
    "> * `sample_time_offset` (list of per-sample ms within the row)\n",
    "> * per-axis arrays like `calibrated_accel_x|y|z` or already `x|y|z`.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "accel_raw = pd.read_excel(ACCEL_PATH)\n",
    "gyro_raw  = pd.read_excel(GYRO_PATH)\n",
    "\n",
    "print(\"ACCEL cols:\", list(accel_raw.columns))\n",
    "print(\"GYRO  cols:\", list(gyro_raw.columns))\n",
    "display(accel_raw.head(2))\n",
    "display(gyro_raw.head(2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3) Normalize column names to a simple schema\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> To make later steps easy, we normalize headers:\n",
    ">\n",
    "> * Accelerometer → `x, y, z`\n",
    "> * Gyroscope     → `x, y, z`\n",
    ">   (We’ll add the `A`/`G` prefixes **after** exploding.)\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# For accel\n",
    "accel_map = {\n",
    "    \"calibrated_accel_x\": \"x\",\n",
    "    \"calibrated_accel_y\": \"y\",\n",
    "    \"calibrated_accel_z\": \"z\"\n",
    "}\n",
    "for k,v in accel_map.items():\n",
    "    if k in accel_raw.columns:\n",
    "        accel_raw = accel_raw.rename(columns={k:v})\n",
    "\n",
    "# For gyro\n",
    "gyro_map = {\n",
    "    \"calibrated_gyro_x\": \"x\",\n",
    "    \"calibrated_gyro_y\": \"y\",\n",
    "    \"calibrated_gyro_z\": \"z\"\n",
    "}\n",
    "for k,v in gyro_map.items():\n",
    "    if k in gyro_raw.columns:\n",
    "        gyro_raw = gyro_raw.rename(columns={k:v})\n",
    "\n",
    "# Check we have the basics we need\n",
    "needed = [\"timestamp\",\"timestamp_ms\",\"sample_time_offset\",\"x\",\"y\",\"z\"]\n",
    "missing_acc = [c for c in needed if c not in accel_raw.columns]\n",
    "missing_gyr = [c for c in needed if c not in gyro_raw.columns]\n",
    "print(\"Missing in accel:\", missing_acc)\n",
    "print(\"Missing in gyro :\", missing_gyr)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4) Convert list-like cells into real Python lists\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> In the raw files, `sample_time_offset` and `x/y/z` may be stored as JSON strings (like `\"[\"0\",\"20\",\"40\"]\"`) or Python-like lists (like `\"[0, 20, 40]\"`).\n",
    "> We parse them into real lists so we can **explode** them.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "def parse_list_cell(v):\n",
    "    \"\"\"Return a Python list from JSON/Python-like representations.\"\"\"\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return list(v)\n",
    "    if pd.isna(v):\n",
    "        return []\n",
    "    s = str(v).strip()\n",
    "    # Try JSON first\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        # Then Python literal (safe) e.g. \"[0, 20, 40]\"\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            # Fallback: try comma-split\n",
    "            s = s.strip(\"[]\")\n",
    "            if not s:\n",
    "                return []\n",
    "            return [pd.to_numeric(x, errors=\"coerce\") for x in s.split(\",\")]\n",
    "\n",
    "# Apply to both dataframes\n",
    "for df in (accel_raw, gyro_raw):\n",
    "    for c in [\"sample_time_offset\", \"x\", \"y\", \"z\"]:\n",
    "        df[c] = df[c].apply(parse_list_cell)\n",
    "\n",
    "print(\"Parsed list lengths (first row accel):\",\n",
    "      [len(accel_raw.iloc[0][c]) for c in [\"sample_time_offset\",\"x\",\"y\",\"z\"]])\n",
    "print(\"Parsed list lengths (first row gyro):\",\n",
    "      [len(gyro_raw.iloc[0][c]) for c in [\"sample_time_offset\",\"x\",\"y\",\"z\"]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5) Drop any rows where list lengths are mismatched\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> Each row’s `offsets`, `x`, `y`, and `z` must have the **same length**.\n",
    "> If not, we drop that row (rare, but safer than guessing).\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "def ok_lengths(row):\n",
    "    a = len(row[\"sample_time_offset\"])\n",
    "    b = len(row[\"x\"]); c = len(row[\"y\"]); d = len(row[\"z\"])\n",
    "    return (a == b == c == d)\n",
    "\n",
    "acc_ok = accel_raw.apply(ok_lengths, axis=1)\n",
    "gyr_ok = gyro_raw.apply(ok_lengths, axis=1)\n",
    "\n",
    "print(\"Accel bad rows:\", (~acc_ok).sum(), \" | Gyro bad rows:\", (~gyr_ok).sum())\n",
    "\n",
    "accel_raw = accel_raw.loc[acc_ok].reset_index(drop=True).copy()\n",
    "gyro_raw  = gyro_raw.loc[gyr_ok].reset_index(drop=True).copy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6) Explode to **one row per sample** (accel & gyro)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> We convert batched rows into **per-sample** rows.\n",
    "> Then we create **true timestamps** as:\n",
    "> `timestamp_true = to_datetime(timestamp, UTC) + timestamp_ms (ms) + sample_time_offset (ms)`.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# Explode in-place (pandas can explode multiple columns at once)\n",
    "accel_long = accel_raw.explode([\"sample_time_offset\",\"x\",\"y\",\"z\"], ignore_index=True).copy()\n",
    "gyro_long  = gyro_raw.explode ([\"sample_time_offset\",\"x\",\"y\",\"z\"], ignore_index=True).copy()\n",
    "\n",
    "# Build true timestamps (UTC)\n",
    "def make_true_time(df):\n",
    "    # base = timestamp (UTC) + timestamp_ms\n",
    "    base = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\") \\\n",
    "           + pd.to_timedelta(pd.to_numeric(df[\"timestamp_ms\"], errors=\"coerce\").fillna(0).astype(\"int64\"), unit=\"ms\")\n",
    "    # add per-sample offset\n",
    "    off  = pd.to_timedelta(pd.to_numeric(df[\"sample_time_offset\"], errors=\"coerce\").fillna(0).astype(\"int64\"), unit=\"ms\")\n",
    "    return base + off\n",
    "\n",
    "accel_long[\"timestamp\"] = make_true_time(accel_long)\n",
    "gyro_long[\"timestamp\"]  = make_true_time(gyro_long)\n",
    "\n",
    "# Keep only what we need, and rename axes with prefixes\n",
    "accel_long = accel_long[[\"timestamp\",\"x\",\"y\",\"z\"]].rename(columns={\"x\":\"Ax\",\"y\":\"Ay\",\"z\":\"Az\"})\n",
    "gyro_long  = gyro_long [[\"timestamp\",\"x\",\"y\",\"z\"]].rename(columns={\"x\":\"Gx\",\"y\":\"Gy\",\"z\":\"Gz\"})\n",
    "\n",
    "# Ensure numeric types\n",
    "for c in [\"Ax\",\"Ay\",\"Az\"]:\n",
    "    accel_long[c] = pd.to_numeric(accel_long[c], errors=\"coerce\")\n",
    "for c in [\"Gx\",\"Gy\",\"Gz\"]:\n",
    "    gyro_long[c]  = pd.to_numeric(gyro_long[c],  errors=\"coerce\")\n",
    "\n",
    "# Sort\n",
    "accel_long = accel_long.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "gyro_long  = gyro_long.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(\"accel_long:\", accel_long.shape, \"gyro_long:\", gyro_long.shape)\n",
    "display(accel_long.head(3))\n",
    "display(gyro_long.head(3))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7) Align accel & gyro by **nearest timestamp** (with tolerance)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> For each accel sample, find the nearest gyro sample within a small time window.\n",
    "> If there’s no gyro within the window, we drop that accel row.\n",
    "> We also print **match ratio** and basic **lag** statistics.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "tolerance = pd.Timedelta(milliseconds=MERGE_TOLERANCE_MS)\n",
    "\n",
    "gyro_for_merge = gyro_long.rename(columns={\"timestamp\":\"gyro_ts\"})\n",
    "merged = pd.merge_asof(\n",
    "    accel_long.sort_values(\"timestamp\"),\n",
    "    gyro_for_merge.sort_values(\"gyro_ts\"),\n",
    "    left_on=\"timestamp\", right_on=\"gyro_ts\",\n",
    "    direction=\"nearest\", tolerance=tolerance\n",
    ")\n",
    "\n",
    "have_gyro = merged[[\"Gx\",\"Gy\",\"Gz\"]].notna().all(axis=1)\n",
    "match_ratio = have_gyro.mean()\n",
    "lag_ms = ((merged.loc[have_gyro, \"timestamp\"].astype(\"int64\")\n",
    "          - merged.loc[have_gyro, \"gyro_ts\"].astype(\"int64\")) / 1e6)\n",
    "\n",
    "print(f\"Matched within {MERGE_TOLERANCE_MS} ms: {match_ratio:.1%}\")\n",
    "if len(lag_ms):\n",
    "    print(f\"Lag ms — mean: {lag_ms.mean():.3f}, std: {lag_ms.std():.3f}, max_abs: {np.abs(lag_ms).max():.3f}\")\n",
    "else:\n",
    "    print(\"No matched samples. Increase MERGE_TOLERANCE_MS?\")\n",
    "\n",
    "# Keep only matched rows and drop helper column\n",
    "merged = merged.loc[have_gyro, [\"timestamp\",\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]].reset_index(drop=True)\n",
    "display(merged.head(5))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8) Resample to **exact 50 Hz** (gap-free)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> We convert the aligned series into a perfect **50 Hz** timeline (every 20 ms).\n",
    "> We aggregate with `mean` within bins and use time interpolation for tiny gaps.\n",
    "> Finally we add `timestamp_ms` for convenience.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "step_ms = int(round(1000 / TARGET_HZ))\n",
    "freq = f\"{step_ms}ms\"\n",
    "\n",
    "# Ensure numeric (protect against stray objects)\n",
    "for c in [\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]:\n",
    "    merged[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n",
    "\n",
    "fixed = (merged\n",
    "         .set_index(\"timestamp\")[[\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]]\n",
    "         .resample(freq)\n",
    "         .mean()\n",
    "         .interpolate(method=\"time\", limit_direction=\"both\")\n",
    "         .reset_index())\n",
    "\n",
    "# Add timestamp_ms (int)\n",
    "fixed[\"timestamp_ms\"] = (fixed[\"timestamp\"].astype(\"int64\") // 10**6).astype(\"int64\")\n",
    "\n",
    "print(\"Fixed @\", TARGET_HZ, \"Hz:\", fixed.shape)\n",
    "display(fixed.head(5))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 9) Sanity checks (order, duplicates, finites, rough ranges)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> Quick guards to catch obvious problems early.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# monotonic timestamps\n",
    "assert fixed[\"timestamp\"].is_monotonic_increasing, \"Timestamps not sorted!\"\n",
    "\n",
    "# duplicates\n",
    "dupes = fixed[\"timestamp\"].duplicated().sum()\n",
    "print(\"Duplicate timestamps:\", dupes)\n",
    "\n",
    "# finite values\n",
    "assert np.isfinite(fixed[[\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]]).all().all(), \"Non-finite values found!\"\n",
    "\n",
    "# rough magnitude checks (tweak if your units differ)\n",
    "accel_ok = (fixed[[\"Ax\",\"Ay\",\"Az\"]].abs() < 50).all().all()      # ~5g if m/s² (~49)\n",
    "gyro_ok  = (fixed[[\"Gx\",\"Gy\",\"Gz\"]].abs() < 2000).all().all()    # <2000 deg/s typical\n",
    "print(\"Accel magnitude OK:\", accel_ok, \"| Gyro magnitude OK:\", gyro_ok)\n",
    "\n",
    "duration_s = (fixed[\"timestamp\"].iloc[-1] - fixed[\"timestamp\"].iloc[0]).total_seconds()\n",
    "print(f\"Duration: {duration_s:.2f}s | Rows: {len(fixed)} | Target Hz: {TARGET_HZ}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 10) Save outputs (Parquet + CSV)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> Parquet is fast & typed; CSV is handy to eyeball in Excel.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# >>> EDIT THESE OUTPUT PATHS IF YOU WANT <<<\n",
    "PARQUET_PATH = \"processed/session_50hz.parquet\"\n",
    "CSV_PATH     = \"processed/session_50hz.csv\"\n",
    "\n",
    "import os\n",
    "os.makedirs(os.path.dirname(PARQUET_PATH), exist_ok=True)\n",
    "\n",
    "fixed.to_parquet(PARQUET_PATH, index=False)\n",
    "fixed.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "print(\"Saved:\", PARQUET_PATH, \"|\", CSV_PATH)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Why this is easier\n",
    "\n",
    "* No functions except tiny **parse\\_list\\_cell** and **make\\_true\\_time**-style code; everything is **linear**.\n",
    "* Each step has a **single purpose** and small edit points (paths, tolerance, Hz).\n",
    "* We rely on **pandas explode** (clear + fast) instead of manual loops.\n",
    "* Simple **diagnostics** (match %, lag) tell you when to tweak `MERGE_TOLERANCE_MS`.\n",
    "\n",
    "## If you still hit errors\n",
    "\n",
    "1. Run these quick prints right after the failing step and share the output:\n",
    "\n",
    "```python\n",
    "print(accel_raw.dtypes); print(accel_raw.head(2))\n",
    "print(gyro_raw.dtypes);  print(gyro_raw.head(2))\n",
    "print(accel_long.dtypes); print(accel_long.head(2))\n",
    "print(gyro_long.dtypes);  print(gyro_long.head(2))\n",
    "print(merged.dtypes);     print(merged.head(2))\n",
    "```\n",
    "\n",
    "2. Common fixes:\n",
    "\n",
    "* Increase `MERGE_TOLERANCE_MS` from 5 → 10 or 20 if match ratio is low.\n",
    "* If your pandas is old and can’t explode multiple columns at once, explode one by one:\n",
    "\n",
    "  ```python\n",
    "  accel_tmp = accel_raw.explode(\"sample_time_offset\")\n",
    "  accel_tmp[\"x\"] = accel_tmp[\"x\"].apply(lambda lst: lst.pop(0) if isinstance(lst, list) and len(lst)>0 else np.nan)\n",
    "  # (or switch to a small loop like we used earlier)\n",
    "  ```\n",
    "\n",
    "Want me to package these cells into a polished `.ipynb` with headings already in place?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cb445",
   "metadata": {},
   "source": [
    "love it — let’s add **labels** and **windowing** the same “easy-mode” way.\n",
    "No config objects. Tiny “EDIT HERE” lines. Clear markdown. Drop-in cells.\n",
    "\n",
    "---\n",
    "\n",
    "# 11) Add labels (time-window style)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> We’ll tag each row in `fixed` with a `label` based on **time windows**.\n",
    "> Two simple options for your label windows file:\n",
    ">\n",
    "> 1. **JSON** list (recommended)\n",
    ">\n",
    ">    ```json\n",
    ">    [\n",
    ">      {\"label\":\"rest\", \"start_ms\":1711185600000, \"end_ms\":1711185660000},\n",
    ">      {\"label\":\"walk\", \"start_ms\":1711185660000, \"end_ms\":1711185780000}\n",
    ">    ]\n",
    ">    ```\n",
    ">\n",
    ">    (times in **milliseconds since epoch**; inclusive start, exclusive end)\n",
    "> 2. **CSV** with header: `label,start_iso,end_iso` (ISO 8601 timestamps, UTC)\n",
    ">\n",
    ">    ```\n",
    ">    rest, 2025-03-23T10:00:00Z, 2025-03-23T10:01:00Z\n",
    ">    walk, 2025-03-23T10:01:00Z, 2025-03-23T10:03:00Z\n",
    ">    ```\n",
    ">\n",
    "> We’ll support both. Pick one by editing a variable.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ====== EDIT HERE ======\n",
    "LABEL_SOURCE = \"json\"      # \"json\" or \"csv\"\n",
    "JSON_PATH    = \"labels/session_001.json\"\n",
    "CSV_PATH     = \"labels/session_001.csv\"\n",
    "# =======================\n",
    "\n",
    "# Prepare an empty label column first\n",
    "fixed[\"label\"] = \"unknown\"\n",
    "\n",
    "# Helper: apply time windows to 'fixed'\n",
    "def apply_label_windows_from_ms(fixed_df, windows, default=\"unknown\"):\n",
    "    # windows: list of dicts with label, start_ms, end_ms (end is exclusive)\n",
    "    tsms = fixed_df[\"timestamp_ms\"].values\n",
    "    lbl  = np.array([default]*len(fixed_df), dtype=object)\n",
    "    for w in windows:\n",
    "        s = int(w[\"start_ms\"]); e = int(w[\"end_ms\"])\n",
    "        mask = (tsms >= s) & (tsms < e)\n",
    "        lbl[mask] = w[\"label\"]\n",
    "    fixed_df = fixed_df.copy()\n",
    "    fixed_df[\"label\"] = lbl\n",
    "    return fixed_df\n",
    "\n",
    "def read_windows():\n",
    "    if LABEL_SOURCE == \"json\" and os.path.exists(JSON_PATH):\n",
    "        with open(JSON_PATH, \"r\") as f:\n",
    "            L = json.load(f)\n",
    "        # basic validation\n",
    "        for w in L:\n",
    "            assert \"label\" in w and (\"start_ms\" in w or \"start\" in w)\n",
    "        # Support optional ISO keys too\n",
    "        out = []\n",
    "        for w in L:\n",
    "            if \"start_ms\" in w and \"end_ms\" in w:\n",
    "                out.append({\"label\": w[\"label\"], \"start_ms\": int(w[\"start_ms\"]), \"end_ms\": int(w[\"end_ms\"])})\n",
    "            else:\n",
    "                # if ISO strings provided\n",
    "                s = pd.Timestamp(w[\"start\"], tz=\"UTC\").value // 10**6\n",
    "                e = pd.Timestamp(w[\"end\"],   tz=\"UTC\").value // 10**6\n",
    "                out.append({\"label\": w[\"label\"], \"start_ms\": int(s), \"end_ms\": int(e)})\n",
    "        return out\n",
    "\n",
    "    if LABEL_SOURCE == \"csv\" and os.path.exists(CSV_PATH):\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        assert {\"label\",\"start_iso\",\"end_iso\"}.issubset(df.columns), \"CSV needs columns: label,start_iso,end_iso\"\n",
    "        out = []\n",
    "        for _, r in df.iterrows():\n",
    "            s = pd.to_datetime(r[\"start_iso\"], utc=True).value // 10**6\n",
    "            e = pd.to_datetime(r[\"end_iso\"],   utc=True).value // 10**6\n",
    "            out.append({\"label\": r[\"label\"], \"start_ms\": int(s), \"end_ms\": int(e)})\n",
    "        return out\n",
    "\n",
    "    print(\"⚠️ No labels file found. Keeping 'unknown' for all rows.\")\n",
    "    return []\n",
    "\n",
    "windows = read_windows()\n",
    "if len(windows):\n",
    "    fixed = apply_label_windows_from_ms(fixed, windows, default=\"unknown\")\n",
    "\n",
    "fixed[\"label\"].value_counts(dropna=False).head()\n",
    "```\n",
    "\n",
    "**What you’ll get**\n",
    "\n",
    "* `fixed` now has a `label` column.\n",
    "* Prints the class counts (unknown + any applied labels).\n",
    "\n",
    "---\n",
    "\n",
    "# 12) Sliding windowing (build model inputs)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> We’ll convert the 50 Hz stream into **overlapping windows** for ML/DL:\n",
    ">\n",
    "> * Window length = `WINDOW_SEC` seconds\n",
    "> * Hop (stride)   = `HOP_SEC` seconds\n",
    "> * Features per sample: `[Ax, Ay, Az, Gx, Gy, Gz]`\n",
    "> * Window label = **majority label** inside the window\n",
    ">\n",
    ">   * We’ll drop windows where the majority label coverage is below a threshold (e.g. 80%), or if the majority is `\"unknown\"`.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ====== EDIT HERE ======\n",
    "WINDOW_SEC          = 2.0   # e.g., 2-second windows\n",
    "HOP_SEC             = 1.0   # e.g., 50% overlap at 50 Hz\n",
    "ASSUMED_HZ          = 50    # our 'fixed' is at 50 Hz\n",
    "MIN_LABEL_COVERAGE  = 0.80  # require at least 80% of samples in a window to be the majority label\n",
    "DROP_UNKNOWN_WINDOWS = True # drop windows whose majority is \"unknown\"\n",
    "FEATURE_COLS = [\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]\n",
    "# =======================\n",
    "\n",
    "# Safety: ensure order and no NaNs in features\n",
    "fixed = fixed.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "for c in FEATURE_COLS:\n",
    "    fixed[c] = pd.to_numeric(fixed[c], errors=\"coerce\")\n",
    "fixed[FEATURE_COLS] = fixed[FEATURE_COLS].interpolate(limit_direction=\"both\")\n",
    "assert np.isfinite(fixed[FEATURE_COLS].values).all(), \"Non-finite values remain after interpolation.\"\n",
    "\n",
    "win_len = int(round(WINDOW_SEC * ASSUMED_HZ))\n",
    "hop_len = int(round(HOP_SEC * ASSUMED_HZ))\n",
    "assert win_len > 0 and hop_len > 0, \"Window/Hop must be >= 1 sample.\"\n",
    "\n",
    "X_list, y_list, t_start_list, t_end_list = [], [], [], []\n",
    "labels_arr = fixed[\"label\"].values\n",
    "feat_arr   = fixed[FEATURE_COLS].values\n",
    "ts_arr     = fixed[\"timestamp\"].values\n",
    "\n",
    "N = len(fixed)\n",
    "i = 0\n",
    "while i + win_len <= N:\n",
    "    window_feats  = feat_arr[i:i+win_len]\n",
    "    window_labels = labels_arr[i:i+win_len]\n",
    "    # majority label\n",
    "    c = Counter(window_labels)\n",
    "    majority_label, maj_count = c.most_common(1)[0]\n",
    "    coverage = maj_count / win_len\n",
    "\n",
    "    # rules to keep/drop\n",
    "    if coverage >= MIN_LABEL_COVERAGE and (not DROP_UNKNOWN_WINDOWS or majority_label != \"unknown\"):\n",
    "        X_list.append(window_feats.copy())\n",
    "        y_list.append(majority_label)\n",
    "        t_start_list.append(ts_arr[i])\n",
    "        t_end_list.append(ts_arr[i+win_len-1])\n",
    "\n",
    "    i += hop_len\n",
    "\n",
    "X = np.array(X_list, dtype=np.float32)          # shape: (num_windows, win_len, 6)\n",
    "y_labels = np.array(y_list, dtype=object)       # shape: (num_windows,)\n",
    "t_bounds = np.array(list(zip(t_start_list, t_end_list)), dtype=object)\n",
    "\n",
    "print(\"Windows created:\", X.shape, \"| kept labels:\", len(y_labels))\n",
    "print(\"Label distribution:\", Counter(y_labels))\n",
    "print(\"Window length (samples):\", win_len, \"| Hop (samples):\", hop_len)\n",
    "```\n",
    "\n",
    "**What you’ll get**\n",
    "\n",
    "* `X`: `(num_windows, win_len, 6)` float32 array\n",
    "* `y_labels`: `(num_windows,)` array of string labels\n",
    "* `t_bounds`: start/end timestamps per window\n",
    "\n",
    "---\n",
    "\n",
    "# 13) Encode labels to integers (optional but handy)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> DL frameworks prefer integer class IDs. We’ll map label strings → integers.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# Build mapping from seen labels\n",
    "unique_labels = sorted(list(set(y_labels.tolist())))\n",
    "label_to_id = {lbl:i for i,lbl in enumerate(unique_labels)}\n",
    "id_to_label = {i:lbl for lbl,i in label_to_id.items()}\n",
    "\n",
    "y = np.array([label_to_id[lbl] for lbl in y_labels], dtype=np.int64)\n",
    "print(\"label_to_id:\", label_to_id)\n",
    "print(\"X:\", X.shape, \"| y:\", y.shape)\n",
    "```\n",
    "\n",
    "**What you’ll get**\n",
    "\n",
    "* `y`: integer labels aligned with `X`\n",
    "* `label_to_id` / `id_to_label` dicts\n",
    "\n",
    "---\n",
    "\n",
    "# 14) Quick train/val/test split (stratified-ish)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> We’ll do a simple randomized split that keeps class balance roughly intact.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# ====== EDIT HERE ======\n",
    "TRAIN_FRAC = 0.7\n",
    "VAL_FRAC   = 0.15\n",
    "TEST_FRAC  = 0.15\n",
    "RANDOM_SEED = 7\n",
    "# =======================\n",
    "\n",
    "assert abs(TRAIN_FRAC + VAL_FRAC + TEST_FRAC - 1.0) < 1e-9\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "idx = np.arange(len(X))\n",
    "rng.shuffle(idx)\n",
    "\n",
    "n = len(idx)\n",
    "n_train = int(TRAIN_FRAC * n)\n",
    "n_val   = int(VAL_FRAC   * n)\n",
    "train_idx = idx[:n_train]\n",
    "val_idx   = idx[n_train:n_train+n_val]\n",
    "test_idx  = idx[n_train+n_val:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_val,   y_val   = X[val_idx],   y[val_idx]\n",
    "X_test,  y_test  = X[test_idx],  y[test_idx]\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 15) Save windowed dataset (NPZ)\n",
    "\n",
    "**Markdown**\n",
    "\n",
    "> Save a compact `.npz` you can load in PyTorch/TF or scikit-learn later.\n",
    "\n",
    "**Code**\n",
    "\n",
    "```python\n",
    "# ====== EDIT HERE ======\n",
    "NPZ_PATH = \"processed/session_windows_2s_1s_50hz.npz\"\n",
    "# =======================\n",
    "\n",
    "os.makedirs(os.path.dirname(NPZ_PATH), exist_ok=True)\n",
    "np.savez_compressed(\n",
    "    NPZ_PATH,\n",
    "    X=X, y=y,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_val=X_val,     y_val=y_val,\n",
    "    X_test=X_test,   y_test=y_test,\n",
    "    label_to_id=json.dumps(label_to_id),\n",
    "    id_to_label=json.dumps(id_to_label),\n",
    "    feature_names=json.dumps(FEATURE_COLS),\n",
    "    window_sec=WINDOW_SEC, hop_sec=HOP_SEC, hz=ASSUMED_HZ\n",
    ")\n",
    "print(\"Saved:\", NPZ_PATH)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Sanity recap (what you have now)\n",
    "\n",
    "* `fixed`: clean **50 Hz** stream with `Ax..Gz` + `label` per row\n",
    "* `X, y_labels, y`: windowed inputs + labels (majority vote)\n",
    "* Train/Val/Test splits + a saved **.npz** bundle\n",
    "\n",
    "---\n",
    "\n",
    "## Tips you can tweak anytime\n",
    "\n",
    "* **Overlap & window size:** change `WINDOW_SEC` / `HOP_SEC`.\n",
    "* **Coverage rule:** raise/lower `MIN_LABEL_COVERAGE` to be stricter/looser.\n",
    "* **Unknown handling:** set `DROP_UNKNOWN_WINDOWS=False` if you want to keep them (often better to drop).\n",
    "* **Unit conversion:** if your gyro is deg/s and a model expects rad/s, toggle earlier (Step 8 in the base pipeline).\n",
    "\n",
    "If you want, I can add a tiny **feature standardization** cell (fit on train, apply to val/test) and a **PyTorch dataset** snippet next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c39a92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
