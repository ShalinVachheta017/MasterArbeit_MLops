{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64671975",
   "metadata": {},
   "source": [
    "## üìÇ Step 1: Setup & Configuration\n",
    "\n",
    "**What:** Import libraries and define paths/parameters.\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `window_size` | 200 | Samples per window (4 sec @ 50Hz) |\n",
    "| `overlap` | 50% | Windows overlap by half |\n",
    "| `step` | 100 | Samples between window starts |\n",
    "| `conversion_factor` | 0.00981 | milliG to m/s¬≤ |\n",
    "\n",
    "**Input:** None  \n",
    "**Output:** `CONFIG` dict, path variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths (CORRECT folder names)\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "DATA_PREPARED = PROJECT_ROOT / 'data' / 'prepared'\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'input_file': DATA_PROCESSED / 'sensor_fused_50Hz.csv',\n",
    "    'window_size': 200,\n",
    "    'overlap': 0.5,\n",
    "    'step': 100,\n",
    "    'conversion_factor': 0.00981,  # milliG to m/s¬≤\n",
    "    'sensor_columns': ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz'],\n",
    "    'accel_columns': ['Ax', 'Ay', 'Az'],\n",
    "}\n",
    "\n",
    "print(f\"‚úì Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"‚úì Data Processed: {DATA_PROCESSED}\")\n",
    "print(f\"‚úì Data Prepared: {DATA_PREPARED}\")\n",
    "print(f\"\\nüì• Input File: {CONFIG['input_file']}\")\n",
    "print(f\"üìä Window Size: {CONFIG['window_size']} samples (4 sec @ 50Hz)\")\n",
    "print(f\"üîÑ Overlap: {CONFIG['overlap']*100:.0f}%\")\n",
    "print(f\"üî¢ Step: {CONFIG['step']} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54660d00",
   "metadata": {},
   "source": [
    "## üì• Step 2: Load Production Data\n",
    "\n",
    "**What:** Load the production CSV file and validate columns.\n",
    "\n",
    "### Expected Columns\n",
    "| Column | Sensor | Unit (raw) |\n",
    "|--------|--------|------------|\n",
    "| Ax, Ay, Az | Accelerometer | milliG or m/s¬≤ |\n",
    "| Gx, Gy, Gz | Gyroscope | deg/s |\n",
    "\n",
    "**Input:** `data/processed/sensor_fused_50Hz.csv`  \n",
    "**Output:** `df` DataFrame with ~181,699 rows √ó 6+ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(CONFIG['input_file'])\n",
    "\n",
    "print(f\"‚úì Loaded: {CONFIG['input_file'].name}\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"\\nüìã First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe67680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate required columns exist\n",
    "missing = [col for col in CONFIG['sensor_columns'] if col not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"‚ùå Missing columns: {missing}\")\n",
    "    \n",
    "print(f\"‚úì All sensor columns present: {CONFIG['sensor_columns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be5045",
   "metadata": {},
   "source": [
    "## üîç Step 3: Unit Detection & Conversion\n",
    "\n",
    "**What:** Automatically detect if accelerometer data is in milliG or m/s¬≤, then convert if needed.\n",
    "\n",
    "### Detection Logic\n",
    "\n",
    "| Condition | Units Detected | Action |\n",
    "|-----------|----------------|--------|\n",
    "| max absolute > 100 | **milliG** | Convert √ó 0.00981 |\n",
    "| max absolute < 50 | **m/s¬≤** | No conversion |\n",
    "| 50-100 (ambiguous) | unknown | Assume m/s¬≤ |\n",
    "\n",
    "### Why This Matters\n",
    "- Training data was in **m/s¬≤**\n",
    "- Production data from Garmin is often in **milliG**\n",
    "- Mismatch = wrong predictions!\n",
    "\n",
    "**Input:** `df` (raw sensor data)  \n",
    "**Output:** `df_converted`, `units_detected`, `conversion_applied`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902f4d3",
   "metadata": {},
   "source": [
    "### üìñ Understanding Units: milliG vs m/s¬≤\n",
    "\n",
    "**milliG (milligravity):**\n",
    "- 1 milliG = 1/1000 of Earth's gravity\n",
    "- Used by: Garmin watches, fitness trackers\n",
    "- Example: Az at rest = -1000 milliG (gravity pointing down)\n",
    "\n",
    "**m/s¬≤ (meters per second squared):**\n",
    "- Standard physics unit of acceleration\n",
    "- Used by: Scientific datasets, ML models\n",
    "- Example: Az at rest = -9.81 m/s¬≤ (Earth's gravity)\n",
    "\n",
    "**Conversion:**\n",
    "```python\n",
    "m_s2 = milliG √ó 0.00981\n",
    "\n",
    "# Why? 1 milliG = 0.001 G √ó 9.81 m/s¬≤ = 0.00981 m/s¬≤\n",
    "```\n",
    "\n",
    "**Why Critical:**\n",
    "- ‚ö†Ô∏è Training data: m/s¬≤ (values around -9.8)\n",
    "- ‚ö†Ô∏è Garmin production: milliG (values around -1000)\n",
    "- ‚ö†Ô∏è Mismatch = 100√ó wrong scale = Bad predictions!\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Training learns: \"Standing has Az ‚âà -9.8\"\n",
    "Production sends: Az = -1000 (not converted)\n",
    "Model thinks: \"This is 100√ó gravity!\" ‚Üí Wrong prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a647e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_convert_units(df, accel_cols, conversion_factor=0.00981):\n",
    "    \"\"\"\n",
    "    Automatically detect accelerometer units and convert if needed.\n",
    "    \n",
    "    Returns:\n",
    "        df_converted: DataFrame with converted values\n",
    "        units_detected: 'milliG' or 'm/s¬≤'\n",
    "        conversion_applied: True if conversion was applied\n",
    "    \"\"\"\n",
    "    # Check max absolute value\n",
    "    max_abs = df[accel_cols].abs().max().max()\n",
    "    mean_abs = df[accel_cols].abs().mean().mean()\n",
    "    \n",
    "    print(f\"üìä Accelerometer Statistics:\")\n",
    "    print(f\"  Max absolute value: {max_abs:.2f}\")\n",
    "    print(f\"  Mean absolute value: {mean_abs:.2f}\")\n",
    "    \n",
    "    # Detection\n",
    "    if max_abs > 100:\n",
    "        units_detected = 'milliG'\n",
    "        conversion_applied = True\n",
    "        print(f\"\\nüîç Detected: milliG (max > 100)\")\n",
    "        print(f\"üîÑ Converting with factor: {conversion_factor}\")\n",
    "        \n",
    "        # Convert\n",
    "        df_converted = df.copy()\n",
    "        for col in accel_cols:\n",
    "            df_converted[col] = df[col] * conversion_factor\n",
    "        \n",
    "        # Validate\n",
    "        az_mean = df_converted['Az'].mean()\n",
    "        print(f\"\\n‚úÖ Validation:\")\n",
    "        print(f\"  Az mean after conversion: {az_mean:.2f} m/s¬≤\")\n",
    "        if -11 < az_mean < -8:\n",
    "            print(f\"  ‚úì Valid (expected ‚âà -9.8 for gravity)\")\n",
    "        else:\n",
    "            print(f\"  ‚ö† Warning: Az not near -9.8, check data\")\n",
    "            \n",
    "    elif max_abs < 50:\n",
    "        units_detected = 'm/s¬≤'\n",
    "        conversion_applied = False\n",
    "        df_converted = df.copy()\n",
    "        print(f\"\\nüîç Detected: m/s¬≤ (max < 50)\")\n",
    "        print(f\"‚úì No conversion needed\")\n",
    "    else:\n",
    "        # Ambiguous range\n",
    "        units_detected = 'unknown'\n",
    "        conversion_applied = False\n",
    "        df_converted = df.copy()\n",
    "        print(f\"\\n‚ö† Ambiguous range (50-100), assuming m/s¬≤\")\n",
    "    \n",
    "    return df_converted, units_detected, conversion_applied\n",
    "\n",
    "# Apply detection and conversion\n",
    "df_converted, units_detected, conversion_applied = detect_and_convert_units(\n",
    "    df, CONFIG['accel_columns'], CONFIG['conversion_factor']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Result: units={units_detected}, converted={conversion_applied}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show before/after comparison\n",
    "if conversion_applied:\n",
    "    comparison = pd.DataFrame({\n",
    "        'Column': CONFIG['accel_columns'],\n",
    "        'Before (milliG)': [df[col].mean() for col in CONFIG['accel_columns']],\n",
    "        'After (m/s¬≤)': [df_converted[col].mean() for col in CONFIG['accel_columns']],\n",
    "    })\n",
    "    print(\"üìä Before/After Conversion (means):\")\n",
    "    display(comparison)\n",
    "else:\n",
    "    print(\"‚úì No conversion applied - data already in m/s¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564c45e",
   "metadata": {},
   "source": [
    "## üßπ Step 4: Handle NaN Values\n",
    "\n",
    "**What:** Fill missing values using forward-fill then backward-fill.\n",
    "\n",
    "### Strategy\n",
    "```\n",
    "ffill ‚Üí Use previous valid value\n",
    "bfill ‚Üí Use next valid value (for leading NaNs)\n",
    "```\n",
    "\n",
    "**Why:** Windows with NaN cannot be used for inference.\n",
    "\n",
    "**Input:** `df_converted` (may contain NaN)  \n",
    "**Output:** `df_clean` (no NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e177cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN\n",
    "nan_count = df_converted[CONFIG['sensor_columns']].isna().sum().sum()\n",
    "print(f\"üîç NaN values in sensor columns: {nan_count}\")\n",
    "\n",
    "if nan_count > 0:\n",
    "    # Fill NaN using forward fill then backward fill\n",
    "    df_clean = df_converted.copy()\n",
    "    df_clean[CONFIG['sensor_columns']] = df_clean[CONFIG['sensor_columns']].ffill().bfill()\n",
    "    \n",
    "    remaining_nan = df_clean[CONFIG['sensor_columns']].isna().sum().sum()\n",
    "    print(f\"‚úì After ffill+bfill: {remaining_nan} NaN remaining\")\n",
    "else:\n",
    "    df_clean = df_converted.copy()\n",
    "    print(\"‚úì No NaN values to handle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e6a36",
   "metadata": {},
   "source": [
    "## üìè Step 5: Normalization (StandardScaler)\n",
    "\n",
    "**What:** Normalize data using the **same scaler as training**.\n",
    "\n",
    "### Formula\n",
    "```\n",
    "normalized = (value - mean) / scale\n",
    "```\n",
    "\n",
    "### Important\n",
    "- Mean and scale come from `config.json` (saved during training)\n",
    "- Using different scaler = bad predictions!\n",
    "\n",
    "**Input:** `df_clean[sensor_columns]` + `config.json`  \n",
    "**Output:** `sensor_normalized` (range ~[-3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f698f4",
   "metadata": {},
   "source": [
    "### üìñ Why Use SAME Scaler as Training?\n",
    "\n",
    "**What StandardScaler Does:**\n",
    "```python\n",
    "normalized = (value - mean) / std\n",
    "```\n",
    "\n",
    "**During Training:**\n",
    "```python\n",
    "# Fit scaler on training data (all_users_data_labeled.csv)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(training_data)\n",
    "\n",
    "# This calculates:\n",
    "mean = [0.12, -0.08, 9.81, ...]  # Per sensor\n",
    "std = [2.34, 1.98, 0.87, ...]\n",
    "\n",
    "# Save these values\n",
    "config.json: { \"scaler_mean\": [...], \"scaler_scale\": [...] }\n",
    "```\n",
    "\n",
    "**During Production (MUST use same scaler):**\n",
    "```python\n",
    "# Load saved mean and std from training\n",
    "mean = load_from_config()  # Same values from training!\n",
    "std = load_from_config()\n",
    "\n",
    "# Apply to production data\n",
    "production_normalized = (production_data - mean) / std\n",
    "```\n",
    "\n",
    "**Why Same Scaler is Critical:**\n",
    "\n",
    "‚úÖ **Correct (same scaler):**\n",
    "```python\n",
    "Training:   (5.0 - 3.0) / 2.0 = 1.0\n",
    "Production: (5.0 - 3.0) / 2.0 = 1.0  ‚Üê Same normalized value!\n",
    "Model recognizes this ‚úì\n",
    "```\n",
    "\n",
    "‚ùå **Wrong (fit new scaler on production):**\n",
    "```python\n",
    "Training:   (5.0 - 3.0) / 2.0 = 1.0\n",
    "Production: (5.0 - 7.0) / 4.0 = -0.5  ‚Üê Different normalized value!\n",
    "Model confused ‚úó\n",
    "```\n",
    "\n",
    "**Real Example:**\n",
    "```\n",
    "Training: Az_norm = (9.8 - 9.81) / 0.5 = -0.02  ‚Üê Model learned \"standing\"\n",
    "Production (same scaler): Az_norm = (9.85 - 9.81) / 0.5 = 0.08  ‚Üê Recognized as \"standing\" ‚úì\n",
    "Production (new scaler): Az_norm = (9.85 - 10.2) / 0.8 = -0.44  ‚Üê Model thinks different activity ‚úó\n",
    "```\n",
    "\n",
    "**Where Scaler is Stored:**\n",
    "```\n",
    "data/prepared/config.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05791e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scaler parameters from training config\n",
    "scaler_config_path = DATA_PREPARED / 'config.json'\n",
    "\n",
    "if scaler_config_path.exists():\n",
    "    with open(scaler_config_path, 'r') as f:\n",
    "        scaler_config = json.load(f)\n",
    "    \n",
    "    scaler_mean = np.array(scaler_config['scaler_mean'])\n",
    "    scaler_scale = np.array(scaler_config['scaler_scale'])\n",
    "    \n",
    "    print(f\"‚úì Loaded scaler from: {scaler_config_path.name}\")\n",
    "    print(f\"  Mean: {scaler_mean}\")\n",
    "    print(f\"  Scale: {scaler_scale}\")\n",
    "    \n",
    "    # Apply normalization\n",
    "    sensor_data = df_clean[CONFIG['sensor_columns']].values\n",
    "    sensor_normalized = (sensor_data - scaler_mean) / scaler_scale\n",
    "    \n",
    "    print(f\"\\nüìä Normalized data:\")\n",
    "    print(f\"  Shape: {sensor_normalized.shape}\")\n",
    "    print(f\"  Range: [{sensor_normalized.min():.2f}, {sensor_normalized.max():.2f}]\")\n",
    "    print(f\"  Mean: {sensor_normalized.mean():.4f}\")\n",
    "else:\n",
    "    print(f\"‚ö† Scaler config not found at {scaler_config_path}\")\n",
    "    print(\"Using raw data without normalization\")\n",
    "    sensor_normalized = df_clean[CONFIG['sensor_columns']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca221a",
   "metadata": {},
   "source": [
    "## ü™ü Step 6: Create Sliding Windows\n",
    "\n",
    "**What:** Split continuous data into fixed-size windows for model input.\n",
    "\n",
    "### Parameters\n",
    "| Parameter | Value | Meaning |\n",
    "|-----------|-------|---------||\n",
    "| Window size | 200 | 200 timesteps = 4 seconds @ 50Hz |\n",
    "| Overlap | 50% | Each window shares 100 samples with next |\n",
    "| Step | 100 | Start of next window = current + 100 |\n",
    "\n",
    "### Visualization\n",
    "```\n",
    "Data:     [--------------------181,699 samples--------------------]\n",
    "Window 1: [######]\n",
    "Window 2:    [######]\n",
    "Window 3:       [######]\n",
    "...\n",
    "```\n",
    "\n",
    "**Input:** `sensor_normalized` (181,699 √ó 6)  \n",
    "**Output:** `X_prod` (N √ó 200 √ó 6), `window_metadata`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f32dc5",
   "metadata": {},
   "source": [
    "### üìñ Understanding Windows, Overlap, and Hz\n",
    "\n",
    "**What is a Window?**\n",
    "- Fixed-size chunk of continuous sensor data\n",
    "- Like a 4-second video clip from a longer video\n",
    "- Model analyzes one window at a time\n",
    "\n",
    "**Window Size = 200 samples:**\n",
    "```\n",
    "200 samples √∑ 50 Hz = 4 seconds of data\n",
    "```\n",
    "- ‚úÖ Long enough to capture activity patterns (walking = 2 steps/sec)\n",
    "- ‚úÖ Short enough to detect activity changes\n",
    "- ‚úÖ Model architecture expects exactly 200 timesteps\n",
    "\n",
    "**What is Overlap = 50%?**\n",
    "```\n",
    "Window 1:  [########]          samples 0-199\n",
    "           0       199\n",
    "Window 2:      [########]      samples 100-299\n",
    "              100       299\n",
    "Window 3:          [########]  samples 200-399\n",
    "                  200       399\n",
    "```\n",
    "- Each window shares 100 samples (50%) with next window\n",
    "- **Why?** Captures transitions between activities\n",
    "- Without overlap: might miss moment when activity changes!\n",
    "\n",
    "**What is Hz (Hertz)?**\n",
    "- Sampling rate = samples per second\n",
    "- 50 Hz = 50 samples every second\n",
    "- 1 sample every 0.02 seconds (20 milliseconds)\n",
    "\n",
    "**Why 50 Hz?**\n",
    "- ‚úÖ Human activities: Walking ~2 Hz, Running ~3 Hz\n",
    "- ‚úÖ 50 Hz captures all movements well (25√ó faster than walking)\n",
    "- ‚úÖ Standard in activity recognition research\n",
    "\n",
    "**Step = 100 samples:**\n",
    "- Distance between window starts\n",
    "- Step = Window_size √ó (1 - Overlap)\n",
    "- Step = 200 √ó 0.5 = 100\n",
    "\n",
    "**Result:**\n",
    "```python\n",
    "From 181,699 samples ‚Üí ~1,772 windows\n",
    "Each window = 4 seconds of sensor data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size, step):\n",
    "    \"\"\"\n",
    "    Create sliding windows from sensor data.\n",
    "    \n",
    "    Args:\n",
    "        data: numpy array of shape (samples, features)\n",
    "        window_size: number of timesteps per window\n",
    "        step: step size between windows\n",
    "        \n",
    "    Returns:\n",
    "        windows: numpy array of shape (n_windows, window_size, features)\n",
    "        metadata: list of dicts with window info\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    metadata = []\n",
    "    \n",
    "    n_samples = len(data)\n",
    "    n_windows = (n_samples - window_size) // step + 1\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        start = i * step\n",
    "        end = start + window_size\n",
    "        window = data[start:end]\n",
    "        \n",
    "        # Skip if window has NaN\n",
    "        if np.isnan(window).any():\n",
    "            continue\n",
    "            \n",
    "        windows.append(window)\n",
    "        metadata.append({\n",
    "            'window_index': len(windows) - 1,\n",
    "            'start_sample': start,\n",
    "            'end_sample': end,\n",
    "        })\n",
    "    \n",
    "    return np.array(windows), metadata\n",
    "\n",
    "# Create windows\n",
    "X_prod, window_metadata = create_windows(\n",
    "    sensor_normalized,\n",
    "    CONFIG['window_size'],\n",
    "    CONFIG['step']\n",
    ")\n",
    "\n",
    "print(f\"‚úì Windows created:\")\n",
    "print(f\"  Shape: {X_prod.shape}\")\n",
    "print(f\"  Format: (windows, timesteps, features)\")\n",
    "print(f\"  Total: {len(X_prod):,} windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7a2e8",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Preprocessed Data\n",
    "\n",
    "**What:** Export windows and metadata for model inference.\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Format | Contains |\n",
    "|------|--------|----------|\n",
    "| `production_X.npy` | NumPy | Windows array (N, 200, 6) |\n",
    "| `production_metadata.json` | JSON | Pipeline info, unit detection result |\n",
    "\n",
    "### Metadata Contents\n",
    "- Source file name\n",
    "- Units detected (milliG / m/s¬≤)\n",
    "- Conversion applied (true/false)\n",
    "- Window parameters\n",
    "- Total windows created\n",
    "\n",
    "**Input:** `X_prod`, pipeline info  \n",
    "**Output:** Files in `data/prepared/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feeb8ff",
   "metadata": {},
   "source": [
    "### üìñ Why .npy Format?\n",
    "\n",
    "**What is production_X.npy?**\n",
    "- NumPy binary file containing windowed arrays\n",
    "- Shape: **(1772, 200, 6)** = 1772 windows √ó 200 timesteps √ó 6 sensors\n",
    "- Ready for direct model input\n",
    "\n",
    "**Why .npy vs other formats?**\n",
    "\n",
    "| Format | Size | Load Speed | Preserves Shape | Use Case |\n",
    "|--------|------|------------|-----------------|----------|\n",
    "| **.npy** | 8.5 MB | ‚ö° 0.1s | ‚úÖ Yes | Model input |\n",
    "| **.csv** | 45 MB | üêå 2s | ‚ùå No | Human reading |\n",
    "| **.pkl** | 9 MB | ‚ö° 0.2s | ‚úÖ Yes | Python objects |\n",
    "\n",
    "**Advantages:**\n",
    "1. **Fast loading:** 20√ó faster than CSV\n",
    "2. **Smaller size:** 80% smaller than CSV\n",
    "3. **Preserves structure:** No reshaping needed\n",
    "4. **Type safety:** Keeps float32/float64\n",
    "5. **Direct use:** `model.predict(np.load('X.npy'))`\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Save\n",
    "np.save('production_X.npy', X)  # Shape: (1772, 200, 6)\n",
    "\n",
    "# Load (no reshaping needed!)\n",
    "X = np.load('production_X.npy')\n",
    "predictions = model.predict(X)  # Works directly ‚úì\n",
    "```\n",
    "\n",
    "**CSV would require:**\n",
    "```python\n",
    "# Load CSV\n",
    "df = pd.read_csv('production.csv')  # 2 seconds, loses shape\n",
    "X = df.values.reshape(1772, 200, 6)  # Manual reshaping needed\n",
    "predictions = model.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164357ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "DATA_PREPARED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save windows\n",
    "output_X = DATA_PREPARED / 'production_X.npy'\n",
    "np.save(output_X, X_prod)\n",
    "print(f\"‚úì Saved: {output_X}\")\n",
    "print(f\"  Shape: {X_prod.shape}\")\n",
    "print(f\"  Size: {X_prod.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save metadata\n",
    "output_meta = DATA_PREPARED / 'production_metadata.json'\n",
    "meta_summary = {\n",
    "    'created': datetime.now().isoformat(),\n",
    "    'source_file': str(CONFIG['input_file'].name),\n",
    "    'units_detected': units_detected,\n",
    "    'conversion_applied': conversion_applied,\n",
    "    'conversion_factor': CONFIG['conversion_factor'] if conversion_applied else None,\n",
    "    'window_size': CONFIG['window_size'],\n",
    "    'overlap': CONFIG['overlap'],\n",
    "    'total_windows': len(X_prod),\n",
    "    'original_samples': len(df),\n",
    "}\n",
    "\n",
    "with open(output_meta, 'w') as f:\n",
    "    json.dump(meta_summary, f, indent=2)\n",
    "print(f\"‚úì Saved: {output_meta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105307b",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 8: Summary & Next Steps\n",
    "\n",
    "**What:** Print final summary and show what to do next.\n",
    "\n",
    "### Pipeline Complete!\n",
    "```\n",
    "Input:  sensor_fused_50Hz.csv (181,699 samples)\n",
    "Output: production_X.npy (~1,772 windows)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "1. **Load model:** `keras.models.load_model('model.keras')`\n",
    "2. **Run inference:** `predictions = model.predict(X_prod)`\n",
    "3. **Analyze results:** Check prediction distribution & confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüì• Input:\")\n",
    "print(f\"  File: {CONFIG['input_file'].name}\")\n",
    "print(f\"  Samples: {len(df):,}\")\n",
    "print(f\"\\nüîç Unit Detection:\")\n",
    "print(f\"  Detected: {units_detected}\")\n",
    "print(f\"  Converted: {conversion_applied}\")\n",
    "if conversion_applied:\n",
    "    print(f\"  Factor: {CONFIG['conversion_factor']}\")\n",
    "print(f\"\\nüì§ Output:\")\n",
    "print(f\"  Windows: {len(X_prod):,}\")\n",
    "print(f\"  Shape: {X_prod.shape}\")\n",
    "print(f\"  Files:\")\n",
    "print(f\"    - production_X.npy\")\n",
    "print(f\"    - production_metadata.json\")\n",
    "print(f\"\\nüöÄ Next: Load model and run inference\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
