# Unlabeled Data Evaluation Framework for HAR MLOps

> **ğŸ“ Summary:** This document provides a scientifically defensible framework for evaluating model predictions on **unlabeled deployment data**. It explains why accuracy cannot be computed without labels, what metrics CAN be computed, and how to obtain estimated accuracy through minimal labeling.

**Version:** 1.0  
**Date:** January 15, 2026  
**Author:** Master Thesis MLOps Project

---

## Table of Contents
1. [Why We Cannot Compute Accuracy Without Labels](#1-why-we-cannot-compute-accuracy-without-labels)
2. [What We CAN Evaluate on Unlabeled Data](#2-what-we-can-evaluate-on-unlabeled-data)
3. [Three-Layer Monitoring Framework](#3-three-layer-monitoring-framework)
4. [Metrics Interpretation Guide](#4-metrics-interpretation-guide)
5. [Estimated Accuracy via Minimal Labeling](#5-estimated-accuracy-via-minimal-labeling)
6. [Implementation Guide](#6-implementation-guide)
7. [MLflow Integration](#7-mlflow-integration)
8. [References](#8-references)

---

## 1. Why We Cannot Compute Accuracy Without Labels

### The Fundamental Problem

**Accuracy** is defined as:

$$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Predictions}}$$

To compute this, we need to know:
1. **Predicted labels** (from the model) âœ… Available
2. **True labels** (ground truth) âŒ **NOT Available** for deployment data

Without ground truth labels, we **cannot know** if a prediction is correct or incorrect. Therefore:

> **Scientific Fact:** Accuracy, Precision, Recall, F1-Score, and Confusion Matrices are **undefined** for unlabeled data. Any system that reports these metrics on unlabeled data is either:
> 1. Using pseudo-labels (which are assumptions, not ground truth)
> 2. Reporting metrics from a validation set (not the current data)
> 3. Faking the metrics (scientifically invalid)

### What This Means for This Pipeline

| Data Type | Location | Labels | Can Compute Accuracy? |
|-----------|----------|--------|----------------------|
| Training Data | `data/raw/all_users_data_labeled.csv` | âœ… Yes | âœ… Yes |
| Validation Data | Split from training | âœ… Yes | âœ… Yes |
| Production Data | `data/prepared/production_X.npy` | âŒ No | âŒ **No** |
| New Garmin CSVs | `decoded_csv_files/` | âŒ No | âŒ **No** |

---

## 2. What We CAN Evaluate on Unlabeled Data

Although we cannot compute accuracy, we CAN compute **label-free quality metrics** that indicate:
- **Model confidence and uncertainty**
- **Temporal plausibility of predictions**
- **Distribution shift from training data**
- **Signal quality and sensor integrity**

### 2.1 Confidence & Uncertainty Metrics

These metrics come from the **model's output probabilities** (softmax outputs):

| Metric | Formula | What It Tells Us |
|--------|---------|------------------|
| **Max Probability** | $p_{max} = \max_i p_i$ | How confident the model is in its top prediction |
| **Entropy** | $H = -\sum p_i \log p_i$ | Overall uncertainty (higher = more uncertain) |
| **Margin** | $m = p_{top1} - p_{top2}$ | Difference between top two predictions |
| **Energy Score** | $E = -\log \sum \exp(z_i)$ | OOD detection score (requires logits, not probs) |

**Interpretation:**
- High confidence ($p_{max} > 0.9$) â†’ Model is certain (but not necessarily correct!)
- Low confidence ($p_{max} < 0.5$) â†’ Model is uncertain â†’ Flag for review
- Near-zero margin â†’ Model is confused between two classes

**Critical Note:** High confidence does NOT guarantee correctness. Neural networks are often **overconfident**. This is why we need calibration metrics (see Section 2.4).

### 2.2 Temporal Plausibility Metrics

These metrics analyze **sequences of predictions** to detect unrealistic behavior:

| Metric | Definition | Threshold |
|--------|------------|-----------|
| **Flip Rate** | % of consecutive windows with different predictions | < 30% is typical |
| **Dwell Time** | Average duration of each predicted activity | > 4 seconds expected |
| **Transition Violations** | Count of "impossible" transitions (e.g., sitting â†’ nail_biting â†’ sitting in 1 second) | Should be 0 |
| **Activity Entropy** | Diversity of predicted activities per session | Context-dependent |

**Why This Matters:**
- Human activities have **temporal structure** (you don't switch from sitting to standing 10 times per second)
- High flip rate suggests the model is unstable/uncertain
- Impossible transitions suggest OOD data or model confusion

### 2.3 Distribution Drift Metrics

These metrics compare **production data** to **training data baselines**:

| Metric | Method | What It Detects |
|--------|--------|-----------------|
| **Feature Drift** | KS test, Wasserstein distance on raw features | Sensor characteristics changed |
| **Mean/Std Shift** | Compare Î¼ and Ïƒ per channel | Systematic bias or variability change |
| **Embedding Drift** | Compare model embeddings | Semantic shift in data |
| **Variance Collapse** | Std deviation near zero | Idle/stationary data or sensor failure |

**Interpretation:**
- KS test p-value < 0.05 â†’ Significant drift detected
- Wasserstein distance > threshold â†’ Feature distribution changed
- Variance collapse â†’ Data is idle or sensor is malfunctioning

### 2.4 Signal Quality Metrics (Sensor Integrity)

| Metric | Check | Implication |
|--------|-------|-------------|
| **Sampling Rate** | Actual Hz vs expected 50 Hz | Resampling issues |
| **Missing Values** | % NaN per channel | Data integrity |
| **Clipping** | Values at min/max sensor limits | Sensor saturation |
| **Gravity Check** | Az mean â‰ˆ -9.8 m/sÂ² | Unit conversion correct |
| **Noise Floor** | Minimum variance | Sensor working |

---

## 3. Three-Layer Monitoring Framework

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POST-INFERENCE MONITORING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  LAYER 1: Per-Window Confidence/Uncertainty                         â”‚
â”‚  â”œâ”€â”€ Max softmax probability                                        â”‚
â”‚  â”œâ”€â”€ Entropy of probabilities                                       â”‚
â”‚  â”œâ”€â”€ Margin (top1 - top2)                                          â”‚
â”‚  â””â”€â”€ Flag uncertain windows (confidence < threshold)                â”‚
â”‚                                                                      â”‚
â”‚  LAYER 2: Sequence Temporal Plausibility                            â”‚
â”‚  â”œâ”€â”€ Flip rate (class changes / total windows)                      â”‚
â”‚  â”œâ”€â”€ Dwell time per activity                                        â”‚
â”‚  â”œâ”€â”€ Transition matrix validation                                   â”‚
â”‚  â””â”€â”€ Smoothing recommendations (majority voting, HMM)               â”‚
â”‚                                                                      â”‚
â”‚  LAYER 3: Batch-Level Drift vs Training Baseline                    â”‚
â”‚  â”œâ”€â”€ Feature drift (KS test, Wasserstein)                          â”‚
â”‚  â”œâ”€â”€ Embedding drift (cosine similarity)                            â”‚
â”‚  â”œâ”€â”€ Sensor integrity (sampling rate, missingness, clipping)        â”‚
â”‚  â””â”€â”€ Gating decision (PASS / WARN / BLOCK)                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer 1: Per-Window Confidence/Uncertainty

**Purpose:** Flag individual predictions that may be unreliable.

**Inputs:**
- Softmax probabilities: shape `(n_windows, 11)`
- (Optional) Logits for energy score

**Outputs:**
- `confidence_scores.csv`: Per-window confidence metrics
- `uncertain_windows.csv`: Windows flagged for review
- Summary statistics in MLflow

**Thresholds (configurable):**
| Metric | Threshold | Action |
|--------|-----------|--------|
| `confidence < 0.50` | Uncertain | Flag for review |
| `confidence < 0.30` | Very uncertain | Consider excluding |
| `entropy > 2.0` | High uncertainty | Flag for review |
| `margin < 0.10` | Ambiguous | Two classes equally likely |

### Layer 2: Sequence Temporal Plausibility

**Purpose:** Detect unrealistic prediction sequences.

**Inputs:**
- Predicted class sequence: shape `(n_windows,)`
- Timestamps or window IDs

**Outputs:**
- `temporal_analysis.json`: Flip rate, dwell times, transition counts
- Warnings for unrealistic patterns

**Metrics:**
```python
flip_rate = (n_class_changes) / (n_windows - 1)
mean_dwell_time = mean([duration for each continuous activity bout])
transition_matrix[i][j] = count of transitions from class i to class j
```

**Example Impossible Transitions (HAR-specific):**
- `sitting` â†’ `standing` â†’ `sitting` in < 2 seconds (unrealistic)
- Any anxiety activity lasting < 1 second (too short)

### Layer 3: Batch-Level Drift/OOD Detection

**Purpose:** Detect if production data distribution differs from training.

**Inputs:**
- Production data: `(n_windows, 200, 6)` NumPy array
- Training baseline: `baseline_stats.json` (mean, std, percentiles)
- (Optional) Training embeddings: `baseline_embeddings.npz`

**Outputs:**
- `drift_report.json`: Per-channel drift scores
- Gating decision: PASS / WARN / BLOCK

**Metrics:**
```python
# Feature-level drift (per channel)
for channel in ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']:
    ks_stat, ks_pvalue = scipy.stats.ks_2samp(train_channel, prod_channel)
    wasserstein = scipy.stats.wasserstein_distance(train_channel, prod_channel)
    mean_diff = abs(train_mean - prod_mean)
    std_ratio = prod_std / train_std

# Gating logic
if any(ks_pvalue < 0.01) or wasserstein > threshold:
    decision = "WARN" or "BLOCK"
```

---

## 4. Metrics Interpretation Guide

### What These Metrics PROVE

| Metric Category | What It Proves | Scientific Basis |
|-----------------|----------------|------------------|
| Confidence/Uncertainty | Model's internal certainty about prediction | Information theory (entropy) |
| Temporal Plausibility | Predictions follow realistic activity patterns | Domain knowledge (HAR) |
| Drift Detection | Data distribution differs from training | Statistical testing (KS, Wasserstein) |
| Sensor Integrity | Data collection was successful | Physics (gravity, units) |

### What These Metrics DO NOT PROVE

| Claim | Why It's Invalid |
|-------|------------------|
| "High confidence = correct prediction" | Neural networks are often overconfident |
| "No drift = high accuracy" | Model could still be systematically wrong |
| "Plausible sequence = correct predictions" | Wrong predictions can still be temporally plausible |
| "All checks pass = deploy safely" | Still need minimal labeling to estimate accuracy |

### Decision Matrix

| Layer 1 | Layer 2 | Layer 3 | Overall Decision |
|---------|---------|---------|------------------|
| âœ… High confidence | âœ… Plausible | âœ… No drift | PASS (but still get labeled samples!) |
| âš ï¸ Some uncertain | âœ… Plausible | âœ… No drift | WARN - Review uncertain windows |
| âš ï¸ Some uncertain | âš ï¸ Unstable | âœ… No drift | WARN - Model may be confused |
| Any | Any | âŒ Drift detected | BLOCK - Investigate data pipeline |
| âŒ Low confidence | âŒ Unstable | Any | BLOCK - Serious issues |

---

## 5. Estimated Accuracy via Minimal Labeling

### Why Minimal Labeling?

Without labels, we cannot know true accuracy. But we can **estimate** accuracy by labeling a **small random sample** of deployment data.

### Strategy 1: Random Sampling

**Protocol:**
1. From each batch of N windows, randomly sample k windows (e.g., k=50)
2. Have a human label these windows (ground truth)
3. Compute accuracy on this sample
4. Calculate confidence interval

**Statistical Basis:**
- Sample accuracy $\hat{a}$ is an unbiased estimator of true accuracy
- 95% confidence interval: $\hat{a} \pm 1.96 \sqrt{\frac{\hat{a}(1-\hat{a})}{k}}$

**Example:**
- Sample 50 windows, 42 correct â†’ $\hat{a} = 84\%$
- 95% CI: $84\% \pm 10.2\%$ â†’ True accuracy likely 74-94%
- Sample 200 windows for tighter bounds

### Strategy 2: Active Sampling (Uncertainty-Based)

**Protocol:**
1. Sort windows by uncertainty (low confidence first)
2. Label the most uncertain windows
3. This gives **worst-case accuracy** estimate
4. Also label some high-confidence windows to check calibration

**Rationale:**
- Uncertain predictions are more likely to be wrong
- Labeling these first gives faster insight into model failures
- Reveals confusion patterns between specific classes

### Strategy 3: Sentinel Protocol (Controlled Sessions)

**Protocol:**
1. Weekly: Record a **controlled session** (5-10 minutes)
2. Perform known activities in a scripted sequence
3. These sessions have **ground truth labels**
4. Run inference and compute accuracy on sentinel data

**Benefits:**
- Direct accuracy measurement on real deployment conditions
- Detects degradation over time
- Controls for device/wearer variability

**Implementation:**
```yaml
sentinel_session:
  duration_minutes: 5
  activities:
    - name: sitting
      duration_seconds: 60
    - name: standing
      duration_seconds: 60
    - name: nail_biting
      duration_seconds: 30
    - name: ear_rubbing
      duration_seconds: 30
    - name: walking (transition)
      duration_seconds: 60
  frequency: weekly
```

---

## 6. Implementation Guide

### File Structure

```
scripts/
â”œâ”€â”€ post_inference_monitoring.py    # Main monitoring script (NEW)
â”œâ”€â”€ build_training_baseline.py      # Build baseline from training data (NEW)
â”œâ”€â”€ inference_smoke.py              # Existing smoke test
â””â”€â”€ preprocess_qc.py                # Existing QC

src/
â”œâ”€â”€ monitoring/                     # NEW monitoring module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ confidence_metrics.py       # Layer 1 metrics
â”‚   â”œâ”€â”€ temporal_metrics.py         # Layer 2 metrics
â”‚   â”œâ”€â”€ drift_metrics.py            # Layer 3 metrics
â”‚   â””â”€â”€ gating.py                   # Decision logic

data/prepared/
â”œâ”€â”€ baseline_stats.json             # Training feature statistics (NEW)
â”œâ”€â”€ baseline_embeddings.npz         # Training embeddings (optional, NEW)
â””â”€â”€ config.json                     # Existing scaler config

reports/
â””â”€â”€ monitoring/                     # NEW output directory
    â”œâ”€â”€ 2026-01-15_batch001/
    â”‚   â”œâ”€â”€ confidence_report.json
    â”‚   â”œâ”€â”€ temporal_report.json
    â”‚   â”œâ”€â”€ drift_report.json
    â”‚   â””â”€â”€ summary.json
    â””â”€â”€ dashboards/
        â””â”€â”€ monitoring_dashboard.html
```

### Gating Rules

**In MLflow Run:**
```python
# Tag run with monitoring status
if drift_score > DRIFT_THRESHOLD:
    mlflow.set_tag("monitoring_status", "DRIFT_DETECTED")
    mlflow.set_tag("needs_review", "true")
    
if uncertain_ratio > 0.3:  # >30% uncertain windows
    mlflow.set_tag("monitoring_status", "HIGH_UNCERTAINTY")
    mlflow.set_tag("needs_review", "true")
```

**In Pipeline:**
```python
if monitoring_result.decision == "BLOCK":
    raise PipelineGatingError("Monitoring checks failed - see drift_report.json")
```

---

## 7. MLflow Integration

### Metrics to Log

| Metric | Type | Key |
|--------|------|-----|
| Mean confidence | Metric | `monitoring/mean_confidence` |
| Uncertain ratio | Metric | `monitoring/uncertain_ratio` |
| Flip rate | Metric | `monitoring/flip_rate` |
| KS test p-value (min) | Metric | `monitoring/ks_pvalue_min` |
| Drift detected | Tag | `drift_detected` |
| Monitoring status | Tag | `monitoring_status` |

### Artifacts to Log

| Artifact | Description |
|----------|-------------|
| `confidence_report.json` | Per-window confidence scores |
| `temporal_report.json` | Sequence analysis |
| `drift_report.json` | Drift detection results |
| `confidence_histogram.png` | Visualization |

---

## 8. References

### Papers Used in This Framework

| Paper | Method | Applied Here |
|-------|--------|--------------|
| `NeurIPS-2020-energy-based-out-of-distribution-detection-Paper.pdf` | Energy Score for OOD | Layer 3 (optional) |
| `NeurIPS-2021-adaptive-conformal-inference-under-distribution-shift-Paper.pdf` | Conformal prediction under shift | Future extension |
| `When Does Optimizing a Proper Loss Yield Calibration.pdf` | Calibration theory | ECE metric |
| `MACHINE LEARNING OPERATIONS A SURVEY ON MLOPS.pdf` | MLOps monitoring best practices | Overall framework |
| `Are Anxiety Detection Models Generalizable-A Cross-Activity...pdf` | Cross-domain HAR | Drift detection motivation |
| `Resilience of Machine Learning Models in Anxiety Detection...pdf` | Noise robustness in HAR | Sensor integrity checks |
| `Domain Adaptation for Inertial Measurement Unit-based Human.pdf` | IMU domain shift | Feature drift metrics |

### Google Scholar Queries for Further Reading

If you need additional papers, search for:

1. `"expected calibration error" neural network confidence`
2. `"energy score" out-of-distribution detection`
3. `"drift detection" machine learning production`
4. `"conformal prediction" time series classification`
5. `"human activity recognition" domain adaptation wearable`
6. `"uncertainty quantification" deep learning classification`
7. `"temporal consistency" activity recognition prediction`
8. `"Kolmogorov-Smirnov test" distribution shift detection`
9. `"minimal labeling" active learning deployment`
10. `"MLOps monitoring" model performance production`

---

## Appendix A: Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNLABELED EVALUATION QUICK REF                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  âŒ CANNOT compute on unlabeled data:                               â”‚
â”‚     â€¢ Accuracy, Precision, Recall, F1-Score                         â”‚
â”‚     â€¢ Confusion Matrix                                              â”‚
â”‚     â€¢ Per-class accuracy                                            â”‚
â”‚                                                                      â”‚
â”‚  âœ… CAN compute on unlabeled data:                                  â”‚
â”‚     â€¢ Confidence scores (max prob, entropy, margin)                 â”‚
â”‚     â€¢ Temporal plausibility (flip rate, dwell time)                 â”‚
â”‚     â€¢ Drift metrics (KS test, Wasserstein, mean shift)              â”‚
â”‚     â€¢ Sensor quality (sampling rate, missingness)                   â”‚
â”‚                                                                      â”‚
â”‚  ğŸ“Š To get ESTIMATED ACCURACY:                                      â”‚
â”‚     1. Random sample: Label 50-200 windows â†’ compute accuracy       â”‚
â”‚     2. Active sample: Label uncertain windows â†’ worst-case          â”‚
â”‚     3. Sentinel: Weekly controlled session â†’ direct measurement     â”‚
â”‚                                                                      â”‚
â”‚  ğŸš¨ Gating Rules:                                                   â”‚
â”‚     â€¢ Drift detected (KS p < 0.01) â†’ WARN/BLOCK                     â”‚
â”‚     â€¢ >30% uncertain windows â†’ WARN                                 â”‚
â”‚     â€¢ Variance collapse â†’ BLOCK (sensor issue)                      â”‚
â”‚     â€¢ All pass â†’ PASS (still need labeled samples!)                 â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
