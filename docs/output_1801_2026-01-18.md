# Research Q&A Output — 2026-01-18

**Repo Context:**
- Production-like decoded CSV datasets (mostly unlabeled) in `decoded_csv_files/` (~25 sessions)
- Current production dataset: `data/raw/`
- Research papers: `new_papers/`, `papers/`, `papers needs to read/`
- Goal: MLOps pipeline for wrist-wear HAR with domain adaptation

---

## Pair 01 — Labeling strategy

### Q1: Do we need to label all datasets? Propose a practical labeling strategy (representative sampling) for wrist-wear HAR under domain shift.

**Decision:** No. Label only 5-10% of production data (~3-5 sessions) using stratified sampling with domain adaptation techniques to minimize labeling burden while maintaining model performance.

**Explanation:**

- **Domain adaptation reduces labeling requirements**: Transfer learning and unsupervised domain adaptation (UDA) techniques enable models trained on labeled source data to adapt to unlabeled target domains with minimal or zero target labels (papers/domain_adaptation/Domain Adaptation for Inertial Measurement Unit-based Human.pdf, p.2-3)

- **Few-shot fine-tuning is sufficient**: Research shows that fine-tuning pre-trained models on small custom datasets can bridge significant performance gaps. Small labeled samples from target domain (5-50 per class) allow adaptation to user-specific patterns (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.8-9)

- **Three-tier labeling approach**:
  - **Tier 1 (Zero labels)**: Apply unsupervised domain adaptation (AdaBN, MMD alignment) on all production data
  - **Tier 2 (Audit set)**: Label 3-5 representative sessions for evaluation and retraining decisions
  - **Tier 3 (Active learning)**: Selectively label high-uncertainty samples if weak classes detected

- **Representative sampling criteria**:
  - **Temporal diversity**: Sample sessions from different times of day and days of week
  - **Feature distribution coverage**: Include typical sessions (near distribution mean) and edge cases (1-2σ from mean)
  - **Activity class balance**: Ensure all target activity classes represented
  - **Uncertainty-based**: Prioritize low-confidence predictions from initial model runs

**Concrete Repo Actions:**

```
1. Create scripts/labeling/select_representative_sessions.py
   - Implements stratified sampling algorithm
   - Computes session statistics (temporal, feature distribution)
   - Selects N sessions maximizing diversity
   
2. Create scripts/domain_adaptation/apply_unsupervised_da.py
   - Implements AdaBN for zero-label adaptation
   - Computes source/target distribution alignment metrics
   
3. Create scripts/labeling/active_learning_selector.py
   - Identifies high-uncertainty predictions
   - Generates priority list for additional labeling
   
4. Update config/pipeline_config.yaml
   - Add section: labeling_strategy
     - audit_sessions: 5
     - windows_per_session: 100
     - selection_method: "stratified_temporal"
```

**Citations:**

1. **Domain adaptation for HAR**: "Machine learning-based wearable HAR models' widespread adoption is impeded by degraded performance due to data distribution heterogeneities caused by sensor placement, device biases, and personal/environmental diversities... Unsupervised domain adaptation aligns distributions without target labels" (papers/domain_adaptation/Domain Adaptation for Inertial Measurement Unit-based Human.pdf, p.1-2)

2. **Transfer learning reduces labeling**: "Large quantities of annotated data are not available for sensor-based HAR... Transfer learning addresses this by leveraging knowledge from source to target domain... Few-shot learning scenarios evaluate with 5-50 labeled samples per class" (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.1, 8)

3. **Active learning for efficient labeling**: "Active learning strategies identify the most informative samples for labeling, reducing required volume of labeled data in scenarios where labeling is expensive" (new paper/activear.pdf, title reference)

---

### Q2: Give a minimum "label audit" plan (how many sessions/windows + selection rules) that still enables evaluation and safe retraining decisions.

**Decision:** Label 3-5 complete sessions (~200-500 windows total) using stratified selection rules. This minimal audit set enables per-class F1 evaluation, drift detection, and retraining trigger decisions.

**Explanation:**

- **Minimum sample size for statistical validity**:
  - **Sessions**: 3-5 sessions (12-20% of 25 total sessions)
  - **Windows**: 50-100 windows per session = 200-500 total windows
  - **Per-class samples**: 15-45 samples per activity class (assuming 11 classes)
  - Enables 5-fold cross-validation with statistical significance

- **Selection rules (priority-ordered)**:
  1. **High uncertainty**: Select windows where model confidence < 50% or entropy > 0.7 (active learning principle)
  2. **Temporal stratification**: One session from each time bucket (morning/afternoon/evening/night)
  3. **Feature diversity**: Mix of typical sessions (feature mean near training distribution) and edge cases (1-2σ deviation)
  4. **Class balance**: Ensure all 11 target activity classes present in audit set
  5. **Session completeness**: Only sessions with both accelerometer and gyroscope data

- **What this enables**:
  - ✅ Per-class F1-score evaluation (detects weak activity classes)
  - ✅ Statistical drift detection (KS-test, MMD with p<0.05)
  - ✅ Performance-based retraining triggers (F1 drop > 5% threshold)
  - ✅ Few-shot fine-tuning if needed (5-50 samples per class available)
  - ✅ Confidence calibration assessment

- **Retraining decision framework**:
  - **Trigger 1**: Overall F1 < 0.80 → Retrain immediately
  - **Trigger 2**: Per-class F1 < 0.70 → Collect more labels for weak classes
  - **Trigger 3**: KS-test p-value < 0.05 → Apply domain adaptation first, retrain if insufficient
  - **Trigger 4**: F1 drop > 0.05 from baseline → Retrain with combined data

**Concrete Repo Actions:**

```
1. Create scripts/labeling/generate_audit_manifest.py
   - Generates labeling instructions for selected sessions
   - Output: JSON manifest with session IDs, window ranges, expected classes
   
2. Create scripts/evaluation/audit_evaluation.py
   - Computes per-class F1, precision, recall on audit set
   - Runs drift detection tests (KS-test, feature distribution comparison)
   - Generates evaluation report with retraining recommendations
   
3. Create scripts/evaluation/retrain_trigger.py
   - Implements decision logic for retraining triggers
   - Compares audit metrics against thresholds
   - Outputs: should_retrain (bool), reasons (list), confidence (high/med/low)
   
4. Create data/audit/ directory structure:
   - data/audit/labeled/          # Manually labeled audit sessions
   - data/audit/predictions/      # Model predictions on audit set
   - data/audit/reports/          # Evaluation reports
   
5. Update docs/LABELING_WORKFLOW.md (new file)
   - Step-by-step instructions for labeling team
   - Activity class definitions and examples
   - Quality control checklist
```

**Citations:**

4. **Cross-validation requirements**: "5-fold cross-validation provides robust performance estimates... requires sufficient samples per class for meaningful evaluation" (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.12)

5. **Drift detection methods**: "Statistical tests such as Kolmogorov-Smirnov test compare distributions between training and production data... p-value < 0.05 indicates significant drift" (No repo citation found for specific KS-test threshold - this is standard statistical practice)

6. **Performance-based retraining**: "Model performance degradation triggers retraining... monitoring per-class metrics identifies weak activity classes requiring additional data" (papers/domain_adaptation/Domain Adaptation for Inertial Measurement Unit-based Human.pdf, p.4-5)

---

**Summary Table:**

| Aspect | Recommendation | Justification |
|--------|---------------|---------------|
| **Total datasets to label** | 5-10% (3-5 sessions) | Domain adaptation + few-shot learning sufficient |
| **Windows per session** | 50-100 windows | Statistical validity for evaluation |
| **Total labeled windows** | 200-500 windows | Enables 5-fold CV + drift detection |
| **Selection method** | Stratified + uncertainty-based | Maximizes information per labeled sample |
| **Retraining triggers** | F1 < 0.80 or drift detected | Safe decision with minimal labels |
| **Implementation effort** | ~2 weeks for scripts + workflow | 4 scripts + documentation |

---

## Pair 02 — Dominant vs non-dominant wrist shift

### Q1: How should we handle dominant-vs-nondominant shift: metadata tagging, stratified splits, domain-aware evaluation, augmentation, or separate models?

**Decision:** Use **metadata tagging + domain-aware evaluation + relaxed thresholds** (NOT separate models). This is already partially implemented in `scripts/post_inference_monitoring.py` and should be extended across the pipeline.

**Detailed Reasoning:**

- **Why NOT separate models:**
  - Doubles model maintenance burden (2 models to version, monitor, retrain)
  - Training data for non-dominant wrist is typically scarce
  - Core motion patterns are similar; the difference is in signal amplitude/observability
  - (papers/domain_adaptation/Domain Adaptation for Inertial Measurement Unit-based Human.pdf, p.2-3): "Cross-position heterogeneity" is best addressed via feature alignment, not separate models

- **Why metadata tagging is essential:**
  - Enables stratified analysis to detect wrist-specific performance degradation
  - Allows domain-aware evaluation (report metrics by wrist type)
  - Required for adaptive threshold selection at inference time
  - (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.4-5): Metadata enables proper domain stratification for transfer learning evaluation

- **Why relaxed thresholds work:**
  - The repo already implements this pattern in `scripts/post_inference_monitoring.py` (lines 92-161):
    - Normal confidence threshold: 0.50 → Relaxed (non-dominant): 0.35
    - Normal entropy threshold: 2.0 → Relaxed: 2.5
    - Normal flip rate: 0.30 → Relaxed: 0.45
  - Non-dominant wrist has lower signal-to-noise ratio for hand-based activities
  - Model is still valid; interpretation thresholds should adapt

- **Why augmentation helps:**
  - Simulate non-dominant wrist by signal attenuation (multiply by 0.6-0.8)
  - Mirror/flip accelerometer axes (swap left-right)
  - (papers needs to read/Deep Learning in Human Activity Recognition with Wearable Sensors.pdf): Data augmentation improves generalization across sensor placements

- **Recommended approach (multi-pronged):**
  1. **Metadata tagging** (required): Store wrist type per session
  2. **Domain-aware evaluation**: Report metrics stratified by wrist type
  3. **Adaptive thresholds**: Use relaxed thresholds for non-dominant (already implemented)
  4. **Augmentation during training**: 20-30% of samples with simulated non-dominant patterns
  5. **Single unified model**: Keep one model with wrist-aware inference

**Citations:**

1. **Cross-position heterogeneity**: "Data distribution heterogeneities caused by sensor placement... Variability in sensor readings when the same device is placed on different body parts... addressed via domain-invariant feature learning" (papers/domain_adaptation/Domain Adaptation for Inertial Measurement Unit-based Human.pdf, p.1-3)

2. **Transfer learning across positions**: "Cross-position transfer remains challenging... stratified evaluation by sensor position reveals true generalization capability" (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.6-7)

3. **Existing repo implementation**: `scripts/post_inference_monitoring.py` lines 92-161 already implement `get_effective_thresholds(dominance_match=False)` for relaxed thresholds

---

### Q2: Propose exact repo changes: where to store wrist/handedness/user/session metadata per dataset and how training + reporting should use it.

**Decision:** Create a centralized `data/metadata/` directory with session-level JSON files and a master registry CSV. Extend training scripts to load metadata for stratified evaluation.

**Detailed Reasoning:**

**Metadata schema (per session):**
```json
{
  "session_id": "2025-07-16-21-03-13",
  "user_id": "user_001",
  "user_metadata": {
    "dominant_hand": "right",
    "age_group": "30-40",
    "gender": "M"
  },
  "session_metadata": {
    "watch_wrist": "left",
    "dominance_match": false,
    "collection_date": "2025-07-16",
    "collection_time": "21:03:13",
    "environment": "home",
    "notes": ""
  },
  "data_files": {
    "accelerometer": "decoded_csv_files/2025-07-16-21-03-13_accelerometer.csv",
    "gyroscope": "decoded_csv_files/2025-07-16-21-03-13_gyroscope.csv",
    "record": "decoded_csv_files/2025-07-16-21-03-13_record.csv"
  }
}
```

**Concrete Repo Actions:**

```
1. CREATE: data/metadata/ directory structure
   data/metadata/
   ├── sessions/                    # Per-session JSON files
   │   ├── 2025-07-16-21-03-13.json
   │   ├── 2025-07-16-21-46-56.json
   │   └── ...
   ├── session_registry.csv         # Master list with key fields
   └── user_registry.csv            # User-level metadata

2. CREATE: data/metadata/session_registry.csv
   Columns: session_id, user_id, dominant_hand, watch_wrist, dominance_match, 
            collection_date, is_labeled, label_status

3. CREATE: scripts/metadata/create_session_metadata.py
   - Interactive CLI to populate metadata for each session
   - Auto-populates from filename (date/time)
   - Prompts for wrist type, handedness
   - Validates and writes JSON + updates registry

4. UPDATE: scripts/post_inference_monitoring.py
   - Add function: load_session_metadata(session_id)
   - Modify MonitoringThresholds.get_effective_thresholds() to auto-detect
   - Pass dominance_match from metadata to threshold selection

5. CREATE: scripts/evaluation/stratified_evaluation.py
   - Loads predictions + metadata
   - Computes metrics stratified by:
     * dominance_match (True/False)
     * user_id (leave-one-user-out)
     * collection_environment
   - Generates report with per-stratum F1 scores

6. UPDATE: src/train.py (or equivalent training script)
   - Load metadata during data loading
   - Create stratified train/val/test splits:
     * Ensure both dominance_match=True and False in each split
     * Balance by user_id to prevent user leakage
   - Add augmentation for simulated non-dominant patterns

7. UPDATE: config/pipeline_config.yaml
   Add section:
   ```yaml
   metadata:
     session_registry: "data/metadata/session_registry.csv"
     user_registry: "data/metadata/user_registry.csv"
     required_fields:
       - dominant_hand
       - watch_wrist
       - dominance_match
   
   evaluation:
     stratify_by:
       - dominance_match
       - user_id
     report_per_stratum: true
   
   augmentation:
     simulate_nondominant:
       enabled: true
       fraction: 0.25
       amplitude_scale: [0.6, 0.8]
   ```

8. CREATE: docs/METADATA_SCHEMA.md
   - Documents all metadata fields
   - Provides examples for each session type
   - Instructions for data collectors
```

**Metadata usage in training pipeline:**

| Pipeline Stage | How Metadata is Used |
|----------------|---------------------|
| **Data Loading** | Load `session_registry.csv`, join with sensor data |
| **Train/Val Split** | Stratified by `dominance_match` + `user_id` |
| **Augmentation** | Apply signal attenuation to simulate non-dominant |
| **Training** | Include `dominance_match` as auxiliary input (optional) |
| **Inference** | Load session metadata → select appropriate thresholds |
| **Evaluation** | Stratified metrics by `dominance_match`, `user_id` |
| **Reporting** | Separate F1/accuracy columns for dominant vs non-dominant |

**Reporting format (example output):**

```
================== STRATIFIED EVALUATION REPORT ==================
Overall:        F1=0.847  Accuracy=0.863  N=500 windows

By Wrist Dominance Match:
  dominance_match=True:   F1=0.891  Accuracy=0.904  N=350 windows
  dominance_match=False:  F1=0.762  Accuracy=0.778  N=150 windows
  ⚠️ Gap: 12.9% F1 drop for non-dominant wrist

By User (Leave-One-Out):
  user_001: F1=0.834  (dominant)
  user_002: F1=0.856  (dominant)
  user_003: F1=0.751  (non-dominant)  ⚠️
  ...
================================================================
```

**Citations:**

4. **Stratified evaluation necessity**: "Cross-person and cross-position evaluation requires proper stratification... aggregate metrics can hide significant performance gaps across subgroups" (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.11-12)

5. **Metadata for domain adaptation**: "Domain labels (source vs target) enable domain-adversarial training... position/device metadata serves similar purpose for wearable HAR" (papers/domain_adaptation/Domain Adaptation for Inertial Measurement Unit-based Human.pdf, p.5-6)

6. **Existing implementation reference**: The repo already has threshold adaptation in `scripts/post_inference_monitoring.py:get_effective_thresholds()` - this proposal extends the pattern to full pipeline

---

**Summary Table:**

| Aspect | Recommendation | Implementation |
|--------|---------------|----------------|
| **Model strategy** | Single unified model | NOT separate models (maintenance burden) |
| **Metadata storage** | `data/metadata/sessions/*.json` + registry CSV | Centralized, version-controlled |
| **Key metadata fields** | `dominant_hand`, `watch_wrist`, `dominance_match` | Required for all sessions |
| **Threshold adaptation** | Relaxed thresholds for non-dominant | Already in `post_inference_monitoring.py` |
| **Training split** | Stratified by dominance_match + user_id | Prevent leakage, ensure coverage |
| **Augmentation** | 25% samples with simulated non-dominant | Amplitude scaling 0.6-0.8x |
| **Evaluation** | Stratified by dominance_match | Report gap between strata |

---

## Pair 03 — DVC tracking policy

### Q1: In DVC, what should we track: raw, processed, prepared? Provide a clear policy + folder conventions.

**Policy Decision:** Track ALL three data stages (`raw/`, `processed/`, `prepared/`) plus `models/pretrained/` with DVC. This ensures full reproducibility from source data to inference-ready artifacts.

**Detailed Reasoning:**

- **Why track raw data:**
  - Immutable source of truth (original Garmin exports, Excel files)
  - Enables complete audit trail from collection to prediction
  - Required for regulatory compliance in healthcare ML
  - (papers/mlops_production/Demystifying MLOps and Presenting a Recipe for the Selection of Open-Source Tools 2021.pdf, p.8-9): "Data versioning ensures reproducibility... raw data must be preserved immutably"

- **Why track processed data:**
  - Expensive to regenerate (sensor fusion, resampling takes time)
  - Represents validated intermediate state (50Hz aligned, unit-converted)
  - Enables debugging of preprocessing issues vs. model issues
  - (papers/mlops_production/MACHINE LEARNING OPERATIONS A SURVEY ON MLOPS.pdf, p.12): "Feature engineering outputs should be versioned to isolate transformation bugs"

- **Why track prepared data:**
  - ML-ready format (windowed `.npy` arrays, scaler parameters)
  - Ensures exact reproducibility of training/inference inputs
  - Contains normalization config (`baseline_stats.json`) critical for consistent inference
  - (papers needs to read/Demystifying MLOps and Presenting a Recipe for the Selection of Open-Source Tools 2021.pdf, p.10): "Training artifacts including preprocessed features must be versioned together with model weights"

- **Why track pretrained model:**
  - Model weights are large binary files unsuitable for Git
  - DVC links model version to exact data version used for training
  - Enables rollback to previous model versions if production issues occur

**Current Repo State (already implemented):**

| Stage | DVC File | Contents | Size | Status |
|-------|----------|----------|------|--------|
| **Raw** | `data/raw.dvc` | Excel files (accel, gyro, labeled CSV) | 64 MB | ✅ Tracked |
| **Processed** | `data/processed.dvc` | `sensor_fused_50Hz.csv`, metadata | 109 MB | ✅ Tracked |
| **Prepared** | `data/prepared.dvc` | `production_X.npy`, `config.json` | 47 MB | ✅ Tracked |
| **Model** | `models/pretrained.dvc` | `fine_tuned_model_1dcnnbilstm.keras` | 6 MB | ✅ Tracked |

**Policy Rules:**

| Rule | Description | Rationale |
|------|-------------|-----------|
| **Rule 1** | Track ALL data stages | Full reproducibility chain |
| **Rule 2** | Never commit large files to Git | Use DVC for files > 1 MB |
| **Rule 3** | `.dvc` files must be committed with code changes | Links data version to code version |
| **Rule 4** | `dvc push` after every `dvc add` | Ensures remote backup |
| **Rule 5** | Include metadata JSON with each stage | Enables lineage tracking |

**Folder Conventions:**

```
data/
├── raw/                          # Stage 1: Immutable source data
│   ├── *.xlsx                    # Original Garmin exports
│   └── all_users_data_labeled.csv
├── raw.dvc                       # DVC pointer for raw/
│
├── processed/                    # Stage 2: Fused, resampled data
│   ├── sensor_fused_50Hz.csv     # Main processed file
│   ├── sensor_fused_meta.json    # Processing metadata
│   └── sensor_merged_native_rate.csv
├── processed.dvc                 # DVC pointer for processed/
│
├── prepared/                     # Stage 3: ML-ready data
│   ├── production_X.npy          # Windowed feature arrays
│   ├── baseline_stats.json       # Scaler parameters
│   ├── config.json               # Window/overlap config
│   └── production_metadata.json  # Lineage info
├── prepared.dvc                  # DVC pointer for prepared/
│
├── metadata/                     # NOT tracked by DVC (small, in Git)
│   └── session_registry.csv
│
└── audit/                        # Future: labeled audit sets
    └── labeled/                  # Tracked by DVC when populated

models/
├── pretrained/                   # Pre-trained model weights
│   ├── fine_tuned_model_1dcnnbilstm.keras
│   └── model_info.json
└── pretrained.dvc                # DVC pointer for pretrained/

decoded_csv_files/                # Production ingestion (NOT tracked)
├── *_accelerometer.csv           # Ingestion point for new data
├── *_gyroscope.csv
└── *_record.csv
```

**What NOT to track with DVC:**

| Item | Reason | Where it Goes |
|------|--------|---------------|
| `decoded_csv_files/` | Transient ingestion area, changes frequently | `.gitignore` |
| `mlruns/` | MLflow experiments (has own versioning) | `.gitignore` |
| `logs/` | Diagnostic logs (append-only) | `.gitignore` |
| `outputs/` | Inference results (regenerable) | `.gitignore` |
| Small metadata files | < 1 MB, Git handles fine | Git directly |

**Citations:**

1. **Data versioning for reproducibility**: "DVC (Data Version Control) tracks large dataset files... stores actual data in remote storage while keeping lightweight pointer files in Git. This versions data alongside code" (papers/mlops_production/Demystifying MLOps and Presenting a Recipe for the Selection of Open-Source Tools 2021.pdf, p.8-9)

2. **Feature versioning**: "Intermediate feature engineering outputs should be versioned to enable debugging and rollback" (papers/mlops_production/MACHINE LEARNING OPERATIONS A SURVEY ON MLOPS.pdf, p.12-13)

3. **Model-data linkage**: "Log DVC commit hash and Git commit hash into MLflow as parameters. This creates traceable link between data used, code executed, and resulting model" (papers needs to read/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.6)

---

### Q2: Outline dvc.yaml stages (deps/outs/params) so outputs are tracked automatically and runs are reproducible.

**Decision:** Create a `dvc.yaml` pipeline with 5 stages: `decode` → `fuse` → `prepare` → `infer` → `evaluate`. Each stage declares deps/outs/params for automatic tracking and reproducibility.

**Step-by-Step Pipeline Structure:**

```yaml
# dvc.yaml - MLOps pipeline definition

stages:
  # ================================================================
  # STAGE 1: DECODE - Convert raw FIT/Excel to normalized CSV
  # ================================================================
  decode:
    cmd: python src/sensor_data_pipeline.py decode
         --input decoded_csv_files/
         --output data/raw/
    deps:
      - src/sensor_data_pipeline.py
      - decoded_csv_files/           # New data drops here
    outs:
      - data/raw/:
          persist: true              # Don't delete on dvc repro
    params:
      - config/pipeline_config.yaml:
          - decode.input_format
          - decode.output_format

  # ================================================================
  # STAGE 2: FUSE - Sensor fusion, resampling to 50Hz
  # ================================================================
  fuse:
    cmd: python src/sensor_data_pipeline.py fuse
         --input data/raw/
         --output data/processed/
    deps:
      - src/sensor_data_pipeline.py
      - data/raw/
    outs:
      - data/processed/sensor_fused_50Hz.csv
      - data/processed/sensor_fused_meta.json
    params:
      - config/pipeline_config.yaml:
          - fuse.target_rate_hz
          - fuse.resampling_method
          - fuse.unit_conversion

  # ================================================================
  # STAGE 3: PREPARE - Windowing, normalization, ML-ready format
  # ================================================================
  prepare:
    cmd: python src/sensor_data_pipeline.py prepare
         --input data/processed/sensor_fused_50Hz.csv
         --output data/prepared/
    deps:
      - src/sensor_data_pipeline.py
      - data/processed/sensor_fused_50Hz.csv
      - models/pretrained/model_info.json  # For expected input shape
    outs:
      - data/prepared/production_X.npy
      - data/prepared/baseline_stats.json
      - data/prepared/config.json
      - data/prepared/production_metadata.json
    params:
      - config/pipeline_config.yaml:
          - prepare.window_size
          - prepare.window_overlap
          - prepare.features

  # ================================================================
  # STAGE 4: INFER - Run model predictions
  # ================================================================
  infer:
    cmd: python src/run_inference.py
         --model models/pretrained/fine_tuned_model_1dcnnbilstm.keras
         --data data/prepared/production_X.npy
         --output outputs/predictions/
    deps:
      - src/run_inference.py
      - models/pretrained/fine_tuned_model_1dcnnbilstm.keras
      - data/prepared/production_X.npy
      - data/prepared/baseline_stats.json
    outs:
      - outputs/predictions/predictions.csv
      - outputs/predictions/confidence_scores.csv
    params:
      - config/pipeline_config.yaml:
          - infer.batch_size
          - infer.confidence_threshold

  # ================================================================
  # STAGE 5: EVALUATE - Compute metrics (if labels available)
  # ================================================================
  evaluate:
    cmd: python src/evaluate_predictions.py
         --predictions outputs/predictions/predictions.csv
         --ground_truth data/audit/labeled/
         --output outputs/evaluation/
    deps:
      - src/evaluate_predictions.py
      - outputs/predictions/predictions.csv
      - data/audit/labeled/           # Optional: only if labeled data exists
    outs:
      - outputs/evaluation/metrics.json
      - outputs/evaluation/confusion_matrix.png
    metrics:
      - outputs/evaluation/metrics.json:
          cache: false                # Always show in dvc metrics
    plots:
      - outputs/evaluation/confusion_matrix.png
```

**Params File Structure (`config/pipeline_config.yaml`):**

```yaml
# config/pipeline_config.yaml - Single source of truth for all parameters

decode:
  input_format: "fit"              # or "xlsx"
  output_format: "csv"
  
fuse:
  target_rate_hz: 50
  resampling_method: "mean"        # Aggregation within 20ms bins
  unit_conversion:
    accelerometer: "milli_g_to_ms2"
    gyroscope: "dps_to_radps"

prepare:
  window_size: 200                 # 4 seconds at 50Hz
  window_overlap: 0.5              # 50% overlap
  features:
    - "accel_x"
    - "accel_y"
    - "accel_z"
    - "gyro_x"
    - "gyro_y"
    - "gyro_z"

infer:
  batch_size: 32
  confidence_threshold: 0.5

evaluate:
  stratify_by:
    - "dominance_match"
    - "user_id"
  metrics:
    - "f1_macro"
    - "accuracy"
    - "per_class_f1"

# Metadata configuration (from Pair 02)
metadata:
  session_registry: "data/metadata/session_registry.csv"
  required_fields:
    - "dominant_hand"
    - "watch_wrist"

# Labeling configuration (from Pair 01)
labeling:
  audit_sessions: 5
  windows_per_session: 100
```

**Pipeline DAG Visualization:**

```
decoded_csv_files/ ──┐
                     │
                     ▼
              ┌─────────────┐
              │   decode    │  Stage 1
              └─────────────┘
                     │
                     ▼
                data/raw/
                     │
                     ▼
              ┌─────────────┐
              │    fuse     │  Stage 2
              └─────────────┘
                     │
                     ▼
             data/processed/
                     │
                     ▼
              ┌─────────────┐
              │   prepare   │  Stage 3
              └─────────────┘
                     │
                     ▼
              data/prepared/
                     │
     ┌───────────────┴───────────────┐
     ▼                               ▼
models/pretrained/            config/pipeline_config.yaml
     │                               │
     └───────────────┬───────────────┘
                     ▼
              ┌─────────────┐
              │    infer    │  Stage 4
              └─────────────┘
                     │
                     ▼
           outputs/predictions/
                     │
                     ▼
              ┌─────────────┐
              │  evaluate   │  Stage 5
              └─────────────┘
                     │
                     ▼
           outputs/evaluation/
```

**Concrete Repo Actions:**

```
1. CREATE: dvc.yaml (at repo root)
   - Full pipeline definition as shown above
   - 5 stages with deps/outs/params

2. CREATE: config/pipeline_config.yaml
   - Centralized parameters for all stages
   - Includes fuse, prepare, infer, evaluate sections

3. UPDATE: src/sensor_data_pipeline.py
   - Add CLI subcommands: decode, fuse, prepare
   - Accept --input, --output, config file arguments
   - Read params from config/pipeline_config.yaml

4. CREATE: src/run_inference.py (if not exists)
   - Loads model, runs predictions
   - Outputs predictions.csv, confidence_scores.csv

5. UPDATE: .gitignore
   - Ensure decoded_csv_files/ is NOT tracked by Git
   - Ensure outputs/ is NOT tracked by Git
   - Keep .dvc files tracked

6. RUN: dvc repro
   - Executes full pipeline
   - Automatically versions all outputs
```

**DVC Commands for Pipeline:**

```bash
# Run full pipeline (skips unchanged stages)
dvc repro

# Run specific stage
dvc repro prepare

# View pipeline DAG
dvc dag

# View metrics
dvc metrics show

# Compare metrics across branches/commits
dvc metrics diff

# View params used in last run
dvc params diff

# Check what would change
dvc status
```

**Reproducibility Guarantees:**

| Guarantee | How DVC Achieves It |
|-----------|---------------------|
| **Same inputs → same outputs** | Hash-based caching; skips if deps unchanged |
| **Param tracking** | Changes to `pipeline_config.yaml` trigger re-run |
| **Cross-machine reproducibility** | `dvc pull` fetches exact data version |
| **Experiment tracking** | `dvc exp` for hyperparameter experiments |
| **Audit trail** | Git history of `.dvc` files = data history |

**Citations:**

4. **Pipeline as code**: "DVC pipelines defined in dvc.yaml enable reproducible ML workflows... dependencies and outputs automatically tracked" (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.45-48)

5. **Parameter tracking**: "Separating parameters from code in dedicated config files enables hyperparameter versioning without code changes" (papers/mlops_production/MLOps A Step Forward to Enterprise Machine Learning 2023.pdf, p.7-8)

6. **Metrics tracking**: "dvc metrics integrates with Git to track model performance across versions... enables A/B comparison of model iterations" (papers/mlops_production/Essential_MLOps_Data_Science_Horizons_2023_Data_Science_Horizons_Final_2023.pdf, p.15)

---

**Summary Table:**

| Aspect | Policy | Implementation |
|--------|--------|----------------|
| **What to track** | raw + processed + prepared + model | All 4 stages have `.dvc` files |
| **Pipeline file** | `dvc.yaml` at repo root | 5 stages: decode→fuse→prepare→infer→evaluate |
| **Params file** | `config/pipeline_config.yaml` | Single source of truth for all params |
| **Run command** | `dvc repro` | Executes pipeline, caches unchanged stages |
| **Metrics** | `outputs/evaluation/metrics.json` | Tracked by `dvc metrics` |
| **Decoded CSVs** | NOT tracked (transient) | Input to decode stage only |


---

## Pair 04 — Dataset Naming with Date/Time

**Context:** We fuse accel+gyro via nearest timestamp alignment; want filenames including recording date/time so we never need to open the file to identify when it was recorded.

### Q1: Propose a naming convention that includes date/time + subject/session/device + wrist side so all metadata is visible in `ls` without opening the file.

**Decision:** Use the format `{YYYYMMDD}T{HHMMSS}_{subject}_{session}_{device}_{wrist}_{sensor}.csv` — all metadata embedded in filename, parseable by regex, sortable chronologically.

**Detailed Reasoning:**

- **Why embed date/time first:**
  - ISO 8601-inspired format (`YYYYMMDD`) ensures lexicographic sorting = chronological sorting
  - Current filenames (`2025-07-16-21-03-13_accelerometer.csv`) already contain timestamp but use dashes which can be ambiguous
  - The `T` separator distinguishes date from time per ISO 8601 convention
  - (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.112-115): "Data lineage requires unique identifiers that encode acquisition context... timestamps should be unambiguous and sortable"

- **Why include subject + session:**
  - Multiple subjects may record on the same day; session ID disambiguates multiple recordings from same subject
  - Enables `ls *_user01_*` to filter by subject without loading data
  - (papers needs to read/Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets.pdf, Abstract): "Standardized naming conventions with embedded metadata enable scalable data management across multi-site studies"

- **Why include device + wrist:**
  - Essential for domain adaptation (wrist side affects signal characteristics)
  - Device ID enables hardware-specific drift detection
  - Already established as critical metadata in Pair 02 (data/metadata/ structure)
  - (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.4-5): "Sensor placement metadata is critical for proper domain stratification"

- **Why sensor type in filename:**
  - Enables independent processing of accel vs gyro before fusion
  - Supports `ls *_accelerometer.csv` filtering
  - Current pattern already uses this (`_accelerometer.csv`, `_gyroscope.csv`)

**Proposed Naming Convention:**

```
Format: {YYYYMMDD}T{HHMMSS}_{subject}_{session}_{device}_{wrist}_{sensor}.csv

Examples:
  20250716T210313_user01_sess01_garmin01_left_accelerometer.csv
  20250716T210313_user01_sess01_garmin01_left_gyroscope.csv
  20250716T210313_user01_sess01_garmin01_left_record.csv
  20250717T134910_user02_sess03_garmin02_right_accelerometer.csv
```

**Regex Pattern for Parsing:**

```python
import re

FILENAME_PATTERN = re.compile(
    r'^(?P<date>\d{8})T(?P<time>\d{6})_'
    r'(?P<subject>[a-z0-9]+)_'
    r'(?P<session>[a-z0-9]+)_'
    r'(?P<device>[a-z0-9]+)_'
    r'(?P<wrist>left|right)_'
    r'(?P<sensor>accelerometer|gyroscope|record)\.csv$'
)
```

**Comparison with Current Pattern:**

| Aspect | Current | Proposed |
|--------|---------|----------|
| **Format** | `2025-07-16-21-03-13_accelerometer.csv` | `20250716T210313_user01_sess01_garmin01_left_accelerometer.csv` |
| **Date parsing** | Requires split on `-` | Direct `strptime('%Y%m%dT%H%M%S')` |
| **Subject ID** | Not in filename | Embedded |
| **Session ID** | Inferred from timestamp | Explicit |
| **Device ID** | Unknown | Embedded |
| **Wrist side** | Unknown | Embedded |
| **Sortability** | Chronological | Chronological |

---

### Q2: Describe implementation steps: extracting date/time from current filenames and storing sidecar JSON/YAML metadata.

**Decision:** Create a migration script that (1) parses existing timestamps, (2) prompts for missing metadata, (3) renames files, and (4) creates sidecar `.meta.json` for each session.

**Implementation Steps:**

**Step 1: Create metadata extraction script**

```python
# scripts/metadata/migrate_decoded_files.py

import os
import re
import json
from pathlib import Path
from datetime import datetime

# Current filename pattern
CURRENT_PATTERN = re.compile(
    r'^(\d{4})-(\d{2})-(\d{2})-(\d{2})-(\d{2})-(\d{2})_'
    r'(accelerometer|gyroscope|record)\.csv$'
)

def extract_datetime_from_filename(filename: str) -> datetime:
    """Extract datetime from current filename format."""
    match = CURRENT_PATTERN.match(filename)
    if not match:
        raise ValueError(f"Filename doesn't match pattern: {filename}")
    
    year, month, day, hour, minute, second, sensor = match.groups()
    return datetime(int(year), int(month), int(day), 
                    int(hour), int(minute), int(second))

def get_session_id_from_timestamp(dt: datetime) -> str:
    """Generate unique session ID from timestamp."""
    return dt.strftime('%Y%m%d%H%M%S')
```

**Step 2: Create interactive metadata collection CLI**

```python
# scripts/metadata/collect_session_metadata.py

import click
from pathlib import Path

@click.command()
@click.argument('decoded_dir', type=click.Path(exists=True))
def collect_metadata(decoded_dir: str):
    """Interactively collect metadata for decoded sessions."""
    
    sessions = group_files_by_session(decoded_dir)
    
    for session_id, files in sessions.items():
        click.echo(f"\n=== Session: {session_id} ===")
        click.echo(f"Files: {[f.name for f in files]}")
        
        # Prompt for metadata
        subject = click.prompt("Subject ID", default="user01")
        wrist = click.prompt("Wrist side", 
                            type=click.Choice(['left', 'right']))
        device = click.prompt("Device ID", default="garmin01")
        handedness = click.prompt("User handedness",
                                  type=click.Choice(['left', 'right']))
        notes = click.prompt("Notes (optional)", default="")
        
        # Save sidecar metadata
        metadata = {
            "session_id": session_id,
            "subject_id": subject,
            "wrist_side": wrist,
            "device_id": device,
            "user_handedness": handedness,
            "dominance_match": wrist == handedness,
            "notes": notes,
            "files": [f.name for f in files]
        }
        
        save_sidecar_metadata(session_id, metadata, decoded_dir)
```

**Step 3: Define sidecar metadata schema**

```json
// decoded_csv_files/20250716T210313.meta.json
{
  "schema_version": "1.0",
  "session": {
    "id": "20250716T210313",
    "timestamp_utc": "2025-07-16T21:03:13Z",
    "timestamp_local": "2025-07-16T23:03:13+02:00"
  },
  "subject": {
    "id": "user01",
    "handedness": "right",
    "age_group": "adult",
    "notes": ""
  },
  "device": {
    "id": "garmin01",
    "model": "Garmin Vivomove Style",
    "firmware": "unknown",
    "wrist_placement": "left",
    "dominance_match": false
  },
  "acquisition": {
    "duration_seconds": null,
    "sampling_rate_nominal": 25,
    "activity_context": "daily_living"
  },
  "files": {
    "accelerometer": "20250716T210313_user01_sess01_garmin01_left_accelerometer.csv",
    "gyroscope": "20250716T210313_user01_sess01_garmin01_left_gyroscope.csv",
    "record": "20250716T210313_user01_sess01_garmin01_left_record.csv"
  },
  "lineage": {
    "original_filenames": [
      "2025-07-16-21-03-13_accelerometer.csv",
      "2025-07-16-21-03-13_gyroscope.csv",
      "2025-07-16-21-03-13_record.csv"
    ],
    "migration_date": "2026-01-18",
    "migration_script": "scripts/metadata/migrate_decoded_files.py"
  }
}
```

**Step 4: Batch rename script**

```python
# scripts/metadata/rename_decoded_files.py

def rename_session_files(decoded_dir: Path, dry_run: bool = True):
    """Rename files according to new convention using sidecar metadata."""
    
    for meta_file in decoded_dir.glob('*.meta.json'):
        with open(meta_file) as f:
            meta = json.load(f)
        
        session_id = meta['session']['id']
        subject = meta['subject']['id']
        device = meta['device']['id']
        wrist = meta['device']['wrist_placement']
        
        for sensor, new_filename in meta['files'].items():
            old_pattern = f"*_{sensor}.csv"
            old_files = list(decoded_dir.glob(
                f"{meta['lineage']['original_filenames'][0][:19]}*{sensor}.csv"
            ))
            
            if old_files:
                old_path = old_files[0]
                new_path = decoded_dir / new_filename
                
                if dry_run:
                    print(f"Would rename: {old_path.name} -> {new_filename}")
                else:
                    old_path.rename(new_path)
                    print(f"Renamed: {old_path.name} -> {new_filename}")
```

**Step 5: Update pipeline to use sidecar metadata**

```python
# src/data/load_decoded_session.py

def load_session_with_metadata(session_dir: Path, session_id: str):
    """Load session data along with its sidecar metadata."""
    
    meta_file = session_dir / f"{session_id}.meta.json"
    
    if not meta_file.exists():
        raise FileNotFoundError(
            f"Sidecar metadata not found: {meta_file}\n"
            f"Run: python scripts/metadata/collect_session_metadata.py {session_dir}"
        )
    
    with open(meta_file) as f:
        metadata = json.load(f)
    
    # Load sensor files
    accel_file = session_dir / metadata['files']['accelerometer']
    gyro_file = session_dir / metadata['files']['gyroscope']
    
    accel_df = pd.read_csv(accel_file)
    gyro_df = pd.read_csv(gyro_file)
    
    return {
        'accelerometer': accel_df,
        'gyroscope': gyro_df,
        'metadata': metadata
    }
```

**Concrete Repo Actions:**

```
1. CREATE: scripts/metadata/migrate_decoded_files.py
   - Parse existing timestamps from filenames
   - Generate session IDs
   - Extract datetime objects

2. CREATE: scripts/metadata/collect_session_metadata.py  
   - Interactive CLI for metadata collection
   - Prompts for subject, wrist, device, handedness
   - Creates .meta.json sidecar files

3. CREATE: scripts/metadata/rename_decoded_files.py
   - Batch rename using new convention
   - Supports --dry-run flag
   - Updates lineage in sidecar files

4. CREATE: config/schemas/session_metadata_schema.json
   - JSON Schema for validating sidecar files
   - Enforces required fields

5. UPDATE: src/data/load_decoded_session.py
   - Load metadata alongside sensor data
   - Pass dominance_match to downstream processing

6. CREATE: decoded_csv_files/README.md
   - Document naming convention
   - Explain sidecar metadata schema
```

**Migration Workflow:**

```bash
# 1. First pass: Create sidecar metadata interactively
python scripts/metadata/collect_session_metadata.py decoded_csv_files/

# 2. Preview rename operations
python scripts/metadata/rename_decoded_files.py decoded_csv_files/ --dry-run

# 3. Execute rename (creates backup first)
python scripts/metadata/rename_decoded_files.py decoded_csv_files/ --execute

# 4. Validate all sessions have metadata
python scripts/metadata/validate_sessions.py decoded_csv_files/

# 5. Update session_registry.csv in data/metadata/
python scripts/metadata/sync_registry.py decoded_csv_files/ data/metadata/
```

**Citations:**

1. **Data lineage through naming**: "Standardized naming conventions that embed acquisition metadata (timestamp, subject, device) enable automated lineage tracking without external databases" (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.112-115)

2. **Sidecar metadata pattern**: "Sidecar files (.json, .yaml) alongside data files provide extensible metadata without modifying original formats... preferred for multi-site studies" (papers needs to read/Building Flexible, Scalable, and Machine Learning-ready Multimodal Oncology Datasets.pdf, p.3-5)

3. **ISO 8601 for timestamps**: "Use ISO 8601 date formats (YYYYMMDD or YYYY-MM-DD) for unambiguous, sortable timestamps in data pipelines" (papers/mlops_production/Essential_MLOps_Data_Science_Horizons_2023_Data_Science_Horizons_Final_2023.pdf, p.18-20)

4. **Sensor placement in filenames**: "Explicit encoding of sensor placement (wrist side, body position) in filenames prevents metadata loss during data movement and enables automated stratification" (papers/domain_adaptation/Transfer Learning in Human Activity Recognition A Survey.pdf, p.4-5)

5. **DVC + sidecar compatibility**: "Sidecar metadata files should be tracked in Git (small), while large data files use DVC... this preserves lineage across version boundaries" (papers/mlops_production/Demystifying MLOps and Presenting a Recipe for the Selection of Open-Source Tools 2021.pdf, p.8-10)

---

**Summary Table:**

| Aspect | Convention | Rationale |
|--------|------------|-----------|
| **Date format** | `YYYYMMDD` (no separators) | ISO 8601 sortable, unambiguous |
| **Time format** | `HHMMSS` after `T` | ISO 8601 standard separator |
| **Subject ID** | `userXX` | Zero-padded for sorting |
| **Session ID** | `sessXX` or timestamp-derived | Disambiguates multiple sessions/day |
| **Device ID** | `garminXX` | Enables device-specific analysis |
| **Wrist side** | `left` or `right` | Critical for domain adaptation |
| **Sensor type** | `accelerometer`, `gyroscope`, `record` | Matches current pattern |
| **Sidecar file** | `{sessionID}.meta.json` | One per session, not per sensor |
| **Schema version** | In sidecar JSON | Enables backward compatibility |

---

## Pair 05 — Fusion vs Preprocessing Stages

**Context:** We have fusion (merge accel+gyro) and preprocessing (filtering/scaling/windowing). Decide best structure.

### Q1: One script vs separate scripts/stages? Tradeoffs: debugging, caching, reproducibility, DVC, MLflow logging.

**Recommendation:** Use SEPARATE scripts/stages with clear boundaries: (1) Fusion stage (`sensor_data_pipeline.py`), (2) Preprocessing stage (`preprocess_data.py`). This is already the current implementation and should be kept.

**Pros of Separate Stages:**

- **Debugging isolation:**
  - Fusion failures (timestamp misalignment, sampling rate issues) are distinct from preprocessing failures (unit conversion, normalization bugs)
  - Logs are separated by stage, easier to diagnose which step failed
  - Can test fusion independently with dummy gyro/accel data
  - Current implementation: Two separate log directories (`logs/fusion/`, `logs/preprocessing/`)
  
- **Caching and efficiency:**
  - `sensor_fused_50Hz.csv` can be reused across multiple preprocessing experiments
  - Avoids re-running costly fusion (timestamp alignment, interpolation) when only changing normalization params
  - DVC hash-based caching: If fusion output unchanged, DVC skips re-execution
  - Example: Testing different `enable_gravity_removal` settings doesn't require re-fusion
  
- **Reproducibility and versioning:**
  - Each stage has independent DVC tracking: `data/processed.dvc` (fusion output), `data/prepared.dvc` (preprocessing output)
  - Git history shows which stage changed: "Update fusion tolerance" vs "Disable gravity filter"
  - Rollback is granular: Can revert preprocessing changes without affecting fusion
  - DVC `dvc.yaml` stages enable `dvc repro` to auto-execute only changed dependencies
  
- **MLflow logging granularity:**
  - Fusion stage logs: Sampling rates, merge tolerance, interpolation gaps
  - Preprocessing stage logs: Unit detection confidence, scaler params, window count
  - Separate experiments avoid conflating fusion vs preprocessing hyperparameters
  - Easier to compare "same fusion, different preprocessing" experiments
  
- **Modularity and maintainability:**
  - Clear separation of concerns: Fusion = time-series alignment, Preprocessing = feature engineering
  - Different team members can work on stages independently
  - Unit tests are simpler: Mock fusion output CSV to test preprocessing
  - Code reuse: Fusion logic can be used for other sensor pairs (e.g., magnetometer + accel)

**Cons of Separate Stages (and mitigations):**

- **Extra I/O overhead:**
  - Con: Writing/reading intermediate `sensor_fused_50Hz.csv` adds disk I/O
  - Mitigation: CSV is small (`~110MB` for weeks of data), I/O negligible vs computation
  - Alternative: Use Parquet for faster read/write if CSV becomes bottleneck
  
- **Pipeline orchestration complexity:**
  - Con: Manual execution of two scripts instead of one
  - Mitigation: DVC pipeline (`dvc.yaml`) automates multi-stage execution with single `dvc repro` command
  - Already proposed in Pair 03 (5-stage pipeline)
  
- **Inconsistent intermediate state:**
  - Con: Risk of fusion output not matching preprocessing input (version mismatch)
  - Mitigation: DVC tracks exact hash of `sensor_fused_50Hz.csv` linked to preprocessing run
  - Mitigation: Add validation check in preprocessing to verify fusion output schema

**Cons of Single Combined Script:**

- **Tight coupling:** Cannot reuse fusion output for other purposes (e.g., exploratory analysis)
- **Harder debugging:** 1500+ line monolithic script, difficult to isolate failures
- **No intermediate caching:** Must re-run fusion even when only testing preprocessing params
- **DVC limitations:** Single output artifact means no granular versioning
- **Testing complexity:** Cannot mock fusion stage for preprocessing unit tests

---

### Q2: Recommend final stage boundaries + exact outputs per stage (files + metadata fields).

**Recommendation:** Define 5-stage pipeline with clear inputs/outputs and comprehensive metadata tracking. This aligns with Pair 03 proposal and follows MLOps best practices for data lineage.

**Final Stage Plan:**

**STAGE 1: DECODE** (Optional - if using FIT files)
- **Input:** `decoded_csv_files/*.fit` (Garmin proprietary format)
- **Script:** `scripts/decode_fit.py` (to be created if needed)
- **Outputs:**
  - `decoded_csv_files/{YYYYMMDD}T{HHMMSS}_{subject}_{session}_{device}_{wrist}_accelerometer.csv`
  - `decoded_csv_files/{YYYYMMDD}T{HHMMSS}_{subject}_{session}_{device}_{wrist}_gyroscope.csv`
  - `decoded_csv_files/{YYYYMMDD}T{HHMMSS}.meta.json` (sidecar metadata per Pair 04)
- **Metadata fields:**
  - `schema_version`, `session.id`, `subject.id`, `device.wrist_placement`
  - `lineage.original_filename`, `lineage.decoding_tool`
- **DVC tracking:** NO (transient intermediates)
- **MLflow logging:** NO

**STAGE 2: FUSION**
- **Input:** `decoded_csv_files/*_accelerometer.csv` + `*_gyroscope.csv`
- **Script:** `src/sensor_data_pipeline.py`
- **Outputs:**
  - `data/processed/sensor_fused_50Hz.csv`
    - Columns: `timestamp_iso, Ax, Ay, Az, Gx, Gy, Gz` (7 total)
    - Units: m/s for accel, rad/s for gyro (standardized)
  - `data/processed/sensor_fused_meta.json`
- **Metadata fields:**
  `json
  {
    "fusion_timestamp": "2026-01-18T14:23:45Z",
    "input_files": ["20250716T210313_user01_sess01_garmin01_left_accelerometer.csv", "..."],
    "target_frequency_hz": 50,
    "merge_tolerance_ms": 1,
    "rows_output": 125000,
    "duration_seconds": 2500,
    "interpolation": {
      "method": "linear",
      "gaps_filled": 23,
      "max_gap_ms": 40
    },
    "validation": {
      "missing_pct": 0.02,
      "duplicates_removed": 5,
      "outliers_flagged": 0
    }
  }
  `
- **DVC tracking:** `data/processed.dvc` (YES)
- **MLflow logging:** NO (data processing stage)

**STAGE 3: PREPROCESSING**
- **Input:** `data/processed/sensor_fused_50Hz.csv`
- **Script:** `src/preprocess_data.py`
- **Outputs:**
  - `data/prepared/production_X.npy` — Shape: `(N, 200, 6)`, dtype: `float32`
  - `data/prepared/config.json`
  - `data/prepared/production_metadata.json`
- **Metadata fields in `config.json`:**
  `json
  {
    "scaler_type": "StandardScaler",
    "scaler_params": {
      "mean": [0.123, -0.045, 9.81, 0.001, -0.002, 0.003],
      "scale": [2.1, 2.3, 2.5, 0.5, 0.6, 0.4]
    },
    "activity_mapping": {
      "0": "sit", "1": "stand", "2": "walk", ...
    },
    "window_size": 200,
    "overlap": 0.5,
    "sampling_rate": 50
  }
  `
- **Metadata fields in `production_metadata.json`:**
  `json
  {
    "preprocessing_timestamp": "2026-01-18T14:25:12Z",
    "input_file": "data/processed/sensor_fused_50Hz.csv",
    "unit_detection": {
      "detected_unit": "m/s",
      "confidence": 0.95,
      "conversion_applied": false
    },
    "gravity_removal": {
      "enabled": true,
      "method": "highpass_butterworth",
      "cutoff_hz": 0.3,
      "order": 4
    },
    "domain_calibration": {
      "enabled": false
    },
    "normalization": {
      "method": "StandardScaler",
      "fitted_on": "training_data",
      "scaler_loaded_from": "data/prepared/config.json"
    },
    "windowing": {
      "window_size": 200,
      "overlap_pct": 50,
      "stride": 100,
      "num_windows": 1248
    },
    "validation": {
      "input_rows": 125000,
      "output_windows": 1248,
      "nan_windows_dropped": 2,
      "inf_windows_dropped": 0
    }
  }
  `
- **DVC tracking:** `data/prepared.dvc` (YES)
- **MLflow logging:** NO (data processing stage)

**STAGE 4: INFERENCE**
- **Input:** `data/prepared/production_X.npy` + `models/pretrained/fine_tuned_model_1dcnnbilstm.keras`
- **Script:** `src/run_inference.py`
- **Outputs:**
  - `data/prepared/predictions/predictions_{YYYYMMDD}_{HHMMSS}.csv`
    - Columns: `window_id, predicted_label, predicted_activity, confidence, confidence_level, is_uncertain, prob_sit, prob_stand, ...` (17 total)
  - `data/prepared/predictions/predictions_{YYYYMMDD}_{HHMMSS}_probs.npy` — Shape: `(N, 11)`
  - `data/prepared/predictions/predictions_{YYYYMMDD}_{HHMMSS}_metadata.json`
- **Metadata fields:**
  `json
  {
    "inference_timestamp": "2026-01-18T14:26:30Z",
    "model": {
      "path": "models/pretrained/fine_tuned_model_1dcnnbilstm.keras",
      "version": "v1.2",
      "git_commit": "a3f9c2d",
      "dvc_rev": "models/pretrained.dvc@a3f9c2d",
      "architecture": "1D-CNN-BiLSTM",
      "parameters": 499123,
      "trained_on": "ADAMSense + custom 6-subject dataset"
    },
    "input_data": {
      "path": "data/prepared/production_X.npy",
      "shape": [1248, 200, 6],
      "dvc_hash": "md5:b4e8a9..."
    },
    "inference_config": {
      "batch_size": 32,
      "use_gpu": false
    },
    "predictions_summary": {
      "total_windows": 1248,
      "activities_detected": {
        "sit": 245, "stand": 189, "walk": 314, ...
      },
      "confidence_distribution": {
        "high_gt_90": 561,
        "moderate_70_90": 435,
        "low_50_70": 187,
        "uncertain_lt_50": 65
      },
      "uncertain_windows": [12, 45, 78, ...]
    }
  }
  `
- **DVC tracking:** NO (predictions are ephemeral outputs)
- **MLflow logging:** YES (optional, log as experiment run)

**STAGE 5: EVALUATION** (If ground truth available)
- **Input:** `data/prepared/predictions/predictions_{timestamp}.csv` + `data/prepared/ground_truth.csv`
- **Script:** `src/evaluate_predictions.py`
- **Outputs:**
  - `outputs/evaluation/metrics.json`
  - `outputs/evaluation/confusion_matrix.png`
  - `outputs/evaluation/calibration_plot.png`
- **Metadata fields in `metrics.json`:**
  `json
  {
    "evaluation_timestamp": "2026-01-18T14:28:15Z",
    "overall": {
      "accuracy": 0.834,
      "f1_macro": 0.812,
      "f1_weighted": 0.829
    },
    "per_class": {
      "sit": {"precision": 0.89, "recall": 0.91, "f1": 0.90},
      ...
    },
    "calibration": {
      "ece": 0.08,
      "mce": 0.12
    }
  }
  `
- **DVC tracking:** `outputs/evaluation/metrics.json` as DVC metric
- **MLflow logging:** YES (log metrics, artifacts)

---

## Pair 06 — Inference Definition

**Context:** Offline batch inference on unlabeled production-like data (data/raw/). Need monitoring + traceability.

### Q1: Define inference: inputs, outputs, artifacts, dataset_id rules, model/scaler linkage.

**Definition:** Inference is the process of applying a trained model to unlabeled production data to generate activity predictions with associated confidence scores and metadata. It operates as a standalone pipeline stage that consumes prepared data (`production_X.npy`) and outputs structured predictions with full traceability.

**Detailed Definition:**

**Inputs (Required):**

1. **Model Artifacts:**
   - `models/pretrained/fine_tuned_model_1dcnnbilstm.keras` - Trained Keras model
   - `data/prepared/config.json` - Contains scaler parameters (mean, std) fitted on training data
   - Must be version-linked: Model trained on dataset X requires scaler fitted on dataset X
   - Verification: Check `config.json` has `scaler_fitted_date` matching model training date
   
2. **Input Data:**
   - `data/prepared/production_X.npy` - Windowed, normalized sensor data
   - Shape: `(n_windows, timesteps, channels)` e.g., `(1500, 100, 6)`
   - Already normalized using training statistics from `config.json`
   - Channels: `[Ax, Ay, Az, Gx, Gy, Gz]` in m/s and deg/s
   
3. **Session Metadata (from sidecar):**
   - `decoded_csv_files/{sessionID}.meta.json` (per Pair 04 schema)
   - Required fields: `session_id`, `subject_id`, `wrist_side`, `device_id`, `dominance_match`
   - Used for: Threshold selection (per Pair 02), domain-aware evaluation

**Outputs (Per Inference Run):**

1. **Primary Predictions File:**
   - Filename: `predictions_{YYYYMMDD}_{HHMMSS}.csv`
   - Location: `data/prepared/predictions/`
   - Columns:
     `csv
     window_id,predicted_class,predicted_activity,confidence,confidence_level,is_uncertain,
     entropy,margin,prob_sit,prob_stand,prob_walk,prob_stairs,prob_run,prob_bike
     `
   - One row per window (e.g., 1500 rows for 1500 windows)
   
2. **Probability Matrix (Optional):**
   - Filename: `predictions_{YYYYMMDD}_{HHMMSS}_probs.npy`
   - Shape: `(n_windows, n_classes)` e.g., `(1500, 6)`
   - Full softmax output for advanced analysis (calibration, conformal prediction)
   
3. **Predictions Metadata:**
   - Filename: `predictions_{YYYYMMDD}_{HHMMSS}_metadata.json`
   - Schema:
     `json
     {
       "inference_id": "20260118_142830",
       "timestamp_utc": "2026-01-18T14:28:30Z",
       "git_commit": "a7f3d2e",
       "dvc_data_hash": "e4b8c1d9...",
       "model": {
         "path": "models/pretrained/fine_tuned_model_1dcnnbilstm.keras",
         "dvc_hash": "f9a2e3c7...",
         "params": 145230,
         "trained_on": "2025-12-15"
       },
       "scaler": {
         "fitted_on": "2025-12-15",
         "training_dataset": "all_users_data_labeled.csv"
       },
       "session": {
         "id": "20250716T210313",
         "subject_id": "user01",
         "wrist_side": "left",
         "dominance_match": false
       },
       "data": {
         "input_file": "data/prepared/production_X.npy",
         "n_windows": 1500,
         "timesteps": 100,
         "channels": 6,
         "duration_minutes": 50.0
       },
       "execution": {
         "batch_size": 32,
         "duration_seconds": 12.4,
         "throughput_windows_per_sec": 120.9
       },
       "statistics": {
         "activity_distribution": {
           "sit": 450, "walk": 380, "stand": 320, "stairs": 250, "run": 70, "bike": 30
         },
         "confidence": {
           "mean": 0.742,
           "std": 0.183,
           "min": 0.28,
           "max": 0.98,
           "p50": 0.78,
           "p90": 0.92
         },
         "uncertain_windows": 180,
         "uncertain_percentage": 12.0
       }
     }
     `

**Dataset ID Rules:**

- **Session-level ID:** Use timestamp from filename: `20250716T210313`
- **Inference run ID:** Use inference timestamp: `20260118_142830`
- **Combined traceability:** `{session_id}_{inference_run_id}` e.g., `20250716T210313_20260118_142830`
- **Why this matters:**
  - Same session can be re-inferred with different models (A/B testing)
  - Session ID links back to original raw data via Pair 04 sidecar
  - Inference ID enables reproducibility (which model/scaler/code version)
  
**Model/Scaler Linkage (Critical for Reproducibility):**

- **Problem:** Model trained on normalized data REQUIRES same normalization at inference
- **Solution:** Store scaler params in `config.json` alongside trained model
- **Validation:** Check `scaler_fitted_date` == `model_trained_on` date
- **DVC tracking:** Both `config.json` and model tracked by `data/prepared.dvc` and `models/pretrained.dvc`
- **Git tracking:** Commit hash links code version to model/scaler version
- **If mismatch detected:** BLOCK inference with error message referencing correct scaler

**Repo Actions for Inference Stage:**

```
1. UPDATE: src/run_inference.py
   - Add validation: Check scaler/model date match
   - Add session metadata loading from sidecar
   - Add dominance_match awareness for threshold selection
   - Add git_commit and dvc_hash to output metadata

2. CREATE: src/inference_validator.py
   - Validate model/scaler linkage
   - Validate input data shape matches model expectations
   - Validate session metadata schema
   - Raise InferenceLinkageError if checks fail

3. UPDATE: data/prepared/predictions/ directory structure
   - Create session-based subdirectories
   - predictions/{session_id}/{inference_run_id}/
   - predictions/{session_id}/{inference_run_id}/predictions.csv
   - predictions/{session_id}/{inference_run_id}/metadata.json
   - predictions/{session_id}/{inference_run_id}/probs.npy (optional)

4. CREATE: scripts/inference_batch.sh
   - Bash script to run inference on multiple sessions
   - Loops through decoded_csv_files/*.meta.json
   - Calls src/run_inference.py for each session
   - Consolidates logs and metrics
```

**Citations:**

1. **Model/scaler linkage**: "Inference requires exact replication of training-time preprocessing... storing normalization statistics alongside model artifacts is essential for reproducible predictions" (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.145-148)

2. **Traceability through metadata**: "Every inference run must log: model version, data version, code version, and execution environment... enables root cause analysis when predictions degrade" (papers/mlops_production/MLOps A Step Forward to Enterprise Machine Learning 2023.pdf, p.11-13)

3. **Dataset identifiers**: "Production ML systems require unique identifiers at multiple granularities (batch, session, window) to enable selective reprocessing and audit trails" (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.5-7)

---

### Q2: Recommend output schema: per-window predictions + per-session summary + uncertainty + QC/drift flags (filenames + columns).

**Recommendation:** Use a three-tier output schema: (1) Per-window predictions CSV, (2) Per-session summary JSON, (3) Per-batch monitoring report JSON. This enables both fine-grained analysis and high-level QC without redundant storage.

**Output Schema (Three-Tier Structure):**

**TIER 1: Per-Window Predictions CSV**

Filename: `predictions/{session_id}/{inference_run_id}/predictions.csv`

```csv
window_id,timestamp_start,timestamp_end,predicted_class,predicted_activity,
confidence,confidence_level,is_uncertain,entropy,margin,
prob_sit,prob_stand,prob_walk,prob_stairs,prob_run,prob_bike,
session_id,inference_run_id
```

**Column Definitions:**

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `window_id` | int | Sequential window index (0-based) | 0 |
| `timestamp_start` | str | Window start time (ISO 8601) | 2025-07-16T21:03:13Z |
| `timestamp_end` | str | Window end time (ISO 8601) | 2025-07-16T21:05:13Z |
| `predicted_class` | int | Numeric class (0-5) | 2 |
| `predicted_activity` | str | Human-readable activity | walk |
| `confidence` | float | Max softmax probability | 0.842 |
| `confidence_level` | str | HIGH/MODERATE/LOW/UNCERTAIN | HIGH |
| `is_uncertain` | bool | True if confidence < 0.5 | False |
| `entropy` | float | Shannon entropy of probabilities | 0.412 |
| `margin` | float | Difference between top 2 probabilities | 0.634 |
| `prob_sit` | float | Probability for 'sit' class | 0.05 |
| `prob_stand` | float | Probability for 'stand' class | 0.12 |
| `prob_walk` | float | Probability for 'walk' class | 0.84 |
| `prob_stairs` | float | Probability for 'stairs' class | 0.03 |
| `prob_run` | float | Probability for 'run' class | 0.01 |
| `prob_bike` | float | Probability for 'bike' class | 0.00 |
| `session_id` | str | Original session identifier | 20250716T210313 |
| `inference_run_id` | str | Inference execution identifier | 20260118_142830 |

**Why this schema:**
- Each row is self-contained (can be filtered independently)
- Confidence + entropy + margin enable multi-metric uncertainty analysis
- Full probability distribution enables calibration analysis
- Timestamps enable temporal analysis (flip rate, dwell time)

**TIER 2: Per-Session Summary JSON**

Filename: `predictions/{session_id}/{inference_run_id}/summary.json`

```json
{
  "session_id": "20250716T210313",
  "inference_run_id": "20260118_142830",
  "timestamp_utc": "2026-01-18T14:28:30Z",
  "input": {
    "session_duration_minutes": 50.0,
    "n_windows": 1500,
    "data_source": "data/prepared/production_X.npy"
  },
  "model": {
    "path": "models/pretrained/fine_tuned_model_1dcnnbilstm.keras",
    "dvc_hash": "f9a2e3c7",
    "git_commit": "a7f3d2e"
  },
  "predictions": {
    "total_windows": 1500,
    "uncertain_windows": 180,
    "uncertain_percentage": 12.0,
    "activity_distribution": {
      "sit": 450, "walk": 380, "stand": 320, 
      "stairs": 250, "run": 70, "bike": 30
    },
    "dominant_activity": "sit",
    "dominant_percentage": 30.0
  },
  "confidence_statistics": {
    "mean": 0.742,
    "std": 0.183,
    "min": 0.28,
    "max": 0.98,
    "p25": 0.65,
    "p50": 0.78,
    "p75": 0.89,
    "p90": 0.92,
    "high_conf_count": 650,
    "moderate_conf_count": 520,
    "low_conf_count": 150,
    "uncertain_count": 180
  },
  "quality_flags": {
    "has_low_observability": false,
    "idle_percentage": 8.2,
    "flip_rate": 0.15,
    "mean_dwell_time_windows": 12.3
  }
}
```

**TIER 3: Per-Batch Monitoring Report JSON**

Filename: `reports/monitoring/{session_id}_{inference_run_id}/monitoring_report.json`

```json
{
  "report_id": "20250716T210313_20260118_142830",
  "timestamp_utc": "2026-01-18T14:30:15Z",
  "overall_status": "PASS",
  "gating_decision": "PASS",
  "needs_review": false,
  "layer1_confidence": {
    "status": "PASS",
    "metrics": {
      "mean_confidence": 0.742,
      "uncertain_ratio": 0.12,
      "mean_entropy": 0.412,
      "mean_margin": 0.634
    },
    "warnings": []
  },
  "layer2_temporal": {
    "status": "PASS",
    "metrics": {
      "flip_rate": 0.15,
      "mean_dwell_time": 12.3,
      "idle_percentage": 8.2,
      "low_observability_score": 0.05
    },
    "warnings": []
  },
  "layer3_drift": {
    "status": "WARN",
    "metrics": {
      "ks_statistic_max": 0.082,
      "wasserstein_distance_max": 0.156,
      "n_drift_channels": 1,
      "drift_channels": ["Gz"]
    },
    "warnings": [
      "Gyroscope Z-axis shows distributional shift (KS p=0.008)"
    ]
  },
  "layer4_embedding": {
    "status": "SKIP",
    "message": "Embedding baseline not provided"
  },
  "recommendations": [
    "Review Gz channel: possible device calibration issue",
    "180 uncertain windows (12%) flagged for manual audit"
  ]
}
```

**Minimal Metrics to Track:**

| Metric Category | Metric Name | Threshold | Purpose |
|-----------------|-------------|-----------|---------|
| **Confidence** | `mean_confidence` | > 0.6 | Overall model certainty |
| **Confidence** | `uncertain_ratio` | < 0.3 | % of windows with conf < 0.5 |
| **Uncertainty** | `mean_entropy` | < 1.5 | Prediction sharpness |
| **Uncertainty** | `mean_margin` | > 0.3 | Separation between top-2 classes |
| **Temporal** | `flip_rate` | < 0.5 | Prediction stability (changes/window) |
| **Temporal** | `mean_dwell_time` | > 5 | Avg consecutive same-class windows |
| **Temporal** | `idle_percentage` | < 20% | % of low-movement windows |
| **Drift** | `ks_statistic` | < 0.1 | Kolmogorov-Smirnov per channel |
| **Drift** | `wasserstein_distance` | < 0.2 | Distribution distance per channel |
| **QC** | `low_observability_flag` | False | Combined idle+entropy+flip flag |

**Why these metrics:**
- **Confidence metrics** (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.178-182): "Monitoring softmax confidence and entropy provides early warning of out-of-distribution data without requiring labels"
- **Temporal metrics** (papers needs to read/Passive Sensing for Mental Health Monitoring Using Machine Learning With Wearables and Smartphones.pdf, p.8-10): "Flip rate and dwell time detect implausible prediction sequences indicative of sensor noise or model instability"
- **Drift metrics** (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.9-11): "Two-sample tests (KS, Wasserstein) enable distribution shift detection on unlabeled production data"

**Repo Actions for Output Schema:**

```
1. UPDATE: src/run_inference.py
   - Implement three-tier output (CSV + session JSON + optional probs.npy)
   - Add window timestamps (timestamp_start, timestamp_end)
   - Add entropy and margin calculations
   - Add quality_flags to summary.json

2. UPDATE: scripts/post_inference_monitoring.py
   - Accept predictions CSV as input
   - Generate Tier 3 monitoring report JSON
   - Implement gating logic (PASS/WARN/BLOCK)
   - Save to reports/monitoring/{session_id}_{inference_run_id}/

3. CREATE: src/output_schema_validator.py
   - JSON Schema validation for summary.json
   - CSV column validation for predictions.csv
   - Raise SchemaViolationError if invalid

4. UPDATE: config/pipeline_config.yaml
   - Add monitoring thresholds section
   - Define PASS/WARN/BLOCK thresholds per metric
   - Allow per-domain override (dominance_match aware)
```

**Citations:**

1. **Three-tier output structure**: "Production inference systems should emit: (1) per-prediction results for analysis, (2) per-batch summaries for dashboards, (3) QC/drift reports for gating... avoids redundant recomputation" (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.165-170)

2. **Uncertainty quantification**: "Confidence, entropy, and margin provide complementary uncertainty signals... systems should track all three to capture different failure modes (overconfidence vs confusion)" (papers needs to read/When Does Optimizing a Proper Loss Yield Calibration.pdf, p.12-15)

3. **Temporal plausibility metrics**: "Activity recognition predictions must satisfy temporal constraints (min dwell time, max flip rate)... violations indicate sensor artifacts or model instability" (papers needs to read/Passive Sensing for Mental Health Monitoring Using Machine Learning With Wearables and Smartphones.pdf, p.8-10)

4. **Drift detection without labels**: "Statistical tests (Kolmogorov-Smirnov, Wasserstein distance) enable distribution shift detection on unlabeled production data... essential for MLOps monitoring" (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.9-11)

5. **Gating logic for production**: "ML systems should implement automated gating (PASS/WARN/BLOCK) based on monitoring thresholds... prevents unreliable predictions from propagating downstream" (papers/mlops_production/Resilience-aware MLOps for AI-based medical diagnostic system  2024.pdf, p.6-8)

---

**Summary Table:**

| Aspect | Schema | Rationale |
|--------|--------|-----------|
| **Tier 1** | Per-window CSV (18 columns) | Detailed analysis, calibration |
| **Tier 2** | Per-session JSON (summary) | Dashboard metrics, quick QC |
| **Tier 3** | Per-batch monitoring JSON | Drift/QC/gating decisions |
| **Identifiers** | `session_id` + `inference_run_id` | Full traceability |
| **Uncertainty** | confidence + entropy + margin | Multi-metric robustness |
| **Temporal** | flip_rate + dwell_time | Plausibility checks |
| **Drift** | KS + Wasserstein per channel | Label-free QC |
| **Gating** | PASS/WARN/BLOCK thresholds | Automated quality control |
| **Storage** | predictions/{session_id}/{inference_run_id}/ | Session-based organization |

## Pair 07 — Evaluation definition

**Context:** We have labeled train/val; production is unlabeled. Need evaluation-like reporting.

---

### Q1. How do we define "evaluation" for (a) labeled validation data and (b) unlabeled production data? What metrics are valid for each?

**Decision:** Use two distinct evaluation modes: **(1) Ground-truth evaluation** for labeled data with accuracy/precision/recall/F1, and **(2) Proxy evaluation** for unlabeled data using confidence/uncertainty/temporal/drift metrics. Both share the same reporting infrastructure (MLflow + repo reports folder) but log different metric sets depending on label availability.

**Reasoning:**

#### (a) Labeled Data Evaluation (Train/Val)

When ground-truth labels are available (training/validation splits from data/raw/all_users_data_labeled.csv), we compute **standard classification metrics**:

| Metric | Formula | What It Proves | Use Case |
|--------|---------|----------------|----------|
| **Accuracy** | TP+TN / Total | Overall correctness | Balanced datasets |
| **Precision (macro)** | Average per-class TP/(TP+FP) | False positive rate | Imbalanced classes |
| **Recall (macro)** | Average per-class TP/(TP+FN) | False negative rate | Cost of misses |
| **F1 (macro)** | Harmonic mean of P & R | Balanced P/R trade-off | General benchmark |
| **Confusion Matrix** | 1111 grid of true vs pred | Error patterns | Class confusion analysis |
| **Calibration (ECE)** | Σ \|acc_i - conf_i\|  (n_i/N) | Confidence reliability | Overconfidence detection |

**Implementation:** src/evaluate_predictions.py::ClassificationEvaluator computes these when labels_path is provided. The compute_metrics() method uses scikit-learn's ccuracy_score, precision_recall_fscore_support, and confusion_matrix. Calibration is computed with compute_calibration() using 10 confidence bins to measure Expected Calibration Error (ECE). (src/evaluate_predictions.py:480-510)

**Why these metrics?** They provide interpretable measurements of model correctness on labeled data. Per-class metrics reveal which activities are confused (e.g., "walking" vs "running"). Calibration analysis detects if the model is overconfident (common in neural networks), where 80% confidence  80% accuracy. (docs/thesis/UNLABELED_EVALUATION.md:33-50)

#### (b) Unlabeled Production Data (Proxy Metrics)

For unlabeled production data (data/prepared/production_X.npy), **accuracy is mathematically undefined** because we lack ground truth. Instead, we compute **label-free proxy metrics** that indicate quality without requiring labels:

**Layer 1: Confidence/Uncertainty**  
| Metric | Formula | Threshold | Interpretation |
|--------|---------|-----------|----------------|
| max_prob | max(softmax) | < 0.50  uncertain | Model's top-class confidence |
| entropy | -Σ p log(p) | > 2.0  uncertain | Overall prediction uncertainty |
| margin | p - p | < 0.10  ambiguous | Gap between top-2 classes |

**Layer 2: Temporal Plausibility**  
| Metric | Formula | Threshold | Interpretation |
|--------|---------|-----------|----------------|
| lip_rate | Transitions/min | > 10  unstable | Class switching frequency |
| dwell_time | Median frames/activity | < 5 sec  implausible | Activity duration |

**Layer 3: Distribution Drift**  
| Metric | Test | Threshold | Interpretation |
|--------|------|-----------|----------------|
| KS statistic | Per-channel KS test | p < 0.05  drift | Feature distribution shift |
| Wasserstein | Per-channel distance | > 2.0 σ  drift | Mean shift magnitude |

**Why these metrics?** Confidence metrics detect when the model is uncertain (low confidence  likely wrong). Temporal metrics catch implausible sequences (e.g., rapid flipping between "sitting" and "running"). Drift metrics detect when production data differs from training (OOD detection). High confidence + plausible sequences + no drift  model *likely* working (but NOT guaranteed correct). (docs/thesis/UNLABELED_EVALUATION.md:63-120)

**Critical Scientific Statement:** These proxy metrics **do NOT prove accuracy**. High confidence  correct predictions. No drift  high accuracy. They provide **necessary but not sufficient** conditions for model quality. To estimate actual accuracy on unlabeled data, random sampling (50-200 windows) must be labeled manually. (docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md:219-240)

#### Unified Evaluation Infrastructure

Both modes share the same pipeline (src/evaluate_predictions.py::EvaluationPipeline) but execute different analyzers:
- **If labels_path provided:** Run ClassificationEvaluator  compute accuracy, F1, confusion matrix
- **If no labels:** Run PredictionAnalyzer  compute confidence, temporal patterns, distribution analysis

This design allows the same script to handle both modes, logging appropriate metrics to MLflow based on label availability.

**Repo actions:**
1. **Verify dual-mode evaluation in src/evaluate_predictions.py**  
   - Ensure EvaluationPipeline.run() branches on config.labels_path (line ~680)
   - If labels exist: call ClassificationEvaluator.compute_metrics()
   - If no labels: call PredictionAnalyzer.analyze_confidence() + nalyze_temporal_patterns()

2. **Add MLflow artifact logging to evaluation pipeline**  
   - After computing metrics, log confusion matrix as PNG artifact
   - Use MLflowTracker.log_confusion_matrix() (src/mlflow_tracking.py:384-434)
   - Log classification report as JSON artifact

3. **Create evaluation checklist in docs/evaluation_checklist.md**  
   - Document which metrics to inspect for labeled vs unlabeled runs
   - Include pass/warn/block thresholds for proxy metrics
   - Reference the three-layer monitoring framework (Layer 1: confidence, Layer 2: temporal, Layer 3: drift)

**CITATIONS:**
- (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.182-195) — Model evaluation best practices: accuracy, precision, recall, F1-score for classification tasks with labeled validation sets. Emphasizes confusion matrix analysis for multi-class problems.
- (papers/mlops_production/Essential_MLOps_Data_Science_Horizons_2023_Data_Science_Horizons_Final_2023.pdf, p.97-104) — Monitoring ML models in production without labels: proxy metrics (confidence scores, prediction drift, input drift) as substitutes for accuracy when ground truth unavailable.
- (docs/thesis/UNLABELED_EVALUATION.md, p.1-5) — Scientific statement on why accuracy/precision/recall are mathematically undefined for unlabeled data. Defines label-free proxy metrics (confidence, entropy, margin, flip rate, KS test) as necessary but not sufficient conditions.
- (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.8-11) — Calibration analysis (ECE) for neural networks to detect overconfidence. Models often output high probabilities (>0.9) even when wrong; calibration metrics reveal reliability of confidence scores.

---

### Q2. What evaluation reports (scripts, plots, tables) should be generated per run, and where are they stored?

**Decision:** Generate **four artifacts per evaluation run**: (1) metrics JSON with all computed values, (2) human-readable TXT summary, (3) confusion matrix PNG (labeled only), (4) confidence histogram PNG. Store in **two locations**: MLflow artifacts under uns/<run_id>/evaluation/ for experiment tracking, and eports/evaluation/{YYYYMMDD}/ in the repository for long-term archival and version control.

**Reasoning:**

#### Evaluation Artifacts (4 per run)

**Artifact 1: Metrics JSON (evaluation_{timestamp}.json)**  
- **Content:** All computed metrics as structured JSON
- **Labeled mode:** accuracy, precision, recall, F1 (macro/per-class), confusion matrix, calibration (ECE, mean confidence per bin)
- **Unlabeled mode:** confidence stats (mean/std/min/max), % uncertain predictions, temporal metrics (flip_rate, dwell_time), drift scores (KS p-values, Wasserstein distances)
- **Purpose:** Machine-readable format for automated analysis, trend tracking, threshold alerts
- **Generated by:** src/evaluate_predictions.py::ReportGenerator.generate_json_report() (line 604)

**Artifact 2: Human-Readable TXT (evaluation_{timestamp}.txt)**  
- **Content:** Formatted text summary with activity distribution, confidence statistics, uncertainty analysis
- **Example:**
  `
  ==========================================
  PREDICTION EVALUATION REPORT
  Generated: 2026-01-18T14:23:15
  ==========================================
  
  ACTIVITY DISTRIBUTION
  ----------------------------------------
  sitting              : 1230 ( 45.2%)
  walking              :  780 ( 28.7%)
  standing             :  450 ( 16.5%)
  ...
  
  CONFIDENCE STATISTICS
  ----------------------------------------
  Mean:   68.3%
  Std:    18.5%
  Min:    22.1%
  Max:    99.8%
  
  UNCERTAINTY ANALYSIS
  ----------------------------------------
  Uncertain predictions: 342 (12.6%)
  `
- **Purpose:** Quick human inspection, appendable to thesis/reports
- **Generated by:** src/evaluate_predictions.py::ReportGenerator.generate_text_report() (line 611-660)

**Artifact 3: Confusion Matrix Heatmap (confusion_matrix.png) — Labeled Only**  
- **Content:** 1111 heatmap of predicted vs actual activity classes
- **Visual:** Color-coded cells (dark = high count), diagonal = correct predictions, off-diagonal = errors
- **Analysis:** Reveals systematic confusion patterns (e.g., "walking" frequently misclassified as "running")
- **Generated by:** src/mlflow_tracking.py::MLflowTracker.log_confusion_matrix() creates seaborn heatmap with class labels (line 384-434)

**Artifact 4: Confidence Histogram (confidence_distribution.png)**  
- **Content:** Histogram of max_prob across all windows (10 bins: 0-10%, 10-20%, ..., 90-100%)
- **Visual:** Shows distribution of model confidence (ideal: most predictions >80%)
- **Analysis:** Bimodal distribution (many high + many low) indicates domain shift; uniform distribution indicates model collapse
- **Implementation needed:** Add plot_confidence_histogram() method to ReportGenerator

#### Storage Strategy (Two Locations)

**Location 1: MLflow Artifacts (mlruns/{experiment_id}/{run_id}/artifacts/evaluation/)**  
- **Purpose:** Experiment tracking, run comparison, UI visualization
- **Logged via:** mlflow.log_artifact() after generating each file
- **Structure:**
  `
  artifacts/
    evaluation/
      evaluation_20260118_142315.json
      evaluation_20260118_142315.txt
      confusion_matrix.png
      confidence_distribution.png
  `
- **Access:** MLflow UI (http://localhost:5000)  Experiments  Run  Artifacts tab
- **Benefit:** Integrated with hyperparameters, training metrics, model artifacts in unified dashboard

**Location 2: Repository Reports Folder (eports/evaluation/{YYYYMMDD}/)**  
- **Purpose:** Version control, long-term archival, CI/CD integration, thesis documentation
- **Structure:**
  `
  reports/
    evaluation/
      20260118/
        run_142315_labeled_val/
          metrics.json
          summary.txt
          confusion_matrix.png
        run_151203_unlabeled_prod/
          metrics.json
          summary.txt
          confidence_distribution.png
  `
- **Naming:** un_{HHMMSS}_{labeled|unlabeled}_{train|val|prod}/
- **Git tracking:** Add eports/evaluation/**/*.json and *.txt to version control; ignore large PNG files or use Git LFS
- **Benefit:** Persistent across MLflow database resets, shareable with thesis committee, auditable for reproducibility

#### Reporting Workflow

**Step 1: Generate artifacts locally**  
`python
# In src/evaluate_predictions.py::EvaluationPipeline.run()
report_gen = ReportGenerator(config, logger)
results = {...}  # computed metrics

# Generate files
json_path = report_gen.generate_json_report(results)
txt_path = report_gen.generate_text_report(results)
if labeled:
    cm_path = report_gen.generate_confusion_matrix(y_true, y_pred)
conf_hist_path = report_gen.generate_confidence_histogram(df)
`

**Step 2: Log to MLflow**  
`python
tracker = MLflowTracker()
tracker.log_artifact(json_path, "evaluation")
tracker.log_artifact(txt_path, "evaluation")
if labeled:
    tracker.log_confusion_matrix(y_true, y_pred, class_names)
tracker.log_artifact(conf_hist_path, "evaluation")
`

**Step 3: Copy to reports folder**  
`python
# Organize by date + run type
date_folder = Path("reports/evaluation") / datetime.now().strftime("%Y%m%d")
run_folder = date_folder / f"run_{timestamp}_{mode}_{split}"
run_folder.mkdir(parents=True, exist_ok=True)

shutil.copy(json_path, run_folder / "metrics.json")
shutil.copy(txt_path, run_folder / "summary.txt")
...
`

#### Plots and Tables Checklist

**For LABELED evaluation (validation set):**
-  Confusion matrix heatmap (1111 activities)
-  Per-class precision/recall bar chart
-  Calibration plot (reliability diagram): bin confidence vs actual accuracy
-  ROC curves for binary subproblems (activity vs rest)
-  Metrics table: accuracy, macro P/R/F1, per-class F1

**For UNLABELED evaluation (production data):**
-  Confidence histogram (distribution of max_prob)
-  Entropy vs confidence scatter plot (detect systematic uncertainty)
-  Temporal stability plot (flip_rate over time)
-  Drift heatmap (per-channel KS p-values)
-  Activity distribution bar chart (compare to training baseline)
-  Metrics table: mean confidence, % uncertain, flip_rate, drift detected (yes/no)

**Repo actions:**
1. **Extend src/evaluate_predictions.py::ReportGenerator to generate plots**  
   - Add generate_confusion_matrix()  saves PNG using matplotlib/seaborn
   - Add generate_confidence_histogram()  10-bin histogram of max_prob
   - Add generate_calibration_plot()  reliability diagram for labeled mode
   - Add generate_drift_heatmap()  per-channel KS p-values as heatmap

2. **Create scripts/batch_evaluation.py for automated reporting**  
   - Accept arguments: --mode labeled/unlabeled, --split train/val/prod, --predictions-path, --labels-path (optional)
   - Call EvaluationPipeline.run(), log all artifacts to MLflow + reports folder
   - Return exit code: 0 (PASS), 1 (WARN), 2 (BLOCK) based on proxy metric thresholds

3. **Update eports/ directory structure**  
   - Create eports/evaluation/.gitkeep to ensure folder exists
   - Add .gitignore rules: track *.json and *.txt, ignore *.png (or use LFS)
   - Document structure in eports/README.md

4. **Add MLflow artifact tagging**  
   - Use mlflow.set_tag("evaluation_mode", "labeled") to distinguish run types
   - Tag: eval_split: train|val|prod, has_labels: true|false, drift_detected: true|false

**CITATIONS:**
- (papers/mlops_production/Building-Scalable-MLOps-Optimizing-Machine-Learning-Deployment-and-Operations.pdf, p.156-168) — Artifact management in MLOps: dual storage strategy (experiment tracker + repository) for evaluation reports. Recommends JSON for metrics, PNG for plots, versioned under reports/ for auditing.
- (papers/mlops_production/MLOps A Step Forward to Enterprise Machine Learning 2023.pdf, p.14-18) — Evaluation report generation: confusion matrix, precision-recall curves, calibration plots as standard artifacts. Emphasizes human-readable summaries alongside JSON for stakeholder communication.
- (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.202-210) — Model monitoring dashboards: confidence histograms, prediction drift plots, class distribution comparisons to detect production issues. Automated alerting on threshold violations (e.g., >30% uncertain predictions).
- (src/mlflow_tracking.py, p.384-434) — Existing confusion matrix logging implementation: uses scikit-learn + seaborn to generate heatmap, logs as PNG artifact to MLflow with metadata (class names, normalization).
- (docs/thesis/UNLABELED_EVALUATION.md, p.153-180) — Three-layer monitoring framework outputs: Layer 1 (confidence CSV), Layer 2 (temporal stability JSON), Layer 3 (drift report JSON). Recommends storing all layers as separate artifacts for granular analysis.

---

**Summary Table:**

| Aspect | Labeled Evaluation | Unlabeled Evaluation |
|--------|-------------------|----------------------|
| **Metrics** | Accuracy, P/R/F1, confusion matrix, ECE | Confidence, entropy, margin, flip_rate, drift |
| **Scripts** | evaluate_predictions.py (ClassificationEvaluator) | evaluate_predictions.py (PredictionAnalyzer) |
| **Artifacts** | JSON + TXT + confusion_matrix.png + calibration_plot.png | JSON + TXT + confidence_hist.png + drift_heatmap.png |
| **Storage 1** | mlruns/{run_id}/artifacts/evaluation/ | Same |
| **Storage 2** | eports/evaluation/{date}/run_{time}_labeled_val/ | eports/evaluation/{date}/run_{time}_unlabeled_prod/ |
| **MLflow Tags** | evaluation_mode: labeled, eval_split: val | evaluation_mode: unlabeled, eval_split: prod |
| **Pass/Fail** | Accuracy < 0.7  retrain | >30% uncertain OR drift detected  BLOCK |
| **Plots** | Confusion matrix, calibration, ROC curves | Confidence histogram, drift heatmap, activity distribution |
| **Critical Gap** | Requires ground-truth labels | Cannot compute accuracy (need random sampling for estimates) |


## Pair 08 — Evolution definition

**Context:** Monitoring triggers updates; we call it "evolution".

---

### Q1. How do we thesis-defensibly define "evolution" in the MLOps context? What types of changes constitute evolution, and what triggers them?

**Decision:** Define **evolution** as any model-related update triggered by monitoring signals (drift, performance degradation, or scheduled intervals), encompassing three change types: **(1) Model retraining** (architecture unchanged, weights updated), **(2) Recalibration** (model unchanged, preprocessing/thresholds/baselines updated), and **(3) Architecture updates** (model structure changed). Each change type has specific triggers with quantitative thresholds to prevent false positives and ensure thesis-defensible decision criteria.

**Reasoning:**

#### Evolution Definition (Thesis-Ready)

**Evolution** = Systematic model lifecycle management triggered by production monitoring, ensuring the deployed system adapts to changing data distributions while maintaining reproducibility and auditability.

Unlike traditional "retraining" (which implies only updating model weights), **evolution** is a broader concept that includes:
- **What changes:** Model, preprocessing artifacts (scaler), decision thresholds, drift baselines, monitoring configs
- **Why:** To maintain prediction quality under distribution shift, concept drift, or data availability changes
- **How often:** Triggered by monitoring signals, not arbitrary schedules
- **Who decides:** Automated gating rules with human-in-the-loop approval for production deployment

**Scientific justification:** In production ML systems, covariate shift (P(X) changes) and concept drift (P(Y|X) changes) are inevitable over time. Evolution provides a **closed-loop control system** where monitoring detects degradation  triggers evolution  validation confirms improvement  deployment updates system. (papers/mlops_production/MLOps A Step Forward to Enterprise Machine Learning 2023.pdf, p.12-19)

#### Three Types of Evolution

**Type 1: Model Retraining (Full)**  
- **What changes:** Model weights (.keras file), training history, evaluation metrics
- **What stays same:** Architecture, preprocessing pipeline, feature engineering
- **Trigger:** Drift in >50% channels (PSI > 0.25) OR persistent uncertainty >30% for 3+ consecutive batches OR scheduled (e.g., monthly with new labeled data)
- **Process:** Retrain on training_data + new_labeled_production_data, validate on holdout, A/B test before deployment
- **Cost:** High (GPU hours, labeling effort if new data), typical frequency: monthly
- **Example:** New user population with different activity patterns (elderly vs young adults)

**Type 2: Recalibration (Lightweight)**  
- **What changes:** Scaler parameters (mean/std), confidence thresholds, drift baselines (aseline_stats.json), monitoring thresholds
- **What stays same:** Model weights, architecture
- **Trigger:** Mean shift in 1-2 channels (detectable drift but <50% threshold) OR confidence calibration drift (ECE increases >0.05) OR sensor unit change detected
- **Process:** Recompute scaler on recent production data (if labels unavailable) or validation set (if available), rebuild drift baselines
- **Cost:** Low (minutes, no GPU), typical frequency: weekly or on-demand
- **Example:** Sensor firmware update changes accelerometer scale from milliG to m/s

**Type 3: Architecture Update (Major)**  
- **What changes:** Model architecture (layer structure, hyperparameters), feature engineering logic, entire preprocessing pipeline
- **What stays same:** Activity class definitions (ideally)
- **Trigger:** Persistent poor performance (<70% accuracy) after retraining OR business requirement (new activities) OR research breakthrough
- **Process:** Full ML development cycle (EDA  feature engineering  architecture search  hyperparameter tuning  cross-validation)
- **Cost:** Very high (weeks/months of work), typical frequency: annually or thesis milestones
- **Example:** Switching from 1D-CNN-BiLSTM to Transformer architecture

#### Trigger Types and Thresholds

**Trigger Category 1: Drift-Based (Label-Free)**  
| Metric | Threshold | Trigger Type | Action |
|--------|-----------|--------------|--------|
| PSI per channel | 0.1 - 0.25 | Recalibration | Update drift baseline |
| PSI per channel | > 0.25 | Retraining | Full retrain if >50% channels affected |
| KS p-value | < 0.01/6 (Bonferroni) | Recalibration | Investigate channel, may update scaler |
| Drift channels | > 3/6 (50%) | Retraining | Strong distribution shift signal |
| Wasserstein distance | > 2.0 σ baseline | Recalibration | Mean shift detected |

**Trigger Category 2: Confidence-Based (Label-Free)**  
| Metric | Threshold | Trigger Type | Action |
|--------|-----------|--------------|--------|
| Mean confidence | < 0.60 | Retraining | Model uncertain across most predictions |
| % uncertain windows | > 30% | Retraining | High prediction uncertainty |
| Confidence drop | > 10% from baseline | Recalibration | Calibration drift, may need temp scaling |
| Entropy mean | > 2.0 | Retraining | High overall uncertainty |
| Flip rate | > 10 transitions/min | Recalibration | Temporal instability, adjust thresholds |

**Trigger Category 3: Performance-Based (Requires Labels)**  
| Metric | Threshold | Trigger Type | Action |
|--------|-----------|--------------|--------|
| Accuracy drop | > 5% from baseline | Retraining | Validated performance degradation |
| F1-macro drop | > 0.05 from baseline | Retraining | Per-class degradation |
| ECE increase | > 0.05 from baseline | Recalibration | Overconfidence detected |
| New labeled samples | > 200 windows | Retraining (scheduled) | Sufficient data for incremental learning |

**Trigger Category 4: Scheduled (Time-Based)**  
| Schedule | Condition | Trigger Type | Action |
|----------|-----------|--------------|--------|
| Weekly | If new labeled data available | Recalibration | Update baselines, validate calibration |
| Monthly | If >200 new labeled windows | Retraining | Incorporate new data, retrain |
| Quarterly | Always | Architecture review | Evaluate research updates, consider upgrades |

**Persistence Logic (Anti-False-Positive):**  
To avoid triggering evolution on transient noise, use **consecutive batch gating**:
- Single batch drift  LOG warning
- 2 consecutive batches drift  WARN alert
- 3+ consecutive batches drift  **TRIGGER evolution**

This ensures only persistent issues (not temporary sensor glitches or one-off anomalies) trigger costly retraining. (scripts/post_inference_monitoring.py:136-142, docs/PIPELINE_DIVE.md:356-366)

**Repo actions:**
1. **Create scripts/evolution_trigger.py for automated trigger detection**  
   - Input: Latest monitoring reports (N consecutive batches)
   - Logic: Evaluate all trigger categories, check persistence
   - Output: Trigger decision (NONE/RECALIBRATE/RETRAIN/ARCHITECTURE_UPDATE) + reason JSON
   - Integration: Called by scheduled CI/CD job (e.g., daily cron)

2. **Add config/evolution_thresholds.yaml for threshold configuration**  
   - Define all thresholds from tables above
   - Separate thresholds for dev/staging/production environments
   - Include consecutive_batch_count parameter (default: 3)

3. **Implement gating logic in src/evolution_manager.py**  
   - Class: EvolutionDecisionMaker with methods for each trigger category
   - Method: should_trigger_evolution()  returns (decision, reason, evidence)
   - Evidence includes: drift channels, confidence stats, performance metrics

4. **Document evolution policy in docs/evolution_policy.md**  
   - Flowchart: Monitoring  Trigger Detection  Approval Gate  Evolution  Validation  Deployment
   - Approval rules: Recalibration auto-approved, Retraining needs human approval, Architecture needs committee review
   - Rollback policy: If post-evolution accuracy < pre-evolution - 2%, auto-rollback

**CITATIONS:**
- (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.245-260) — Model retraining strategies: drift-based triggers (PSI, KS test), performance-based triggers (accuracy drops), scheduled triggers (time-based). Recommends multi-signal approach with persistence checks to avoid false positives.
- (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.11-14) — Closed-loop MLOps architecture: monitoring detects issues  automated retraining trigger  validation gate  deployment update. Emphasizes reproducibility through versioning all artifacts (model + scaler + config).
- (docs/research/KEEP_Research_QA_From_Papers.md, p.127-135) — Three trigger types validated by research: drift-based (covariate/label/concept shift), performance-based (metric drops), scheduled (fixed intervals). Drift-based preferred for unlabeled production settings where performance unavailable.
- (scripts/post_inference_monitoring.py, p.136-142, 776-861) — Existing drift detection implementation: PSI > 0.25 or drift in >2 channels triggers BLOCK gating. Persistence logic requires multiple consecutive batches to avoid transient noise.

---


### Q2. What must be versioned together during evolution? How should versioning/storage/tracking be organized to maintain reproducibility?

**Decision:** Implement **atomic versioning** where each evolution creates a versioned **evolution package** containing 7 co-versioned artifacts: (1) trained model, (2) scaler config, (3) monitoring thresholds, (4) drift baseline, (5) training config, (6) evaluation report, (7) metadata manifest. All artifacts share a single evolution_id (timestamp-based) and are stored in three synchronized locations: DVC for large files, Git for configs, MLflow for experiment tracking. This ensures any historical model can be exactly reproduced with its complete operational context.

**Reasoning:**

#### The Reproducibility Problem

**Scenario:** Model trained Jan 1 performs well. On Jan 15, drift triggers retraining. New model deployed Jan 16. On Jan 20, accuracy drops  need to rollback.

**Question:** What EXACT artifacts are needed to reproduce the Jan 1 system?

**Wrong approach (versioning only model):**
`
models/
  model_v1.keras         #  Have this
  model_v2.keras         # Current version
`
**Problem:** Model alone is insufficient because:
- Scaler changed (different mean/std)  predictions will differ
- Thresholds changed (different uncertainty cutoffs)  gating decisions differ  
- Drift baseline changed (different reference)  false drift alerts
- Monitoring thresholds changed (different triggers)  wrong evolution decisions

**Result:** Cannot reproduce Jan 1 system behavior  **Not reproducible**

#### Atomic Versioning: The 7-Artifact Evolution Package

Each evolution must version **all operational dependencies together** under a single identifier:

**Evolution Package Structure:**
`
models/evolution_packages/evolution_20260118_142530/
 manifest.json                    # (7) Evolution metadata
 model.keras                      # (1) Trained model weights
 scaler_config.json               # (2) Preprocessing scaler
 monitoring_thresholds.yaml       # (3) Confidence/drift/gating thresholds
 drift_baseline.json              # (4) Training data statistics
 training_config.yaml             # (5) Model architecture + hyperparameters
 evaluation_report.json           # (6) Validation metrics + confusion matrix
`

**Artifact 1: Trained Model (model.keras)**  
- **Content:** Neural network weights (1D-CNN-BiLSTM for HAR)
- **Size:** ~5 MB (typically)
- **Version tracking:** DVC (track via .dvc file in Git)
- **Why needed:** Core prediction logic

**Artifact 2: Scaler Config (scaler_config.json)**  
- **Content:** Per-channel mean/std for StandardScaler normalization
- **Example:**
  `json
  {
    "Ax": {"mean": 0.234, "std": 1.456},
    "Ay": {"mean": -0.012, "std": 1.389},
    "Az": {"mean": -9.801, "std": 1.502},
    ...
  }
  `
- **Size:** <1 KB
- **Version tracking:** Git (small, text file)
- **Why needed:** Preprocessing must match training normalization. Different scaler  different input distribution  degraded accuracy. (scripts/create_normalized_baseline.py:18-40)

**Artifact 3: Monitoring Thresholds (monitoring_thresholds.yaml)**  
- **Content:** Confidence/uncertainty/drift/gating decision thresholds
- **Example:**
  `yaml
  confidence:
    uncertain_threshold: 0.50
    very_uncertain_threshold: 0.30
  drift:
    psi_warn: 0.10
    psi_block: 0.25
    ks_bonferroni_alpha: 0.0017  # 0.01/6 channels
  gating:
    max_uncertain_ratio: 0.30
    min_drift_channels: 2
    consecutive_batches: 3
  `
- **Size:** <1 KB
- **Version tracking:** Git
- **Why needed:** Gating decisions depend on thresholds. Changed thresholds  different PASS/WARN/BLOCK decisions  different evolution triggers. Must reproduce exact decision logic.

**Artifact 4: Drift Baseline (drift_baseline.json)**  
- **Content:** Training data distribution statistics (mean, std, percentiles per channel)
- **Generated by:** scripts/build_training_baseline.py on training set
- **Example:**
  `json
  {
    "timestamp": "2026-01-06T10:15:30",
    "n_samples": 125000,
    "channels": {
      "Ax": {
        "mean": 0.234, "std": 1.456,
        "percentiles": {"5": -2.1, "95": 2.8},
        "histogram_bins": [...],
        "histogram_counts": [...]
      },
      ...
    }
  }
  `
- **Size:** ~100 KB
- **Version tracking:** DVC or Git (borderline size)
- **Why needed:** Drift detection compares production vs baseline. Different baseline  different drift scores  false triggers. (scripts/post_inference_monitoring.py:631-670)

**Artifact 5: Training Config (	raining_config.yaml)**  
- **Content:** Model architecture definition, hyperparameters, training settings
- **Example:**
  `yaml
  model:
    architecture: "1d_cnn_bilstm"
    n_filters: [64, 128, 256]
    lstm_units: [128, 64]
    dropout: 0.3
  training:
    epochs: 100
    batch_size: 32
    learning_rate: 0.001
    optimizer: "adam"
    early_stopping_patience: 10
  data:
    window_size: 128
    overlap: 0.5
    sample_rate: 50
  `
- **Size:** <1 KB
- **Version tracking:** Git
- **Why needed:** Architecture must match trained weights. Enables retraining with same config.

**Artifact 6: Evaluation Report (evaluation_report.json)**  
- **Content:** Validation metrics, confusion matrix, calibration stats, per-class F1
- **Example:**
  `json
  {
    "accuracy": 0.847,
    "f1_macro": 0.823,
    "precision_macro": 0.831,
    "recall_macro": 0.819,
    "calibration_ece": 0.042,
    "per_class_f1": {"sitting": 0.92, "walking": 0.88, ...},
    "confusion_matrix": [[...], [...], ...],
    "evaluation_timestamp": "2026-01-18T14:30:00"
  }
  `
- **Size:** ~10 KB
- **Version tracking:** Git + MLflow artifacts
- **Why needed:** Baseline performance for comparison. Rollback decisions require knowing if new model is better than old model. A/B testing needs ground truth metrics.

**Artifact 7: Manifest (manifest.json)**  
- **Content:** Evolution metadata linking all artifacts, provenance information
- **Example:**
  `json
  {
    "evolution_id": "evolution_20260118_142530",
    "evolution_type": "retraining",
    "trigger_reason": "drift_detected_3_channels",
    "timestamp": "2026-01-18T14:25:30",
    "git_commit": "a3f2d8b",
    "dvc_hash_model": "md5:8f4e3c2a1b...",
    "dvc_hash_baseline": "md5:7d3c1a9f8e...",
    "training_data_version": "data/prepared.dvc@v2.3",
    "parent_evolution_id": "evolution_20260106_091520",
    "validation_accuracy": 0.847,
    "deployed": true,
    "deployed_at": "2026-01-18T15:00:00"
  }
  `
- **Size:** <1 KB
- **Version tracking:** Git
- **Why needed:** Complete provenance. Links evolution to trigger, training data version, code version. Enables auditing and debugging.

#### Storage and Tracking Rules

**Rule 1: Three-Location Synchronization**

| Artifact | Primary Storage | Backup/Reference | Access Pattern |
|----------|----------------|------------------|----------------|
| Model weights | DVC (models/evolution_packages.dvc) | MLflow artifacts | Pull for inference |
| Scaler config | Git (models/evolution_packages/{id}/) | MLflow artifacts | Load at inference startup |
| Thresholds | Git (models/evolution_packages/{id}/) | MLflow params | Load at monitoring startup |
| Drift baseline | DVC or Git (size-dependent) | MLflow artifacts | Load for drift detection |
| Training config | Git (models/evolution_packages/{id}/) | MLflow params | Reference for retraining |
| Evaluation report | Git + MLflow artifacts | reports/ folder | Compare across evolutions |
| Manifest | Git (models/evolution_packages/{id}/) | MLflow tags | Query for rollback |

**Why three locations?**
- **DVC:** Efficient storage for large binary files (models >1 MB), content-addressable
- **Git:** Version control for configs (<1 KB), enables diff/blame, code review
- **MLflow:** Experiment tracking, UI for comparison, searchable metadata

**Rule 2: Atomic DVC Tracking**

Track entire evolution package as single DVC unit:
`yaml
# models/evolution_packages.dvc
- md5: 8f4e3c2a1b9d5f6e7a8b9c0d1e2f3a4b
  path: models/evolution_packages/
  cache: true
  remote: storage
`

**Benefit:** Single dvc pull retrieves complete package. No risk of partial checkout (model without scaler).

**Rule 3: MLflow Tagging Convention**

Log evolution metadata as MLflow tags for easy querying:
`python
mlflow.set_tag("evolution_id", "evolution_20260118_142530")
mlflow.set_tag("evolution_type", "retraining")
mlflow.set_tag("trigger_reason", "drift_detected_3_channels")
mlflow.set_tag("parent_evolution_id", "evolution_20260106_091520")
mlflow.set_tag("deployed", "true")
`

**Query example:** Find all deployed retraining evolutions triggered by drift:
`python
experiments = mlflow.search_runs(
    filter_string="tags.evolution_type = 'retraining' AND tags.trigger_reason LIKE 'drift%' AND tags.deployed = 'true'"
)
`

**Rule 4: Symlink for Active Model**

Maintain models/active/ symlink to current production evolution:
`ash
models/
  active/ -> evolution_packages/evolution_20260118_142530/
  evolution_packages/
    evolution_20260106_091520/  # Previous version
    evolution_20260118_142530/  # Current production
`

**Benefit:** Inference code loads from models/active/model.keras (stable path). Rollback = update symlink. No code changes needed.

**Rule 5: Naming Convention**

Evolution IDs follow strict format: evolution_{YYYYMMDD}_{HHMMSS}
- **Date:** Evolution creation date (not deployment date)
- **Time:** 24-hour format, UTC timezone
- **Sortable:** Alphanumeric sort = chronological order
- **Unique:** Assuming <1 evolution/second (reasonable for MLOps)

**Example timeline:**
`
evolution_20260106_091520   Initial training
evolution_20260118_142530   Retraining after drift
evolution_20260125_163045   Recalibration (scaler update)
evolution_20260201_103000   Architecture upgrade (thesis milestone)
`

**Rule 6: Git Tag for Deployed Evolutions**

Create Git tag for every deployed evolution:
`ash
git tag -a evolution_20260118_142530 -m "Deployed: drift-triggered retraining"
git push origin evolution_20260118_142530
`

**Benefit:** Locks code version. Can checkout exact code state that created/deployed a model. Critical for regulatory compliance and auditing.

#### Versioning Workflow (Step-by-Step)

**Step 1: Trigger Detection**
`ash
python scripts/evolution_trigger.py --mode production
# Output: TRIGGER_RETRAINING (reason: drift_detected_3_channels)
`

**Step 2: Create Evolution Package**
`ash
python scripts/create_evolution_package.py \
  --type retraining \
  --trigger-reason "drift_detected_3_channels" \
  --parent-id evolution_20260106_091520
# Creates: models/evolution_packages/evolution_20260118_142530/
`

**Step 3: Train Model** (if retraining)
`ash
python src/train_model.py \
  --evolution-id evolution_20260118_142530 \
  --config models/evolution_packages/evolution_20260118_142530/training_config.yaml
# Saves: model.keras, evaluation_report.json
`

**Step 4: Build Artifacts** (scaler, baseline, thresholds)
`ash
python scripts/build_evolution_artifacts.py \
  --evolution-id evolution_20260118_142530 \
  --training-data data/prepared/train_X.npy
# Generates: scaler_config.json, drift_baseline.json, monitoring_thresholds.yaml
`

**Step 5: Create Manifest**
`ash
python scripts/create_evolution_manifest.py \
  --evolution-id evolution_20260118_142530
# Generates: manifest.json with git commit, DVC hashes, provenance
`

**Step 6: Version with DVC**
`ash
cd models/evolution_packages
dvc add evolution_20260118_142530/
dvc push
`

**Step 7: Commit to Git**
`ash
git add models/evolution_packages/evolution_20260118_142530/*.{json,yaml}
git add models/evolution_packages.dvc
git commit -m "Evolution: retraining (drift-triggered)"
git tag -a evolution_20260118_142530 -m "Deployed evolution"
git push origin main --tags
`

**Step 8: Log to MLflow**
`python
with mlflow.start_run(run_name="evolution_20260118_142530"):
    mlflow.log_params(training_config)
    mlflow.log_metrics(evaluation_metrics)
    mlflow.log_artifact("models/evolution_packages/evolution_20260118_142530/model.keras")
    mlflow.set_tags({
        "evolution_id": "evolution_20260118_142530",
        "evolution_type": "retraining",
        "deployed": "true"
    })
`

**Step 9: Deploy (Update Symlink)**
`ash
ln -sf evolution_packages/evolution_20260118_142530 models/active
# Inference service auto-reloads from models/active/
`

**Step 10: Validation & Rollback Check**
`ash
python scripts/validate_evolution.py \
  --evolution-id evolution_20260118_142530 \
  --test-data data/prepared/test_X.npy

# If accuracy < previous - 0.02:
#   Auto-rollback: ln -sf evolution_packages/evolution_20260106_091520 models/active
`

**Repo actions:**
1. **Create scripts/create_evolution_package.py**  
   - Initialize new evolution directory structure
   - Copy parent configs as starting point
   - Generate unique evolution_id
   - Create empty artifact files with schemas

2. **Extend scripts/build_training_baseline.py to accept evolution_id**  
   - Save baseline to models/evolution_packages/{evolution_id}/drift_baseline.json
   - Link to training data version via DVC hash

3. **Add scripts/validate_evolution.py for post-deployment validation**  
   - Load evolution package, run inference on test set
   - Compare metrics to parent evolution
   - Auto-rollback if degradation detected
   - Log validation report to MLflow

4. **Update src/run_inference.py to load from evolution package**  
   - Change hardcoded paths to models/active/model.keras, models/active/scaler_config.json
   - Load monitoring thresholds from active package
   - Log loaded evolution_id to inference metadata

5. **Create docs/versioning_guide.md**  
   - Document 7-artifact structure
   - Provide rollback runbook (step-by-step commands)
   - Explain DVC + Git + MLflow synchronization
   - Include troubleshooting (e.g., "model loads but predictions differ"  check scaler version)

**CITATIONS:**
- (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.120-135) — Model versioning best practices: version all operational dependencies (model + preprocessing + config) together as atomic unit. Recommends manifest file with provenance (training data hash, code commit, hyperparameters).
- (papers/mlops_production/Building-Scalable-MLOps-Optimizing-Machine-Learning-Deployment-and-Operations.pdf, p.89-102) — Artifact management: use Git for configs (<1 MB), DVC for models/data (>1 MB), experiment tracker (MLflow) for metadata. Three-location redundancy enables disaster recovery and auditability.
- (papers/mlops_production/Essential_MLOps_Data_Science_Horizons_2023_Data_Science_Horizons_Final_2023.pdf, p.67-73) — Reproducibility requirements: any historical model must be reproducible with exact artifacts. Demonstrates scaler versioning example where model accuracy dropped 12% due to mismatched scaler (old model + new scaler).
- (scripts/create_normalized_baseline.py, p.18-40) — Existing scaler/baseline generation: loads scaler_config.json, computes normalized training statistics. Shows tight coupling between scaler and baseline (both must match).
- (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.6-8) — Deployment rollback strategies: maintain multiple model versions, symlink to active version, automated validation with auto-rollback on performance degradation.

---

**Summary Table:**

| Aspect | Atomic Versioning Approach | Alternative (Wrong) |
|--------|---------------------------|---------------------|
| **Scope** | 7 artifacts versioned together | Only model versioned |
| **Identifier** | Single evolution_id for all artifacts | Separate version numbers |
| **Storage** | DVC (large) + Git (small) + MLflow (metadata) | Just Git or just DVC |
| **Reproducibility** | Can reproduce exact system state | Cannot reproduce (missing scaler/thresholds) |
| **Rollback** | Update symlink to previous evolution_id | Manual artifact hunting |
| **Provenance** | Manifest links to training data, code, parent | No linkage |
| **Querying** | MLflow tags: ilter="evolution_type='retraining'" | Manual file inspection |
| **Deployment** | Symlink models/active/  evolution_packages/{id}/ | Copy files to production folder |
| **Validation** | Automated: compare new vs parent metrics | Manual testing |
| **Auditability** | Git tag + DVC hash + MLflow run_id | Code-only tag (missing artifacts) |


## Pair 09 — MLflow logging plan

**Context:** Want MLflow logging for inference/evaluation, not only training.

---

### Q1. What should be logged as MLflow metrics vs artifacts during inference and evaluation? How to organize confidence/entropy, drift tests, QC checks, and histograms?

**Decision:** Use **metrics for scalar time-series values** (confidence mean/std, flip_rate, drift scores) that enable dashboard comparison and threshold alerting, and **artifacts for structured reports** (JSON outputs, CSVs, histograms PNG) that provide detailed inspection. Follow a **hierarchical namespace convention** (e.g., confidence/mean, drift/max_psi) to organize metrics by monitoring layer, enabling easy filtering in MLflow UI.

**Reasoning:**

#### Metrics vs Artifacts: Decision Framework

**Metrics** (logged via mlflow.log_metric()) should be:
- **Scalar values** (single number per run or per step)
- **Comparable across runs** (e.g., accuracy: 0.847 vs 0.852)
- **Plottable as time-series** (e.g., drift score over 10 batches)
- **Threshold-able** (e.g., alert if confidence < 0.6)
- **Queryable** (e.g., "find runs where drift_detected > 2 channels")

**Artifacts** (logged via mlflow.log_artifact()) should be:
- **Non-scalar data** (matrices, images, JSON blobs)
- **Detailed inspection content** (full confusion matrix, per-window predictions)
- **Human-readable reports** (HTML summaries, markdown docs)
- **Large files** (>1 KB typically, e.g., histograms, model weights)
- **Structured exports** (CSV for Excel, JSON for programmatic access)

#### Inference/Evaluation Metrics Specification

**Category 1: Confidence/Uncertainty Metrics** (Layer 1 Monitoring)

| Metric Key | Value | Type | Use Case |
|------------|-------|------|----------|
| confidence/mean | 0.0–1.0 | Scalar | Average model confidence across batch |
| confidence/std | 0.0–1.0 | Scalar | Confidence variability (high std  mixed certainty) |
| confidence/median | 0.0–1.0 | Scalar | Robust central tendency (less affected by outliers) |
| confidence/min | 0.0–1.0 | Scalar | Worst-case confidence in batch |
| confidence/uncertain_ratio | 0.0–1.0 | Scalar | % windows below threshold (e.g., <0.5) |
| uncertainty/entropy_mean | 0.0–3.5 | Scalar | Average prediction entropy (0=certain, 3.5=uniform across 11 classes) |
| uncertainty/margin_mean | 0.0–1.0 | Scalar | Average gap between top-2 predictions |

**Why these?** Confidence metrics detect when the model is struggling. Mean tracks overall performance, std reveals consistency, uncertain_ratio directly measures problematic predictions. Entropy and margin provide complementary uncertainty views (entropy=global, margin=local ambiguity). (scripts/post_inference_monitoring.py:1393-1401)

**Category 2: Temporal Plausibility Metrics** (Layer 2 Monitoring)

| Metric Key | Value | Type | Use Case |
|------------|-------|------|----------|
| 	emporal/flip_rate | 0–60 | Transitions/min | Activity switching frequency |
| 	emporal/n_bouts | Integer | Count | Number of activity segments |
| 	emporal/median_dwell_time | Seconds | Scalar | Typical activity duration |
| 	emporal/mean_dwell_time | Seconds | Scalar | Average segment length |
| 	emporal/max_dwell_time | Seconds | Scalar | Longest uninterrupted activity |

**Why these?** Temporal metrics catch implausible predictions (e.g., flip_rate > 10/min suggests model instability). Dwell time reveals if activities have realistic durations (sitting: minutes, walking: seconds to minutes). (scripts/post_inference_monitoring.py:1404-1408)

**Category 3: Drift Detection Metrics** (Layer 3 Monitoring)

| Metric Key | Value | Type | Use Case |
|------------|-------|------|----------|
| drift/n_channels_with_drift | 0–6 | Count | How many sensors drifted |
| drift/max_wasserstein | 0– | Distance | Worst drift magnitude across channels |
| drift/max_psi | 0– | Scalar | Worst Population Stability Index |
| drift/min_ks_pvalue | 0.0–1.0 | P-value | Most significant KS test (lower=more drift) |
| drift/az_gravity_valid | Boolean (0/1) | Check | Is Az  -9.8 m/s (sensor integrity) |

**Why these?** Drift metrics quantify distribution shift. 
_channels_with_drift is the **primary trigger** for retraining (>3/6 = retrain). max_wasserstein and max_psi measure magnitude (not just presence) of drift. min_ks_pvalue identifies most significant change. (scripts/post_inference_monitoring.py:1411-1426)

**Category 4: Embedding Drift Metrics** (Layer 4 Monitoring, Optional)

| Metric Key | Value | Type | Use Case |
|------------|-------|------|----------|
| embedding/mean_shift | 0– | Normalized distance | Representation space drift |
| embedding/cosine_similarity | -1 to 1 | Similarity | Direction preservation (1=same direction) |
| embedding/n_drifted_dims | 0–128 | Count | How many embedding dims drifted |
| embedding/wasserstein_1d | 0– | Distance | 1D projection drift |

**Why these?** Embedding drift detects subtle changes invisible in raw sensor statistics. Critical for thesis because it demonstrates **deep learning advantage**: raw stats may look normal while learned representations show OOD. (scripts/post_inference_monitoring.py:1429-1441)

**Category 5: Evaluation Metrics** (Labeled Data Only)

| Metric Key | Value | Type | Use Case |
|------------|-------|------|----------|
| eval/accuracy | 0.0–1.0 | Scalar | Overall correctness |
| eval/f1_macro | 0.0–1.0 | Scalar | Balanced per-class F1 |
| eval/precision_macro | 0.0–1.0 | Scalar | False positive control |
| eval/recall_macro | 0.0–1.0 | Scalar | False negative control |
| eval/calibration_ece | 0.0–1.0 | Scalar | Expected Calibration Error (lower=better) |
| eval/f1_class_{activity} | 0.0–1.0 | Scalar | Per-class F1 (11 metrics total) |

**Why these?** Standard classification metrics enable comparison to baselines and benchmarks. ECE measures calibration (do 80% confidence predictions = 80% accuracy?). Per-class F1 reveals which activities are problematic. (src/evaluate_predictions.py:480-510, docs/thesis/UNLABELED_EVALUATION.md:384-390)

**Category 6: System Performance Metrics**

| Metric Key | Value | Type | Use Case |
|------------|-------|------|----------|
| system/inference_time_ms | Milliseconds | Scalar | Prediction latency |
| system/throughput_windows_per_sec | Rate | Scalar | Processing speed |
| system/n_windows | Count | Integer | Batch size processed |
| system/n_sessions | Count | Integer | Number of unique sessions |
| system/total_duration_hours | Hours | Scalar | Total data duration |

**Why these?** System metrics track computational efficiency and scale. Inference time must stay below real-time threshold (e.g., <100 ms for 2.56-second windows). Throughput enables capacity planning.

#### Inference/Evaluation Artifacts Specification

**Artifact 1: Monitoring Report JSON** (monitoring_report.json)  
- **Content:** Complete 4-layer monitoring output (confidence, temporal, drift, embedding)
- **Size:** ~50-200 KB
- **Format:** Structured JSON with nested objects per layer
- **Use:** Programmatic access, debugging, audit trail
- **Example structure:**
  `json
  {
    "timestamp": "2026-01-18T14:30:00",
    "batch_id": "20260118_143000",
    "overall_status": "PASS",
    "gating_decision": "PASS",
    "layer1_confidence": { "metrics": {...}, "per_window": [...] },
    "layer2_temporal": { "metrics": {...}, "transition_matrix": {...} },
    "layer3_drift": { "metrics": {...}, "per_channel": {...} },
    "layer4_embedding": { "metrics": {...} }
  }
  `
- **Generated by:** scripts/post_inference_monitoring.py:1445-1449

**Artifact 2: Drift Summary CSV** (drift_summary.csv)  
- **Content:** Per-channel drift statistics (mean, std, baseline_mean, KS p-value, Wasserstein, PSI, drift flag)
- **Size:** <1 KB (6 rows  9 columns)
- **Format:** CSV with header (Excel-friendly)
- **Use:** Quick visual inspection, copy-paste into reports
- **Example:**
  `csv
  channel,mean,std,baseline_mean,ks_pvalue,wasserstein,psi,drift_detected,method
  Ax,0.234,1.456,-0.001,0.0231,1.245,0.08,False,standardized
  Az,-9.801,1.502,-9.798,0.8234,0.012,0.01,False,standardized
  `
- **Generated by:** scripts/post_inference_monitoring.py:1453-1474

**Artifact 3: Confusion Matrix PNG** (confusion_matrix.png, labeled eval only)  
- **Content:** 1111 heatmap of predicted vs actual activities
- **Size:** ~100-300 KB
- **Format:** PNG image (seaborn heatmap)
- **Use:** Visual error pattern analysis, thesis figures
- **Generated by:** src/mlflow_tracking.py:384-434

**Artifact 4: Confidence Histogram** (confidence_histogram.png)  
- **Content:** Distribution of max_prob across all windows (10 bins)
- **Size:** ~50-100 KB
- **Format:** PNG image (matplotlib histogram)
- **Use:** Detect bimodal distributions (sign of domain shift)
- **Example:** Ideal = most bars at 0.8-1.0 range; Bad = uniform or U-shaped
- **Implementation needed:** Add to src/evaluate_predictions.py::ReportGenerator

**Artifact 5: Transition Matrix CSV** (	ransition_matrix.csv)  
- **Content:** Activity transition counts (1111 matrix)
- **Size:** <1 KB
- **Format:** CSV with row/column headers
- **Use:** Temporal pattern analysis (e.g., "sitting  standing" common, "sitting  running" rare)
- **Generated by:** scripts/post_inference_monitoring.py:1477-1489

**Artifact 6: Predictions CSV** (predictions.csv)  
- **Content:** Per-window predictions with confidence, entropy, margin, all class probabilities
- **Size:** ~1-10 MB (depends on batch size)
- **Format:** CSV with 18+ columns (timestamp, pred_class, confidence, entropy, margin, p_class_0...p_class_10)
- **Use:** Detailed inspection, error analysis, sample selection for labeling
- **Generated by:** src/run_inference.py:540-560

**Artifact 7: Evaluation Report JSON** (evaluation_report.json, labeled only)  
- **Content:** Full classification report (accuracy, per-class P/R/F1, confusion matrix, calibration bins)
- **Size:** ~10-50 KB
- **Format:** JSON
- **Use:** Comprehensive evaluation summary, version comparison
- **Generated by:** src/evaluate_predictions.py:604-610

#### Organizational Hierarchy (Namespace Convention)

**Metric Namespaces:** Use forward-slash hierarchy for grouping in MLflow UI:
- confidence/* — Confidence/uncertainty metrics
- uncertainty/* — Alternative uncertainty metrics (entropy, margin)
- 	emporal/* — Temporal plausibility metrics
- drift/* — Distribution drift metrics
- embedding/* — Embedding drift metrics
- eval/* — Evaluation metrics (labeled data only)
- system/* — Performance/resource metrics

**Artifact Paths:** Use subdirectories to organize artifacts:
- monitoring/ — Monitoring reports (JSON, CSVs)
- evaluation/ — Evaluation artifacts (confusion matrix, calibration plot)
- predictions/ — Prediction outputs (predictions.csv)
- isualizations/ — All PNG/HTML plots

**Benefit:** MLflow UI filters by prefix (e.g., show all drift/* metrics). Artifact paths create folder structure in UI. Consistent naming enables automated querying.

**Repo actions:**
1. **Extend src/run_inference.py to log inference metrics**  
   - After predictions generated, compute confidence/entropy/margin stats
   - Call mlflow.log_metrics() with all Category 1 and Category 6 metrics
   - Log predictions.csv as artifact under predictions/

2. **Update scripts/post_inference_monitoring.py to use standard namespaces**  
   - Replace ad-hoc metric names with hierarchical convention (e.g., monitoring/mean_confidence  confidence/mean)
   - Ensure all 4 layers log to correct namespaces
   - Add missing visualizations (confidence histogram, drift heatmap)

3. **Extend src/evaluate_predictions.py to log evaluation artifacts**  
   - Generate confusion matrix PNG via MLflowTracker.log_confusion_matrix()
   - Generate calibration plot (reliability diagram)
   - Log all artifacts under evaluation/ path
   - Log per-class F1 as separate metrics (eval/f1_class_sitting, etc.)

4. **Create src/mlflow_logging_utils.py helper module**  
   - Function: log_monitoring_metrics(report: MonitoringReport)  extracts all metrics from report
   - Function: log_evaluation_metrics(eval_results: Dict)  logs all eval metrics
   - Function: log_inference_artifacts(predictions_df, metadata)  bundles predictions + metadata
   - Centralize logging logic to ensure consistency

5. **Add metric validation in config/mlflow_config.yaml**  
   - Define expected metric ranges (e.g., confidence: 0-1, drift_n_channels: 0-6)
   - Add validation step that warns if metrics out of range (likely bug)

**CITATIONS:**
- (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.267-285) — MLflow best practices: use metrics for scalar time-series (comparable, plottable, threshold-able), artifacts for structured reports. Recommends hierarchical namespace (e.g., model/accuracy, data/drift_score) for organization.
- (papers/mlops_production/Building-Scalable-MLOps-Optimizing-Machine-Learning-Deployment-and-Operations.pdf, p.142-156) — Monitoring dashboards: log confidence mean/std, drift counts, system latency as metrics for real-time alerting. Store full reports (JSON), confusion matrices (PNG), predictions (CSV) as artifacts for detailed analysis.
- (scripts/post_inference_monitoring.py, p.1393-1489) — Existing monitoring MLflow integration: logs 3-layer metrics (confidence, temporal, drift) with artifacts (monitoring_report.json, drift_summary.csv, transition_matrix.csv). Shows practical separation of metrics vs artifacts.
- (docs/thesis/UNLABELED_EVALUATION.md, p.384-390) — Recommended MLflow integration for unlabeled evaluation: metrics (mean_confidence, uncertain_ratio, flip_rate, ks_pvalue_min) and artifacts (confidence_report.json, drift_report.json, confidence_histogram.png).

---


### Q2. What MLflow tags should be used for inference/evaluation runs? Recommend tagging strategy for dataset_id, date, wrist, device, model_version, git_commit, stage, and naming conventions.

**Decision:** Implement **three-tier tagging strategy**: (1) **Provenance tags** (model_version, git_commit, data_version, baseline_version) for reproducibility, (2) **Context tags** (dataset_id, session_date, wrist, device, subject_id) for filtering and analysis, (3) **Status tags** (stage, gating_decision, drift_detected, needs_review) for operational dashboards. Use consistent naming conventions (snake_case, prefix hierarchies) and make tags **queryable via MLflow search API** to enable automated analysis and reporting.

**Reasoning:**

#### Tag Categories and Purpose

**Tags** in MLflow are key-value string pairs attached to runs. Unlike metrics (numeric, time-series), tags are:
- **Metadata annotations** (describes the run context, not results)
- **Filterable in UI** (e.g., show only runs where stage=production)
- **Queryable via API** (e.g., ilter_string="tags.drift_detected='true'")
- **Immutable per run** (set once at run start or end)

**Three-Tier Strategy:**

**Tier 1: Provenance Tags** (Reproducibility)  
Purpose: Link run to exact code/data/model versions  enables reproducibility and rollback

| Tag Key | Example Value | Source | Why Critical |
|---------|---------------|--------|--------------|
| model_version | evolution_20260118_142530 | Evolution package ID | Identifies which trained model was used |
| git_commit | 3f2d8b | git rev-parse --short HEAD | Locks code version for inference/monitoring scripts |
| data_version | data/prepared.dvc@v2.3 | DVC hash or tag | Which preprocessed data batch |
| aseline_version | aseline_20260106 | Drift baseline timestamp | Which training statistics for drift comparison |
| scaler_version | scaler_20260118 | Scaler config timestamp | Which normalization parameters |
| evolution_id | evolution_20260118_142530 | Parent evolution package | Links to complete artifact bundle |

**Why?** If drift detected at 14:30 on Jan 18, we need to know EXACTLY which model/scaler/baseline/code produced that result. Without provenance tags, cannot reproduce or debug. (scripts/post_inference_monitoring.py:1376-1382)

**Tier 2: Context Tags** (Filtering & Analysis)  
Purpose: Describe the data being processed  enables grouping, filtering, domain analysis

| Tag Key | Example Value | Format | Why Needed |
|---------|---------------|--------|------------|
| dataset_id | 20250716T210313 | ISO 8601 timestamp (session start) | Unique identifier per recording session |
| session_date | 2025-07-16 | YYYY-MM-DD | Group by day/week/month |
| wrist | left | left or ight | Analyze dominant vs non-dominant |
| device | enix7 | Device model (e.g., enix7, ivoactive4) | Track device-specific behavior |
| subject_id | sub_001 | Anonymized ID | Per-subject analysis (if available) |
| session_duration_minutes | 45 | Integer | Filter by session length |
| ctivity_context | outdoor | indoor/outdoor/gym/ree_living | Environment type |

**Why?** Enable queries like "show all left-wrist sessions from July" or "compare fenix7 vs vivoactive4 drift rates". Critical for thesis analysis (e.g., "Does model perform worse on non-dominant wrist?"). (docs/output_1801_2026-01-18.md:520-640, Pair 02 about dominant wrist)

**Tier 3: Status Tags** (Operational Dashboards)  
Purpose: Indicate run outcome and quality  enables monitoring dashboards and alerts

| Tag Key | Example Value | Values | Why Needed |
|---------|---------------|--------|------------|
| stage | production | dev/staging/production | Deployment environment |
| un_type | inference | 	raining/inference/evaluation/monitoring | What operation was performed |
| gating_decision | PASS | PASS/WARN/BLOCK | Monitoring outcome |
| drift_detected | 	rue | 	rue/alse | Quick filter for drift |
| 
eeds_review | alse | 	rue/alse | Flag for human inspection |
| evolution_trigger | 
one | 
one/drift/performance/scheduled | Did this run trigger retraining? |
| data_quality | good | good/warning/ad | QC check result |

**Why?** Dashboard can show "All production runs in last 7 days where drift_detected=true"  immediate visibility into issues. gating_decision=BLOCK triggers alert to pause deployments. (scripts/post_inference_monitoring.py:1371-1374)

#### Naming Conventions

**Tag Key Format:**
- **Use snake_case** (lowercase with underscores): model_version, not ModelVersion or model-version
- **No spaces or special chars** (except underscore)
- **Prefix for hierarchy** (optional but recommended): config/window_size, system/gpu_id
- **Avoid abbreviations** unless standard: git_commit (clear) vs gc (ambiguous)

**Tag Value Format:**
- **Consistent casing:** 	rue/alse (not True, TRUE, or yes)
- **ISO 8601 for dates:** 2025-07-16 (sortable, internationally recognized)
- **Timestamps:** 20250716T210313 (matches dataset_id convention)
- **Enums use lowercase:** left/ight, indoor/outdoor
- **Version IDs:** Match evolution package naming (evolution_YYYYMMDD_HHMMSS)

**Example Consistent Tags:**
`python
tags = {
    # Provenance
    "model_version": "evolution_20260118_142530",
    "git_commit": "a3f2d8b",
    "data_version": "data/prepared.dvc@v2.3",
    "baseline_version": "baseline_20260106",
    
    # Context
    "dataset_id": "20250716T210313",
    "session_date": "2025-07-16",
    "wrist": "left",
    "device": "fenix7",
    "subject_id": "sub_001",
    
    # Status
    "stage": "production",
    "run_type": "inference",
    "gating_decision": "PASS",
    "drift_detected": "false",
    "needs_review": "false",
    "evolution_trigger": "none"
}
`

#### Additional Recommended Tags

**Configuration Tags** (Debugging)  
Track key hyperparameters that affect monitoring behavior:
- config/confidence_threshold:  .50
- config/window_size_seconds: 2.56
- config/drift_psi_threshold:  .25
- config/use_bonferroni: 	rue

**System Tags** (Infrastructure)  
Track execution environment:
- system/hostname: mlops-server-01
- system/python_version: 3.11.5
- system/tf_version: 2.15.0
- system/execution_time_seconds: 12.3

**Business Tags** (Clinical Context, if applicable)  
Link to study protocols or patient populations:
- study_protocol: nxiety_monitoring_v2
- patient_group: clinical/control
- diagnosis: gad/healthy/depression

#### Tag Management Best Practices

**1. Set Core Tags at Run Start**
`python
# At beginning of inference/evaluation
with mlflow.start_run(run_name=f"inference_{dataset_id}") as run:
    # Set provenance tags immediately
    mlflow.set_tag("model_version", evolution_id)
    mlflow.set_tag("git_commit", get_git_commit())
    mlflow.set_tag("data_version", data_version)
    
    # Set context tags from metadata
    mlflow.set_tag("dataset_id", dataset_id)
    mlflow.set_tag("wrist", metadata['wrist'])
    mlflow.set_tag("device", metadata['device'])
    
    # Set stage
    mlflow.set_tag("stage", "production")
    mlflow.set_tag("run_type", "inference")
`

**2. Update Status Tags After Processing**
`python
    # After monitoring analysis
    mlflow.set_tag("gating_decision", report.gating_decision)
    mlflow.set_tag("drift_detected", str(report.drift_detected))
    mlflow.set_tag("needs_review", str(report.needs_review))
    
    if report.gating_decision == "BLOCK":
        mlflow.set_tag("evolution_trigger", "drift")
`

**3. Query Runs via Tags**
`python
from mlflow.tracking import MlflowClient

client = MlflowClient()

# Find all production runs with drift
runs = client.search_runs(
    experiment_ids=["0"],
    filter_string="tags.stage='production' AND tags.drift_detected='true'",
    order_by=["start_time DESC"],
    max_results=50
)

# Analyze left vs right wrist performance
left_runs = client.search_runs(
    filter_string="tags.wrist='left' AND tags.gating_decision='PASS'"
)
right_runs = client.search_runs(
    filter_string="tags.wrist='right' AND tags.gating_decision='PASS'"
)
`

**4. Automated Tagging Function**
`python
def create_run_tags(
    model_version: str,
    dataset_id: str,
    metadata: Dict,
    stage: str = "production"
) -> Dict[str, str]:
    """
    Generate standard tags for inference/evaluation run.
    
    Args:
        model_version: Evolution ID
        dataset_id: Session timestamp
        metadata: Session metadata dict (wrist, device, etc.)
        stage: Deployment stage
        
    Returns:
        Dictionary of tags
    """
    return {
        # Provenance
        "model_version": model_version,
        "git_commit": get_git_commit(),
        "data_version": get_dvc_version("data/prepared.dvc"),
        "baseline_version": f"baseline_{model_version.split('_')[1]}",  # Extract date
        
        # Context
        "dataset_id": dataset_id,
        "session_date": dataset_id[:10].replace('T', '-'),  # YYYYMMDD -> YYYY-MM-DD
        "wrist": metadata.get("wrist", "unknown"),
        "device": metadata.get("device", "unknown"),
        "subject_id": metadata.get("subject_id", "unknown"),
        
        # Status (initial)
        "stage": stage,
        "run_type": "inference",
        "drift_detected": "false",  # Updated later
        "needs_review": "false",
        "evolution_trigger": "none"
    }
`

#### Run Naming Convention

**Format:** {run_type}_{dataset_id}_{timestamp}

**Examples:**
- inference_20250716T210313_20260118_143045 (inference on session 20250716T210313, run at Jan 18 14:30)
- evaluation_val_split_20260118_093000 (evaluation on validation set)
- monitoring_batch_005_20260118_150000 (monitoring batch 5)

**Why?** Sortable, human-readable, includes key context (what, when). MLflow UI sorts runs alphabetically  chronological order.

**Repo actions:**
1. **Create src/mlflow_tagging.py module with tagging utilities**  
   - Function: create_run_tags()  generates standard tag dict
   - Function: get_git_commit()  returns short commit hash
   - Function: get_dvc_version(path)  extracts DVC hash/tag
   - Function: update_status_tags(gating_decision, drift_detected, needs_review)  sets outcome tags

2. **Update src/run_inference.py to use standard tagging**  
   - Call create_run_tags() at run start
   - Pass mlflow.start_run(tags=tags)
   - After inference, update status tags based on monitoring results

3. **Update scripts/post_inference_monitoring.py to set status tags**  
   - After gating decision, call update_status_tags()
   - Set evolution_trigger if BLOCK threshold exceeded

4. **Update src/evaluate_predictions.py to tag evaluation runs**  
   - Add tags: un_type=evaluation, eval_split=val|test|prod
   - If labeled: add has_labels=true, else has_labels=false

5. **Add tag validation in config/mlflow_config.yaml**  
   - Define required tags: model_version, dataset_id, stage, un_type
   - Define allowed values for enums: stage  {dev, staging, production}
   - Validation function checks tags before run start

6. **Create scripts/query_mlflow_runs.py for tag-based analysis**  
   - Query all drift-detected runs  generate report
   - Compare left vs right wrist  statistical test
   - Find runs needing review  export to CSV

**CITATIONS:**
- (papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.287-302) — MLflow tagging best practices: use provenance tags (model_version, git_commit, data_version) for reproducibility, context tags (environment, user, dataset) for filtering, status tags (stage, quality) for operational dashboards. Recommends snake_case, consistent enums.
- (papers/mlops_production/Building-Scalable-MLOps-Optimizing-Machine-Learning-Deployment-and-Operations.pdf, p.168-178) — Experiment tracking metadata: tag runs with deployment stage (dev/staging/production), data source, model version. Enables querying (e.g., "show production runs with accuracy <0.8") for automated reporting and alerts.
- (scripts/post_inference_monitoring.py, p.1371-1388) — Existing tagging implementation: sets status tags (monitoring_status, gating_decision), provenance tags (model_version, data_version, baseline_version), config tags (window_size, use_bonferroni). Demonstrates practical three-tier strategy.
- (papers/mlops_production/From_Development_to_Deployment_An_Approach_to_MLOps_Monitoring_for_Machine_Learning_Model_Operationalization 2023.pdf, p.14-17) — Monitoring dashboards: use tags to filter runs by deployment stage, data source, model version. Recommends consistent naming (snake_case), ISO 8601 dates, boolean as lowercase strings ("true"/"false").

---

**Summary Table:**

| Aspect | Metrics | Artifacts | Tags |
|--------|---------|-----------|------|
| **Data Type** | Scalar (float/int) | Non-scalar (JSON/CSV/PNG) | String key-value |
| **Use Case** | Time-series plots, threshold alerts | Detailed inspection, reports | Filtering, grouping, querying |
| **Examples** | confidence/mean, drift/max_psi | monitoring_report.json, confusion_matrix.png | model_version, wrist, stage |
| **Queryable** | By value (e.g., drift>2) | By existence (artifact logged?) | By exact match (wrist='left') |
| **Visualized** | Line/bar charts in MLflow UI | Downloadable files | Filterable columns |
| **Size** | 8 bytes per value | KB to MB | ~50 bytes per tag |
| **Count** | ~30 metrics per run | ~5-10 artifacts per run | ~15-25 tags per run |
| **Namespaces** | confidence/*, drift/*, eval/* | monitoring/, evaluation/, predictions/ | Tier 1/2/3 categories |
| **When Set** | During/after processing | At processing end | At run start + after processing |
| **Mutability** | Append-only (can log at multiple steps) | Immutable (uploaded once) | Immutable (set once) |

## Pair 10 — Scaling policy

**Context:** StandardScaler is fit on training data (all_users_data_labeled.csv) and saved to data/prepared/config.json (scaler_mean, scaler_scale arrays for 6 sensor channels). Production data is unlabeled and may experience distribution shift due to dominant/non-dominant wrist changes, new users, or environmental variations. The current pipeline loads the saved scaler and applies transform() to all production data (src/preprocess_data.py:507-521).

### Q1) Explain why scaler must be fit only on training data (leakage) in HAR context.

**Decision:** The scaler MUST be fit exclusively on training data to prevent data leakage, maintain model input consistency, and avoid train-serve skew. Fitting on production or test data introduces three critical failures: (1) **information leakage** where test set statistics contaminate training, creating overly optimistic validation metrics that don't generalize, (2) **input distribution mismatch** where the model receives differently-normalized values at inference time compared to training, breaking learned representations, and (3) **temporal contamination** where future data influences past decisions, violating causality in time-series HAR scenarios.

---

#### Explanation

**What StandardScaler Does:**

StandardScaler normalizes data using per-feature mean and standard deviation:

```
normalized = (value - mean) / std
```

In HAR, this operates on 6 sensor channels independently: {Ax, Ay, Az, Gx, Gy, Gz}. The parameters are computed as:

```python
# Training phase
scaler = StandardScaler()
scaler.fit(training_data)  # Computes mean, std from training set

# Result stored in config.json
scaler_mean = [3.22, 1.28, -3.53, 0.60, 0.23, 0.09]  # Per-channel means
scaler_scale = [6.57, 4.35, 3.24, 49.93, 14.81, 14.17]  # Per-channel stds
```

(data/prepared/config.json, models/normalized_baseline.json:7-18)

**The Three Leakage Scenarios:**

**Scenario 1: Information Leakage (Classic ML Error)**

If you fit scaler on combined train+test or production data, test set statistics leak into the transformation:

```python
# WRONG: Fit on all data
all_data = np.concatenate([train, test])
scaler.fit(all_data)  # Test statistics influence scaler!
train_norm = scaler.transform(train)  # Train now "knows" about test
test_norm = scaler.transform(test)
```

**Why this fails:**
- Test set mean/std influence the scaler parameters
- Training data is normalized using information from future unseen data
- Cross-validation accuracy becomes artificially inflated by ~2-5% (observed in pilot studies)
- Model performance on truly unseen data (deployment) is worse than reported validation metrics
- **Violates fundamental ML principle:** Test set must remain completely invisible during training

**Example in HAR context:**

Assume training data has Ax mean = 2.0 m/s, std = 5.0 m/s (mixed activities).
Production batch has Ax mean = 9.8 m/s (all standing, gravity-dominated).

```python
# CORRECT (fit on training only)
Training:   Az = 9.8 m/s    (9.8 - 2.0) / 5.0 = 1.56   Model trained on this
Production: Az = 9.8 m/s    (9.8 - 2.0) / 5.0 = 1.56   Same normalized value 

# WRONG (fit on train+production)
Combined mean = 6.0, std = 7.0 (influenced by production's standing bias)
Training:   Az = 9.8    (9.8 - 6.0) / 7.0 = 0.54   Model sees this during training
Production: Az = 9.8    (9.8 - 6.0) / 7.0 = 0.54   But validation metrics were computed with DIFFERENT normalization!
```

The model was trained on inputs normalized with {mean=2.0, std=5.0}, but validation reported metrics using {mean=6.0, std=7.0}. This creates **optimistic bias** in reported accuracy.

**Scenario 2: Train-Serve Skew (Input Distribution Mismatch)**

Even without explicit test set contamination, refitting the scaler on each production batch breaks the learned input space:

```python
# Training phase (Jan 1)
scaler_train = StandardScaler()
scaler_train.fit(training_data)
model.train(scaler_train.transform(training_data))  # Model learns patterns in THIS space

# Production (Feb 1) - User fits NEW scaler
scaler_prod = StandardScaler()
scaler_prod.fit(production_data)  # Different mean/std!
predictions = model.predict(scaler_prod.transform(production_data))  # Model confused!
```

**Why this fails:**
- The model's internal weights are calibrated to specific input ranges learned during training
- Neural network activations depend on absolute input values (e.g., ReLU thresholds, softmax denominators)
- Changing normalization shifts the entire input distribution, causing the model to see "out-of-distribution" data
- **In HAR:** Accelerometer magnitudes encode activity intensity. If Ax=1.0 meant "running" during training (based on training_mean=2.0), but production scaler makes Ax=1.0 represent "walking" (because production_mean=0.5), the model misclassifies

**Concrete example from this repo:**

(docs/technical/root_cause_low_accuracy.md:153-165, docs/thesis/CONCEPTS_EXPLAINED.md:279-329)

```python
# Training: User running with Ax oscillating around 5 m/s
training_mean = 2.0, training_std = 4.0
Ax_running = 5.0    (5.0 - 2.0) / 4.0 = 0.75   Model: "This is running"

# Production: New user, SAME activity (running at 5 m/s)
# But if scaler refitted on production batch with mean=8.0 (standing-biased batch)
Ax_running = 5.0    (5.0 - 8.0) / 4.0 = -0.75   Model: "Negative value? Not running!"
```

The physical sensor value is IDENTICAL (5.0 m/s), but normalized representations are opposite in sign. This causes the model to output wrong predictions because it learned "positive Ax ~0.75 = running" but now sees "negative Ax ~-0.75".

**Scenario 3: Temporal Contamination (Time-Series Leakage)**

HAR data is time-ordered. Fitting scaler on production batches that include "future" data violates causality:

```python
# Batch inference: Process 1 hour of data at once
batch = load_production_data(start="10:00", end="11:00")

# WRONG: Fit scaler on entire batch
scaler.fit(batch)  # Statistics from 10:45 influence normalization at 10:05!
batch_normalized = scaler.transform(batch)

# At timestamp 10:05, the scaler used mean/std computed from data up to 11:00
# This is impossible in real-time deployment!
```

**Why this fails:**
- In real-time HAR (smartwatch app), you cannot access future data when normalizing current window
- Batch processing creates **unrealistic advantage** where past predictions use future statistics
- When deployed as streaming inference, accuracy drops because real-time system cannot compute batch statistics
- **Drift detection becomes meaningless**: If scaler adapts to each batch, you can't detect distribution shifts (you're hiding the shift by refitting!)

**The MLOps Perspective:**

From an MLOps reproducibility standpoint, fitting on production data creates:
- **Non-reproducible predictions**: Same input + same model = different output if scaler changes
- **Impossible rollback**: Cannot reproduce historical predictions because scaler state is lost
- **Failed A/B tests**: Cannot compare model versions if preprocessing changes between runs
- **Broken monitoring**: Drift detection requires stable reference (training distribution)

(docs/PIPELINE_DEEP_DIVE_opus.md:344-360)

---

#### Do/Don't Rules

**DO:**

1. **Fit scaler once on training data, save parameters**  
   ```python
   # Training phase (run once)
   scaler = StandardScaler()
   scaler.fit(training_data)
   config = {
       'scaler_mean': scaler.mean_.tolist(),
       'scaler_scale': scaler.scale_.tolist()
   }
   json.dump(config, open('config.json', 'w'))
   ```
   (src/Archived(prepare traning- production- conversion)/training_cv_experiment/train_with_cv.py:280-290)

2. **Load and transform production data**  
   ```python
   # Production phase (run many times)
   config = json.load(open('config.json'))
   scaler = StandardScaler()
   scaler.mean_ = np.array(config['scaler_mean'])
   scaler.scale_ = np.array(config['scaler_scale'])
   prod_normalized = scaler.transform(production_data)  # transform ONLY
   ```
   (src/preprocess_data.py:507-521)

3. **Use same scaler for drift detection baseline**  
   Apply training scaler to training data to create normalized_baseline.json for KS/PSI tests.  
   (scripts/create_normalized_baseline.py:35-50, models/normalized_baseline.json:7-18)

4. **Version scaler with model**  
   Store scaler in evolution packages: models/evolution_packages/{id}/scaler_config.json alongside model weights.  
   (docs/output_1801_2026-01-18.md:2221-2234, Pair 08)

5. **Validate scaler parameters before inference**  
   Check that loaded scaler has expected shape (6 channels) and reasonable ranges (mean within [-10, 10], std > 0).  
   (scripts/preprocess_qc.py:101-120)

**DON'T:**

1. **Never call fit() on production/test data**  
   ```python
   # FORBIDDEN
   scaler.fit(production_data)  #  Leakage!
   scaler.fit_transform(test_data)  #  Even worse!
   ```

2. **Don't fit separate scalers for train/val/test splits**  
   ```python
   # WRONG: Creates inconsistent input spaces
   scaler_train.fit(X_train)
   scaler_val.fit(X_val)  #  No! Use scaler_train for both
   ```
   Cross-validation must fit scaler on train fold, transform val fold.  
   (src/Archived(prepare traning- production- conversion)/training_cv_experiment/train_with_cv.py:203-208)

3. **Don't refit scaler for each production batch**  
   ```python
   # WRONG: Non-reproducible, hides drift
   for batch in production_batches:
       scaler = StandardScaler()
       scaler.fit(batch)  #  Different scaler each time!
       process(scaler.transform(batch))
   ```

4. **Don't fit scaler on unlabeled data during active learning**  
   Even if collecting labels in production, wait until retraining to refit scaler. Use existing scaler for new labels.

5. **Don't skip scaler versioning**  
   Every model evolution must save scaler. Missing scaler = cannot reproduce predictions = MLOps failure.

---

#### Drift-Aware Decision Tree

**(Covered in Q2 - see below for refit/retrain rules)**

---

#### Repo Actions

**Existing Implementation (Correct):**

1. **Training scaler persistence:**  
   src/Archived(prepare traning- production- conversion)/training_cv_experiment/train_with_cv.py:280-290 saves scaler after training.  
   Stored in data/prepared/config.json with keys: scaler_mean (list[6]), scaler_scale (list[6]).

2. **Production transform-only:**  
   src/preprocess_data.py:507-521 loads scaler and applies transform().  
   ```python
   self.scaler.mean_ = np.array(config['scaler_mean'])
   self.scaler.scale_ = np.array(config['scaler_scale'])
   df_normalized[sensor_cols] = self.scaler.transform(df_normalized[sensor_cols])
   ```

3. **Cross-validation compliance:**  
   src/Archived(prepare traning- production- conversion)/training_cv_experiment/train_with_cv.py:203-208 fits on train fold, transforms val fold.  
   ```python
   scaler.fit(X_train_flat)
   X_train_scaled = scaler.transform(X_train_flat).reshape(X_train.shape)
   X_val_scaled = scaler.transform(X_val_flat).reshape(X_val.shape)  #  No fit!
   ```

4. **Normalized baseline creation:**  
   scripts/create_normalized_baseline.py:35-50 applies training scaler to training data for drift detection baseline.

**Required Additions:**

1. **Scaler validation checks:**  
   Add to scripts/preprocess_qc.py:
   ```python
   def check_scaler_consistency(self, df: pd.DataFrame, sensor_cols: List[str]) -> None:
       # Verify loaded scaler parameters are reasonable
       if self.scaler_config is None:
           self._add_check("Scaler config loaded", False, "CRITICAL", "config.json not found")
           return
       
       mean = np.array(self.scaler_config['scaler_mean'])
       scale = np.array(self.scaler_config['scaler_scale'])
       
       # Check shapes
       if mean.shape != (6,) or scale.shape != (6,):
           self._add_check("Scaler shape", False, "CRITICAL", f"Expected (6,), got mean={mean.shape} scale={scale.shape}")
       
       # Check reasonable ranges (HAR sensors)
       if not np.all((mean > -15) & (mean < 15)):
           self._add_check("Scaler mean range", False, "HIGH", f"Mean outside [-15, 15]: {mean}")
       if not np.all(scale > 0):
           self._add_check("Scaler scale positive", False, "CRITICAL", f"Negative/zero scale: {scale}")
   ```

2. **Document scaler in evolution manifest:**  
   When creating models/evolution_packages/{id}/manifest.json, include scaler metadata:
   ```json
   {
     "scaler": {
       "mean": [3.22, 1.28, -3.53, 0.60, 0.23, 0.09],
       "scale": [6.57, 4.35, 3.24, 49.93, 14.81, 14.17],
       "fit_on": "training_data_20260106.csv",
       "n_samples": 385326
     }
   }
   ```

3. **Scaler drift monitoring (informational only, not for refitting):**  
   Add to scripts/post_inference_monitoring.py Layer 0:
   ```python
   # Compute raw (un-normalized) statistics for production batch
   raw_mean = production_data.mean(axis=0)
   raw_std = production_data.std(axis=0)
   
   # Compare to training scaler
   mean_shift = np.abs(raw_mean - scaler_mean) / scaler_mean
   std_shift = np.abs(raw_std - scaler_scale) / scaler_scale
   
   # Log as informational metric (not a gate)
   mlflow.log_metric("scaler/raw_mean_shift_pct", mean_shift.mean() * 100)
   mlflow.log_metric("scaler/raw_std_shift_pct", std_shift.mean() * 100)
   ```
   This tracks whether raw sensor distributions are changing (which is expected with wrist/user changes), but does NOT refit scaler.

---

#### CITATIONS

1. **(Data Leakage in Preprocessing):** docs/PIPELINE_DEEP_DIVE_opus.md, p.344-360 - Section "4.1 Why StandardScaler Must Fit on TRAINING ONLY" explains train-serve skew and information leakage with concrete examples. States: "If you fit the scaler on production data, you're using information from the future/test set to transform the data."

2. **(Train-Serve Skew Explanation):** docs/thesis/CONCEPTS_EXPLAINED.md, p.279-329 - Section "4 StandardScaler: Why Same Scaler as Training?" provides detailed math examples showing how refitting scaler causes model confusion: "production_value = 5.0    normalized = -0.5 instead of 1.0    Model confused."

3. **(Cross-Validation Scaler Protocol):** src/Archived(prepare traning- production- conversion)/training_cv_experiment/train_with_cv.py, p.203-208 - Implements correct CV protocol: fit on train fold, transform val fold. Comment states: "Standardize (fit on train, transform both)."

4. **(Production Transform-Only Implementation):** src/preprocess_data.py, p.507-521 - Method normalize_data() loads saved scaler and applies transform() without fitting: "mode == 'transform': # Production: use pre-fitted scaler."

5. **(Temporal Leakage in Time-Series):** Research note (implicit in repo design) - HAR data is time-ordered. Batch processing with scaler.fit(batch) creates unrealistic advantage where past predictions use future statistics, impossible in real-time deployment.

6. **(Reproducibility Requirement):** docs/output_1801_2026-01-18.md, p.2221-2234 (Pair 08) - Evolution packages must include scaler_config.json to ensure reproducible predictions: "Any historical model can be exactly reproduced with its complete operational context."

---

### Q2) Should we always use saved scaler for production transforms? If drift detected, what actions are allowed (don't refit vs refit + retrain)?

**Decision:** Always use the saved training scaler for production transforms to maintain input consistency and reproducibility. When drift is detected, the allowed actions depend on drift severity and available resources, following a **four-tier escalation policy**: (1) **No drift (PSI < 0.1):** Continue with existing scaler, log metrics. (2) **Mild drift (PSI 0.1-0.25):** Update monitoring thresholds and baseline, keep scaler unchanged. (3) **Moderate drift (PSI > 0.25 OR >3/6 channels drifted):** Trigger recalibration where scaler can be refit ONLY if paired with model retraining, creating new evolution package. (4) **Severe drift (accuracy drop >10% OR PSI > 0.5):** Mandatory retraining with scaler refit, rollback if performance degrades. **Critical rule:** Never refit scaler without retraining model—this creates train-serve skew.

---

#### Explanation

**The Core Principle: Scaler-Model Coupling**

The scaler and model are **tightly coupled artifacts**. The model's internal weights were learned on inputs normalized by a specific scaler. Changing the scaler without retraining the model breaks this coupling:

```python
# Training (Jan 1)
scaler_v1.fit(training_data)
model_v1.train(scaler_v1.transform(training_data))  # Weights calibrated to scaler_v1's output range

# Production (Feb 1) - WRONG approach
scaler_v2.fit(production_data)  # New scaler!
predictions = model_v1.predict(scaler_v2.transform(production_data))  #  Model sees different input space!
```

**Why scaler-model coupling matters:**

1. **Activation functions depend on absolute values:**  
   ReLU, Sigmoid, Tanh have specific input ranges where they activate. If scaler changes, the same sensor reading produces different activation patterns.

2. **Batch normalization layers compound the issue:**  
   If model has BatchNorm layers, they learned statistics from scaler_v1's output. Feeding scaler_v2's output creates double-normalization mismatch.

3. **Softmax denominator shifts:**  
   Classification logits are calibrated to specific value ranges. Changing scaler shifts these ranges, affecting confidence scores even if predicted class remains correct.

**When Drift is Detected:**

Drift indicates production data distribution differs from training. Three questions:

1. **Should we update scaler?**  Only if retraining model too  
2. **Should we retrain model?**  Depends on drift severity and performance impact  
3. **Should we update baseline?**  Yes, for monitoring continuity

---

#### Do/Don't Rules

**DO:**

1. **Always use saved scaler for transform**  
   ```python
   # Every production batch uses training scaler
   config = json.load(open('config.json'))
   scaler.mean_ = np.array(config['scaler_mean'])
   scaler.scale_ = np.array(config['scaler_scale'])
   production_normalized = scaler.transform(production_data)
   ```
   (src/preprocess_data.py:507-521)

2. **Monitor raw distribution shift (informational only)**  
   ```python
   # Track how much raw sensor statistics diverge from training
   raw_mean_prod = production_data.mean(axis=0)
   raw_std_prod = production_data.std(axis=0)
   
   mean_divergence = np.abs(raw_mean_prod - scaler_mean) / scaler_mean
   std_divergence = np.abs(raw_std_prod - scaler_scale) / scaler_scale
   
   mlflow.log_metrics({
       "raw_stats/mean_shift_pct": mean_divergence.mean() * 100,
       "raw_stats/std_shift_pct": std_divergence.mean() * 100
   })
   # Log but DON'T refit scaler based on this alone
   ```

3. **Refit scaler ONLY when retraining model**  
   ```python
   # Retraining workflow
   if drift_triggers_retrain:  # e.g., PSI > 0.25 on >3 channels
       # Collect new training data (production + labels OR augment existing)
       new_training_data = combine(old_training_data, labeled_production_data)
       
       # Refit scaler on NEW training set
       scaler_v2 = StandardScaler()
       scaler_v2.fit(new_training_data)
       
       # Retrain model with NEW scaler
       model_v2 = train_model(scaler_v2.transform(new_training_data))
       
       # Version both together
       save_evolution_package(
           evolution_id=timestamp,
           model=model_v2,
           scaler=scaler_v2,  #  Coupled with model_v2
           baseline=compute_baseline(scaler_v2.transform(new_training_data))
       )
   ```

4. **Update baseline when recalibrating (no model retrain)**  
   If drift is mild (PSI 0.1-0.25) and accuracy is stable, you can update the drift baseline without retraining:
   ```python
   # Recalibration: No model/scaler change, just update baseline
   if mild_drift_detected:
       # Use EXISTING scaler to normalize recent production data
       recent_prod_normalized = scaler_v1.transform(recent_production_data)
       
       # Compute new baseline from normalized production (becomes new reference)
       new_baseline = compute_statistics(recent_prod_normalized)
       
       # Save as baseline_v2
       save_baseline(new_baseline, version="baseline_20260215")
       
       # Future drift detection compares against baseline_v2 (not original training baseline)
   ```
   This resets the "reference point" for drift without changing model or scaler. (docs/output_1801_2026-01-18.md:2095-2100)

5. **Tag scaler version in MLflow**  
   ```python
   mlflow.set_tag("scaler_version", "scaler_20260106")  # From training
   mlflow.set_tag("baseline_version", "baseline_20260215")  # Updated after recalibration
   ```
   This tracks which scaler/baseline pair produced each inference run. (docs/output_1801_2026-01-18.md:2836-2842)

**DON'T:**

1. **Never refit scaler without retraining model**  
   ```python
   # FORBIDDEN: Creates train-serve skew
   if drift_detected:
       scaler.fit(production_data)  #  NO!
       predictions = model.predict(scaler.transform(production_data))  #  Model confused
   ```

2. **Don't ignore drift signals**  
   If PSI > 0.25 on multiple channels, this indicates significant distribution shift. Continuing with old scaler risks poor performance.

3. **Don't refit scaler on unlabeled production data for retraining**  
   Even when retraining, use labeled data or validated production samples:
   ```python
   # WRONG: Refit on all production (includes outliers, anomalies, errors)
   scaler_new.fit(all_production_data)  #  May include bad data
   
   # CORRECT: Refit on curated training set (may include high-confidence production samples)
   curated_data = filter_high_quality(production_data, confidence_threshold=0.9)
   combined_training = concatenate(original_training, curated_data)
   scaler_new.fit(combined_training)
   ```

4. **Don't create evolution packages with mismatched scaler-model pairs**  
   ```python
   # WRONG: Model v2 with scaler v1
   save_evolution_package(
       model=model_v2,  #  Trained with scaler_v2
       scaler=scaler_v1  #  Mismatch!
   )
   ```
   Evolution package must have scaler that model was trained with.

5. **Don't use production-fitted scaler for baseline creation**  
   Baseline for drift detection must be created with training scaler:
   ```python
   # CORRECT
   training_normalized = scaler_training.transform(training_data)
   baseline = compute_stats(training_normalized)
   
   # WRONG
   scaler_prod.fit(production_data)
   baseline = compute_stats(scaler_prod.transform(production_data))  #  Meaningless comparison
   ```

---

#### Drift-Aware Decision Tree

```
                           Drift Detection Result
                                    |
                 
                 |                  |                  |
           PSI < 0.1          PSI 0.1-0.25      PSI > 0.25 OR >3/6 channels
         (No drift)          (Mild drift)       (Moderate/Severe drift)
                 |                  |                  |
                 v                  v                  v
             
          Action: NONE    Action: UPDATE   Action: EVALUATE
                             BASELINE         RETRAIN NEED 
             
                 |                  |                  |
                 |                  |         
                 v                  v         v                 v
         Keep scaler_v1     Keep scaler_v1   Check accuracy   Check accuracy
         Keep baseline_v1   Create baseline_v2  drop          drop
         Continue inference Update monitoring    |             |
                            thresholds           |             |
                                                 v             v
                                            < 5% drop      5% drop
                                                 |             |
                                                 v             v
                                            
                                          RECALIBR-     RETRAIN     
                                            ATE         REQUIRED    
                                            
                                                 |             |
                                                 v             v
                                          Keep model_v1  Train model_v2
                                          Keep scaler_v1 Refit scaler_v2
                                          Update baseline Compute baseline_v2
                                          Update thresholds Version as evolution
                                          Tag as recalib   Deploy + monitor
```

**Tier 1: No Drift (PSI < 0.1)**
- **Drift Status:** Negligible distribution shift
- **Scaler Action:** Keep using scaler_v1 (from training)
- **Model Action:** No change
- **Baseline Action:** Keep baseline_v1
- **Rationale:** Minor statistical noise, not actionable signal
- **Monitoring:** Log drift metrics for trending
- **Example:** PSI = 0.08 on Az channel (slight gravity calibration variance)

**Tier 2: Mild Drift (PSI 0.1-0.25, <3 channels affected)**
- **Drift Status:** Detectable but manageable shift
- **Scaler Action:** Keep using scaler_v1 (DO NOT refit)
- **Model Action:** No change
- **Baseline Action:** Update to baseline_v2 (normalized with scaler_v1, computed from recent production)
- **Rationale:** Distribution shifted but model still performs adequately. Reset baseline reference to avoid continuous drift alerts.
- **Monitoring:** Update thresholds in monitoring_thresholds.yaml (e.g., relax PSI threshold from 0.25 to 0.30 if drift is directional)
- **Example:** User switched from dominant to non-dominant wrist  Ax/Ay means shifted by 1.5 m/s  PSI = 0.18
- **Implementation:**
  ```python
  # Recalibration workflow (scripts/recalibrate_monitoring.py - to be created)
  recent_prod = load_recent_production_data(days=7)
  
  # Normalize with EXISTING scaler
  recent_normalized = scaler_v1.transform(recent_prod)
  
  # Compute new baseline
  baseline_v2 = {
      'per_channel': {
          ch: {'mean': recent_normalized[:, i].mean(),
               'std': recent_normalized[:, i].std(),
               'samples': recent_normalized[:, i][:10000].tolist()}
          for i, ch in enumerate(['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz'])
      }
  }
  
  # Save and version
  save_json(baseline_v2, 'models/baselines/baseline_20260215.json')
  mlflow.set_tag("baseline_version", "baseline_20260215")
  ```

**Tier 3: Moderate Drift (PSI > 0.25 OR >3/6 channels, accuracy drop <5%)**
- **Drift Status:** Significant shift, model may degrade soon
- **Scaler Action:** **Conditional refit**—only if retraining model
- **Model Action:** Decision point based on labeled data availability
  - **If labels available:** Retrain model + refit scaler
  - **If labels unavailable:** Recalibrate (Tier 2 actions) + collect labels urgently
- **Baseline Action:** Compute baseline_v2 with scaler_v2 (if retrained) or scaler_v1 (if recalibrated)
- **Rationale:** Drift is severe enough to warrant retraining, but if labels aren't ready, buy time with recalibration
- **Example:** PSI = 0.32 on Ax, 0.29 on Ay, 0.21 on Gx (>3 channels)  User population changed (new age group)
- **Retraining Workflow:**
  ```python
  # If labels available (e.g., collected 500 labeled production windows)
  labeled_prod = load_labeled_production_data()
  new_training = concatenate(original_training_data, labeled_prod)
  
  # Refit scaler on NEW training set
  scaler_v2 = StandardScaler()
  scaler_v2.fit(new_training)
  
  # Retrain model
  model_v2 = train_model(scaler_v2.transform(new_training))
  
  # Compute baseline with scaler_v2
  baseline_v2 = compute_baseline(scaler_v2.transform(new_training))
  
  # Version as evolution package
  evolution_id = "20260215_143000"
  save_evolution_package(evolution_id, model_v2, scaler_v2, baseline_v2, ...)
  
  # Deploy
  symlink("models/active/model.keras", f"models/evolution_packages/{evolution_id}/model.keras")
  ```

**Tier 4: Severe Drift (PSI > 0.5 OR accuracy drop 10%)**
- **Drift Status:** Critical failure, model ineffective
- **Scaler Action:** **Mandatory refit** with model retraining
- **Model Action:** Retrain immediately OR rollback to previous evolution
- **Baseline Action:** Recompute baseline_v2
- **Rationale:** Continuing with mismatched model-scaler is worse than downtime
- **Example:** Accuracy dropped from 85% to 68% over 3 days, PSI = 0.62 on Az (sensor hardware change or different device model)
- **Emergency Actions:**
  1. **Immediate:** Rollback to evolution_v1 (if available) to restore service
  2. **Short-term (24h):** Collect labels on drifted production data (200-500 windows)
  3. **Medium-term (3-5 days):** Retrain with augmented training set + refit scaler
  4. **Long-term:** Investigate root cause (sensor calibration, device change, population shift)

**Decision Thresholds Summary:**

| Drift Metric | No Action | Recalibrate | Retrain | Rollback |
|--------------|-----------|-------------|---------|----------|
| PSI (max)    | < 0.1     | 0.1 - 0.25  | > 0.25  | > 0.5    |
| Channels drifted | < 2/6 | 2-3/6       | > 3/6   | > 4/6    |
| Accuracy drop | < 2%     | 2-5%        | 5-10%   | > 10%    |
| Confidence drop | < 5%   | 5-10%       | 10-20%  | > 20%    |

(docs/output_1801_2026-01-18.md:2110-2125, Pair 08 Q1 trigger thresholds)

---

#### Repo Actions

**Existing Implementation:**

1. **Drift detection with training scaler:**  
   scripts/post_inference_monitoring.py:631-792 computes PSI/KS/Wasserstein between production (normalized with training scaler) and baseline (also normalized with training scaler).

2. **Scaler versioning in evolution packages:**  
   docs/output_1801_2026-01-18.md:2221-2234 describes evolution package structure including scaler_config.json.

3. **Baseline creation with training scaler:**  
   scripts/create_normalized_baseline.py:35-50 applies training scaler to training data to create baseline.

**Required Additions:**

1. **Recalibration script (Tier 2 response):**

Create scripts/recalibrate_monitoring.py:
```python
#!/usr/bin/env python3
"""
Recalibration: Update baseline without retraining model.
Used for mild drift (PSI 0.1-0.25) where model still performs well.
"""
import json
import numpy as np
from pathlib import Path
from datetime import datetime

def recalibrate_baseline(production_data_path, scaler_config_path, output_path):
    # Load existing scaler (from training)
    with open(scaler_config_path) as f:
        scaler_cfg = json.load(f)
    mean = np.array(scaler_cfg['scaler_mean'])
    scale = np.array(scaler_cfg['scaler_scale'])
    
    # Load recent production data
    prod_data = pd.read_csv(production_data_path)
    sensor_cols = ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']
    
    # Normalize with EXISTING scaler
    prod_normalized = (prod_data[sensor_cols].values - mean) / scale
    
    # Compute new baseline
    baseline_v2 = {
        'created_at': datetime.now().isoformat(),
        'source': 'Recalibrated from production data (7-day window)',
        'scaler_version': scaler_cfg.get('version', 'scaler_20260106'),
        'per_channel': {}
    }
    
    for i, ch in enumerate(sensor_cols):
        ch_data = prod_normalized[:, i]
        baseline_v2['per_channel'][ch] = {
            'mean': float(ch_data.mean()),
            'std': float(ch_data.std()),
            'samples': ch_data[:10000].tolist()  # For KS tests
        }
    
    # Save
    with open(output_path, 'w') as f:
        json.dump(baseline_v2, f, indent=2)
    
    print(f" Recalibrated baseline saved to {output_path}")
    print(f"  Use this for future drift detection while keeping model/scaler unchanged")

if __name__ == '__main__':
    recalibrate_baseline(
        production_data_path='data/processed/recent_7days.csv',
        scaler_config_path='data/prepared/config.json',
        output_path=f'models/baselines/baseline_{datetime.now():%Y%m%d}.json'
    )
```

2. **Retraining workflow with scaler refit (Tier 3/4 response):**

Add to scripts/retrain_with_drift_data.py (new file):
```python
#!/usr/bin/env python3
"""
Retraining workflow: Retrain model + refit scaler together.
Used for moderate/severe drift (PSI > 0.25 or accuracy drop > 5%).
"""
from sklearn.preprocessing import StandardScaler
import numpy as np
import json

def retrain_with_scaler_update(original_training_path, labeled_production_path, output_dir):
    # Load data
    orig_train = pd.read_csv(original_training_path)
    labeled_prod = pd.read_csv(labeled_production_path)
    
    # Combine training sets
    combined = pd.concat([orig_train, labeled_prod], ignore_index=True)
    
    # Refit scaler on COMBINED training data
    scaler_v2 = StandardScaler()
    sensor_cols = ['Ax_w', 'Ay_w', 'Az_w', 'Gx_w', 'Gy_w', 'Gz_w']
    scaler_v2.fit(combined[sensor_cols])
    
    # Save scaler_v2
    scaler_config_v2 = {
        'scaler_mean': scaler_v2.mean_.tolist(),
        'scaler_scale': scaler_v2.scale_.tolist(),
        'version': f'scaler_{datetime.now():%Y%m%d_%H%M%S}',
        'fit_on': f'combined_training_n={len(combined)}'
    }
    with open(output_dir / 'scaler_config.json', 'w') as f:
        json.dump(scaler_config_v2, f, indent=2)
    
    # Normalize with scaler_v2
    combined_normalized = scaler_v2.transform(combined[sensor_cols])
    
    # Retrain model with combined_normalized
    # (Call existing training script here)
    # model_v2 = train_model(combined_normalized, combined['activity'])
    
    # Create baseline_v2 with scaler_v2
    baseline_v2 = compute_baseline(scaler_v2.transform(combined[sensor_cols]))
    
    # Save evolution package
    evolution_id = datetime.now().strftime('%Y%m%d_%H%M%S')
    evo_path = Path(f'models/evolution_packages/{evolution_id}')
    evo_path.mkdir(parents=True, exist_ok=True)
    # Save model, scaler, baseline, manifest...
    
    print(f" Retrained model_v2 with scaler_v2 in {evo_path}")
    print(f"  Scaler fitted on {len(combined)} samples (original + production)")
```

3. **Update drift monitoring to use active baseline version:**

Modify scripts/post_inference_monitoring.py:631-670 to load baseline based on active version:
```python
# Current: Hard-coded baseline path
baseline_path = PROJECT_ROOT / 'models' / 'normalized_baseline.json'

# Updated: Use versioned baseline
active_baseline_version = mlflow.get_tag("baseline_version") or "baseline_20260106"
baseline_path = PROJECT_ROOT / 'models' / 'baselines' / f'{active_baseline_version}.json'

with open(baseline_path) as f:
    baseline = json.load(f)
# ... rest of drift detection
```

4. **Add scaler consistency check to preprocess_qc.py:**

(Already described in Q1 Repo Actions, item 1)

---

#### CITATIONS

1. **(Scaler-Model Coupling Principle):** docs/thesis/CONCEPTS_EXPLAINED.md, p.279-329 - Explains why changing scaler without retraining breaks learned representations: "Model's internal weights are calibrated to specific input ranges learned during training."

2. **(Drift-Triggered Evolution Taxonomy):** docs/output_1801_2026-01-18.md, p.2068-2125 (Pair 08 Q1) - Defines three evolution types (retraining, recalibration, architecture update) with quantitative triggers. PSI thresholds: 0.1-0.25 = recalibration, >0.25 = retraining.

3. **(Recalibration Process):** docs/output_1801_2026-01-18.md, p.2095-2100 (Pair 08 Q1) - Describes recalibration as updating "scaler parameters, confidence thresholds, drift baselines, monitoring thresholds" when drift is mild. States: "Recompute scaler on recent production data (if labels unavailable) or validation set (if available), rebuild drift baselines."

4. **(Evolution Package Scaler Requirement):** docs/output_1801_2026-01-18.md, p.2221-2234 (Pair 08 Q2) - Evolution packages must include scaler_config.json with mean/scale arrays: "All artifacts share a single evolution_id and are stored in three synchronized locations."

5. **(Drift Detection Implementation):** scripts/post_inference_monitoring.py, p.631-792 - Implements 3-test drift detection (KS, Wasserstein, PSI) comparing production vs baseline. Both are normalized with training scaler: "normalized_prod = (prod_data - baseline['preprocessing']['scaler_mean']) / baseline['preprocessing']['scaler_scale']."

6. **(Normalized Baseline Creation):** scripts/create_normalized_baseline.py, p.35-50 - Applies training scaler to training data to create drift detection baseline: "normalized_data = (raw_data - np.array(scaler['mean'])) / np.array(scaler['scale'])."

---

**Summary Table:**

| Scenario | Drift Level | Scaler Action | Model Action | Baseline Action | Rationale |
|----------|-------------|---------------|--------------|-----------------|-----------|
| **No Drift** | PSI < 0.1 | Keep training scaler | No change | Keep baseline_v1 | Statistical noise, not actionable |
| **Mild Drift** | PSI 0.1-0.25 | Keep training scaler | No change | Update to baseline_v2 (normalized with training scaler) | Reset reference, avoid alert fatigue |
| **Moderate Drift** | PSI > 0.25 OR >3 channels | Refit scaler_v2 IF retraining | Retrain model_v2 with scaler_v2 | Compute baseline_v2 with scaler_v2 | Significant shift requires model update |
| **Severe Drift** | PSI > 0.5 OR accuracy drop >10% | Mandatory refit scaler_v2 | Retrain model_v2 OR rollback | Recompute baseline_v2 | Critical failure, emergency action needed |
| **Recalibration Only** | Accuracy stable, drift moderate | Never refit (leakage risk) | No change (keep model_v1) | Update baseline_v2 with scaler_v1 | Buys time until labels collected |
| **Retraining** | Accuracy degraded, drift severe | Always refit (coupled with model) | Train model_v2 | Compute baseline_v2 with scaler_v2 | Scaler-model must evolve together |
| **Data Leakage Prevention** | Any scenario | Never fit on unlabeled production | Never use model trained with different scaler | Never mix scaler versions in one experiment | Maintains train-serve consistency |
| **Rollback** | Retraining failed | Restore scaler_v1 from evolution package | Restore model_v1 | Restore baseline_v1 | Atomic versioning enables safe rollback |

---

## Pair 11 — Scaler versioning after fine-tuning

**Context:** Fine-tuning changes model weights on new production data (e.g., Garmin device data vs. original ADAM-sense training data). When fine-tuning, the model is trained on data that may have different distributions, potentially requiring scaler refit. Current repo uses atomic evolution packages (models/evolution_packages/{evolution_id}/) containing 7 artifacts (Pair 08), but lacks explicit guidance on fine-tuning workflows where scaler+model must evolve together.

### Q1) How to manage scaler updates safely (model+scaler as one versioned unit)? Best practice?

**Decision:** Manage scaler updates through **coupled atomic versioning** where fine-tuning always creates a new evolution package with both model_v2 AND scaler_v2 stored together, never allowing model and scaler versions to mismatch. Follow a **four-phase fine-tuning workflow**: (1) **Pre-training checkpoint**: Save current evolution package as rollback point, (2) **Training phase**: Refit scaler on combined dataset (old training + new labeled production), train model with new scaler, (3) **Validation gate**: Compare performance on holdout set; only promote if accuracy improves 5% AND absolute accuracy >65%, (4) **Atomic deployment**: Create new evolution package with manifest linking model_v2 to scaler_v2, update active symlinks atomically. If validation fails, rollback to pre-training checkpoint.

---

#### Best Practice: Coupled Atomic Versioning

**Core Principle: Model-Scaler Inseparability**

The scaler and model are **training artifacts** with a fixed coupling established during training:

```python
# Training phase (creates coupling)
scaler_v2 = StandardScaler()
scaler_v2.fit(training_data)  #  Scaler learns distribution

model_v2 = create_model()
model_v2.fit(scaler_v2.transform(training_data))  #  Model learns from scaler_v2's output space

#  model_v2 is calibrated to scaler_v2's output range
#  Using model_v2 with scaler_v1 = wrong input distribution = degraded accuracy
```

**Why coupling matters:**
1. **Numerical calibration**: Model's activation functions, batch normalization layers, and weight magnitudes are calibrated to specific input ranges produced by the scaler
2. **Softmax temperature**: Classification confidence scores depend on logit magnitudes, which shift if scaler changes
3. **Embedding spaces**: Internal representations (BiLSTM outputs) encode patterns relative to normalized input scale

**Three forbidden operations:**
```python
# FORBIDDEN 1: Deploy model_v2 with scaler_v1
scaler_v1 = load_scaler("evolution_20260106/scaler_config.json")
model_v2 = load_model("evolution_20260118/model.keras")  #  Mismatched!
production_normalized = scaler_v1.transform(production_data)
predictions = model_v2.predict(production_normalized)  #  Degraded accuracy

# FORBIDDEN 2: Update scaler without retraining model
scaler_v2.fit(new_production_data)  #  New scaler
save_scaler(scaler_v2, "evolution_20260106/scaler_config.json")  #  Overwrite old scaler!
model_v1 = load_model("evolution_20260106/model.keras")  #  Old model with new scaler
#  Model confused by different input distribution

# FORBIDDEN 3: Deploy incomplete evolution package
evolution_packages/evolution_20260118/
  model.keras        #  New model present
  scaler_config.json #  Missing!  Which scaler to use?
  manifest.json      #  Manifest points to scaler, but file missing
#  Inference pipeline cannot load - missing dependency
```

**The safe approach: Atomic evolution package**
```
evolution_packages/evolution_20260118_fine_tuned/
  manifest.json                 # Links model <-> scaler <-> all artifacts
  model.keras                   # model_v2 (fine-tuned weights)
  scaler_config.json            # scaler_v2 (refit on combined data)
  monitoring_thresholds.yaml    # Updated thresholds for new distribution
  drift_baseline.json           # New baseline from fine-tuning data
  training_config.yaml          # Documents fine-tuning hyperparams
  evaluation_report.json        # Validation metrics on production domain
```

All 7 artifacts share evolution_id "evolution_20260118_fine_tuned", ensuring they are versioned together.

---

#### Four-Phase Fine-Tuning Workflow

**Phase 1: Pre-Training Checkpoint (Rollback Safety)**

Before starting fine-tuning, create explicit checkpoint of current production system:

```bash
# Freeze current production state
cp -r models/active models/checkpoints/pre_finetuning_20260118

# Document pre-training metrics
cat > models/checkpoints/pre_finetuning_20260118/baseline_metrics.json <<EOF
{
  "checkpoint_timestamp": "2026-01-18T09:00:00",
  "production_accuracy": 0.145,
  "mean_confidence": 0.38,
  "evolution_id": "evolution_20260106_091520",
  "reason": "Pre-fine-tuning checkpoint before Garmin adaptation"
}
EOF

# Git tag for safety
git tag -a checkpoint_pre_finetune_20260118 -m "Pre fine-tuning checkpoint"
```

**Why needed:**
- Fine-tuning can cause **catastrophic forgetting** (model forgets original tasks)
- Validation may fail (new model worse than old model)
- Need clean rollback path without manual reconstruction

**Phase 2: Training with Scaler Refit**

Fine-tuning workflow that maintains coupling:

```python
# Step 2.1: Prepare combined dataset
original_training = load_csv("data/raw/all_users_data_labeled.csv")  # Original domain (ADAM-sense)
production_labeled = load_csv("data/prepared/production_labeled_500_windows.csv")  # New domain (Garmin)

# Mixed strategy to prevent catastrophic forgetting
# 30% old + 70% new (configurable ratio)
combined_training = combine_stratified(
    old_data=original_training.sample(frac=0.3),
    new_data=production_labeled,
    balance_classes=True
)

print(f"Combined training: {len(combined_training)} samples")
print(f"  Old domain: {len(original_training)*0.3:.0f} samples")
print(f"  New domain: {len(production_labeled)} samples")

# Step 2.2: Refit scaler on COMBINED dataset
scaler_v2 = StandardScaler()
sensor_cols = ['Ax_w', 'Ay_w', 'Az_w', 'Gx_w', 'Gy_w', 'Gz_w']
scaler_v2.fit(combined_training[sensor_cols])

print(f"Scaler v2 fitted:")
print(f"  Mean: {scaler_v2.mean_}")
print(f"  Scale: {scaler_v2.scale_}")

# Step 2.3: Normalize with scaler_v2
combined_normalized = scaler_v2.transform(combined_training[sensor_cols])

# Step 2.4: Fine-tune model
# Option A: Unfreeze all layers (full retraining)
model_v2 = load_model("models/active/model.keras")  # Start from current model
model_v2.compile(
    optimizer=Adam(learning_rate=1e-4),  # Lower LR for fine-tuning
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Option B: Freeze early layers (transfer learning)
for layer in model_v2.layers[:-5]:  # Freeze CNN layers, unfreeze BiLSTM+Dense
    layer.trainable = False

history = model_v2.fit(
    combined_normalized, combined_training['activity_encoded'],
    validation_split=0.15,
    epochs=20,
    batch_size=32,
    callbacks=[
        EarlyStopping(patience=5, restore_best_weights=True),
        ReduceLROnPlateau(factor=0.5, patience=3)
    ]
)

# Step 2.5: Save scaler_v2 config
scaler_config_v2 = {
    "scaler_mean": scaler_v2.mean_.tolist(),
    "scaler_scale": scaler_v2.scale_.tolist(),
    "version": "scaler_20260118_finetuned",
    "fit_on": "combined_training_n=1500",
    "old_domain_ratio": 0.3,
    "new_domain_ratio": 0.7
}
```

**Critical rule:** Scaler_v2 is fit on the SAME combined dataset that model_v2 trains on. This maintains coupling.

**Phase 3: Validation Gate (Gated Promotion)**

Only promote fine-tuned model if it passes strict criteria:

```python
# Validation on PRODUCTION domain holdout set
production_val = load_csv("data/prepared/production_val_200_windows.csv")  # Held-out Garmin data
production_val_normalized = scaler_v2.transform(production_val[sensor_cols])

# Evaluate model_v2
val_loss_v2, val_acc_v2 = model_v2.evaluate(
    production_val_normalized, 
    production_val['activity_encoded']
)

# Load baseline metrics
with open("models/checkpoints/pre_finetuning_20260118/baseline_metrics.json") as f:
    baseline = json.load(f)

baseline_acc = baseline["production_accuracy"]  # 0.145 (14.5%)
improvement = val_acc_v2 - baseline_acc

print(f"Validation Results:")
print(f"  Baseline (model_v1 + scaler_v1): {baseline_acc*100:.1f}%")
print(f"  Fine-tuned (model_v2 + scaler_v2): {val_acc_v2*100:.1f}%")
print(f"  Improvement: {improvement*100:+.1f}%")

# Gating criteria (from ICTH_16 paper expectations)
GATE_MIN_IMPROVEMENT = 0.05  # Must improve by at least 5%
GATE_MIN_ABSOLUTE = 0.65      # Must reach at least 65% accuracy

if improvement >= GATE_MIN_IMPROVEMENT and val_acc_v2 >= GATE_MIN_ABSOLUTE:
    print("GATE PASSED: Promoting fine-tuned model to production")
    promote_to_production = True
else:
    print("GATE FAILED: Fine-tuned model did not meet criteria")
    print(f"  Required: improvement {GATE_MIN_IMPROVEMENT*100:.0f}% AND accuracy {GATE_MIN_ABSOLUTE*100:.0f}%")
    print(f"  Actual: improvement={improvement*100:.1f}% AND accuracy={val_acc_v2*100:.1f}%")
    print("  Action: Rolling back to pre-training checkpoint")
    promote_to_production = False
    
    # Rollback
    os.system("cp -r models/checkpoints/pre_finetuning_20260118/* models/active/")
```

**Why gating is critical:**
- Fine-tuning can make model WORSE (catastrophic forgetting)
- Small validation sets can give false positives (need significance threshold)
- Production deployment without validation = high risk of service degradation

**Phase 4: Atomic Deployment**

If gate passes, create new evolution package and deploy atomically:

```python
# Create evolution package directory
evolution_id = "evolution_20260118_finetuned"
evo_path = Path(f"models/evolution_packages/{evolution_id}")
evo_path.mkdir(parents=True, exist_ok=True)

# Save all 7 artifacts
# (1) Model
model_v2.save(evo_path / "model.keras")

# (2) Scaler
with open(evo_path / "scaler_config.json", "w") as f:
    json.dump(scaler_config_v2, f, indent=2)

# (3) Monitoring thresholds (may need tuning for new distribution)
shutil.copy("models/active/monitoring_thresholds.yaml", evo_path / "monitoring_thresholds.yaml")

# (4) Drift baseline (recompute from fine-tuning training data)
baseline_v2 = build_baseline(scaler_v2.transform(combined_training[sensor_cols]))
with open(evo_path / "drift_baseline.json", "w") as f:
    json.dump(baseline_v2, f, indent=2)

# (5) Training config
training_config_v2 = {
    "evolution_type": "fine_tuning",
    "parent_evolution_id": "evolution_20260106_091520",
    "model_architecture": "1D-CNN-BiLSTM",
    "fine_tuning_config": {
        "learning_rate": 1e-4,
        "frozen_layers": 10,
        "epochs": 20,
        "old_domain_samples": len(original_training)*0.3,
        "new_domain_samples": len(production_labeled)
    }
}
with open(evo_path / "training_config.yaml", "w") as f:
    yaml.dump(training_config_v2, f)

# (6) Evaluation report
evaluation_report_v2 = {
    "timestamp": datetime.now().isoformat(),
    "validation_accuracy": float(val_acc_v2),
    "baseline_accuracy": float(baseline_acc),
    "improvement": float(improvement),
    "confusion_matrix": confusion_matrix(y_true, y_pred).tolist(),
    "per_class_f1": classification_report(y_true, y_pred, output_dict=True)
}
with open(evo_path / "evaluation_report.json", "w") as f:
    json.dump(evaluation_report_v2, f, indent=2)

# (7) Manifest (links everything together)
manifest_v2 = {
    "evolution_id": evolution_id,
    "evolution_type": "fine_tuning",
    "timestamp": datetime.now().isoformat(),
    "parent_evolution_id": "evolution_20260106_091520",
    "git_commit": subprocess.check_output(["git", "rev-parse", "HEAD"]).decode().strip(),
    "artifacts": {
        "model": {"file": "model.keras", "version": "v2_finetuned", "size_mb": 5.2},
        "scaler": {"file": "scaler_config.json", "version": "scaler_20260118_finetuned"},
        "baseline": {"file": "drift_baseline.json", "version": "baseline_20260118"},
        "thresholds": {"file": "monitoring_thresholds.yaml", "version": "v1"}
    },
    "validation_metrics": {
        "accuracy": float(val_acc_v2),
        "improvement_over_parent": float(improvement),
        "gating_decision": "PASS"
    }
}
with open(evo_path / "manifest.json", "w") as f:
    json.dump(manifest_v2, f, indent=2)

# Atomic deployment: Update symlinks
# OLD: models/active/ -> evolution_20260106_091520/
# NEW: models/active/ -> evolution_20260118_finetuned/
os.unlink("models/active")  # Remove old symlink
os.symlink(f"evolution_packages/{evolution_id}", "models/active")  # Atomic update

print(f"Deployed: models/active -> {evolution_id}")
print(f"  Model: {evolution_id}/model.keras")
print(f"  Scaler: {evolution_id}/scaler_config.json")
print(f"  All 7 artifacts co-versioned")

# Log to MLflow
with mlflow.start_run(run_name=evolution_id):
    mlflow.log_params(training_config_v2["fine_tuning_config"])
    mlflow.log_metrics({
        "val_accuracy": val_acc_v2,
        "baseline_accuracy": baseline_acc,
        "improvement": improvement
    })
    mlflow.log_artifact(str(evo_path / "model.keras"))
    mlflow.log_artifact(str(evo_path / "scaler_config.json"))
    mlflow.log_artifact(str(evo_path / "manifest.json"))
    mlflow.set_tag("evolution_id", evolution_id)
    mlflow.set_tag("evolution_type", "fine_tuning")
    mlflow.set_tag("parent_evolution_id", "evolution_20260106_091520")
    mlflow.set_tag("scaler_version", "scaler_20260118_finetuned")

# Version with DVC
os.system(f"cd models/evolution_packages && dvc add {evolution_id}/ && dvc push")

# Commit to Git
os.system(f"git add models/evolution_packages/{evolution_id}/*.{{json,yaml}}")
os.system(f"git add models/evolution_packages.dvc")
os.system(f'git commit -m "Evolution: fine-tuning on Garmin data"')
os.system(f'git tag -a {evolution_id} -m "Fine-tuned model + scaler_v2"')
os.system("git push origin main --tags")

print(f"Evolution {evolution_id} fully versioned (DVC + Git + MLflow)")
```

**Atomic deployment properties:**
- **Single operation**: Symlink update is atomic (old -> new in one syscall)
- **No partial state**: Either all artifacts from v1 OR all artifacts from v2, never mixed
- **Instant rollback**: ln -sf evolution_20260106_091520 models/active restores previous version
- **Reproducible**: Any historical evolution can be reactivated with exact artifacts

---

#### Best Practices Summary

**DO:**
1. **Always refit scaler when fine-tuning**: Use combined dataset (old + new) to prevent catastrophic forgetting
2. **Version scaler+model together**: Create new evolution package with both artifacts sharing evolution_id
3. **Use validation gates**: Only promote if accuracy improves 5% AND absolute accuracy 65%
4. **Create pre-training checkpoints**: Save current system state before fine-tuning starts
5. **Document parent lineage**: Manifest must include parent_evolution_id to track model genealogy
6. **Log to three systems**: DVC (binaries), Git (configs), MLflow (metadata) for redundancy

**DON'T:**
1. **Never update scaler in-place**: Always create new evolution package, don't overwrite old scaler
2. **Never deploy model without scaler**: All 7 artifacts must be present in evolution package
3. **Never skip validation**: Fine-tuning can degrade performance, always validate before deploy
4. **Never fine-tune on unlabeled data**: Scaler refit requires ground truth for combined dataset
5. **Never delete old evolution packages**: Keep full history for rollback and auditing
6. **Never use different scaler versions in same experiment**: MLflow run must use consistent scaler throughout

---

#### Repo Actions

**Existing Foundation (from Pair 08):**
1. Evolution package structure defined (docs/output_1801_2026-01-18.md:2216-2350)
2. DVC+Git+MLflow three-location synchronization (docs/output_1801_2026-01-18.md:2358-2390)
3. Manifest schema with provenance metadata (docs/output_1801_2026-01-18.md:2331-2350)

**Required Additions:**

1. **Create scripts/fine_tune_with_versioning.py**

```python
#!/usr/bin/env python3
\"\"\"
Fine-tuning workflow with coupled scaler+model versioning.
Implements 4-phase workflow: checkpoint, train, validate, deploy.
\"\"\"
import argparse
from pathlib import Path
import json
import shutil
from datetime import datetime

def create_checkpoint(active_path: Path, checkpoint_name: str):
    \"\"\"Phase 1: Create rollback checkpoint.\"\"\"
    checkpoint_path = Path(f"models/checkpoints/{checkpoint_name}")
    shutil.copytree(active_path, checkpoint_path)
    
    # Log baseline metrics
    # (Read from last inference run or evaluation report)
    baseline_metrics = {
        "checkpoint_timestamp": datetime.now().isoformat(),
        "evolution_id": (active_path / "manifest.json").read_json()["evolution_id"],
        "production_accuracy": 0.145,  # Load from reports/
        "reason": "Pre-fine-tuning safety checkpoint"
    }
    (checkpoint_path / "baseline_metrics.json").write_text(json.dumps(baseline_metrics, indent=2))
    
    print(f"Checkpoint created: {checkpoint_path}")
    return checkpoint_path

def train_with_scaler_refit(
    old_training_path: Path,
    new_training_path: Path,
    old_ratio: float = 0.3
):
    \"\"\"Phase 2: Train model_v2 with scaler_v2 on combined data.\"\"\"
    # Load datasets
    old_df = pd.read_csv(old_training_path)
    new_df = pd.read_csv(new_training_path)
    
    # Combine with stratification
    combined = combine_stratified(old_df.sample(frac=old_ratio), new_df)
    
    # Refit scaler
    scaler_v2 = StandardScaler()
    scaler_v2.fit(combined[sensor_cols])
    
    # Normalize
    combined_normalized = scaler_v2.transform(combined[sensor_cols])
    
    # Fine-tune model
    model_v2 = load_model("models/active/model.keras")
    # (Configure layers, compile, fit...)
    
    return model_v2, scaler_v2, combined

def validate_and_gate(model_v2, scaler_v2, val_data, baseline_metrics):
    \"\"\"Phase 3: Validate and apply gating criteria.\"\"\"
    val_normalized = scaler_v2.transform(val_data[sensor_cols])
    val_acc_v2 = model_v2.evaluate(val_normalized, val_data['y'])[1]
    
    baseline_acc = baseline_metrics["production_accuracy"]
    improvement = val_acc_v2 - baseline_acc
    
    passed = (improvement >= 0.05) and (val_acc_v2 >= 0.65)
    
    return {
        "passed": passed,
        "val_acc_v2": val_acc_v2,
        "baseline_acc": baseline_acc,
        "improvement": improvement
    }

def deploy_evolution_package(model_v2, scaler_v2, validation_result, evolution_id):
    \"\"\"Phase 4: Create evolution package and deploy atomically.\"\"\"
    evo_path = Path(f"models/evolution_packages/{evolution_id}")
    evo_path.mkdir(parents=True, exist_ok=True)
    
    # Save all 7 artifacts (model, scaler, thresholds, baseline, config, report, manifest)
    # (Implementation details as shown in Phase 4 above)
    
    # Atomic symlink update
    Path("models/active").unlink()
    Path("models/active").symlink_to(f"evolution_packages/{evolution_id}")
    
    print(f"Deployed: models/active -> {evolution_id}")

if __name__ == "__main__":
    # Parse args, run 4-phase workflow
    pass
```

2. **Update src/run_inference.py to load scaler from active package**

Currently hardcoded to data/prepared/config.json. Update to:
```python
# OLD: scaler_path = DATA_PREPARED / "config.json"
# NEW: scaler_path = Path("models/active/scaler_config.json")

with open(scaler_path) as f:
    scaler_config = json.load(f)

scaler = StandardScaler()
scaler.mean_ = np.array(scaler_config['scaler_mean'])
scaler.scale_ = np.array(scaler_config['scaler_scale'])

# Log scaler version to inference metadata
metadata['scaler_version'] = scaler_config.get('version', 'unknown')
metadata['evolution_id'] = Path("models/active").resolve().name  # Get evolution ID from symlink
```

3. **Add validation to preprocess_qc.py**

Check model-scaler version consistency:
```python
def check_model_scaler_coupling(self, model_path: Path, scaler_path: Path) -> None:
    \"\"\"Verify model and scaler are from same evolution package.\"\"\"
    # Extract evolution IDs from paths
    model_evo_id = model_path.parent.name  # e.g., "evolution_20260118_finetuned"
    scaler_evo_id = scaler_path.parent.name
    
    if model_evo_id != scaler_evo_id:
        self._add_check(
            name="Model-scaler coupling",
            passed=False,
            severity="CRITICAL",
            message=f"Mismatched evolution IDs: model={model_evo_id}, scaler={scaler_evo_id}",
            details={"model": str(model_path), "scaler": str(scaler_path)}
        )
    else:
        self._add_check(
            name="Model-scaler coupling",
            passed=True,
            severity="INFO",
            message=f"Model and scaler from same evolution: {model_evo_id}"
        )
```

4. **Document fine-tuning runbook in docs/fine_tuning_runbook.md**

Step-by-step guide:
- When to fine-tune (accuracy <50% on production, drift detected, new device type)
- How to collect labeled production data (200-500 windows minimum)
- Fine-tuning command: python scripts/fine_tune_with_versioning.py --old-data data/raw/all_users_data_labeled.csv --new-data data/prepared/production_labeled_500_windows.csv --old-ratio 0.3
- Validation criteria interpretation
- Rollback procedure if fine-tuning fails

---

#### CITATIONS

1. **(Atomic Model-Preprocessing Versioning):** papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.120-135 - Best practices for versioning operational dependencies: "All artifacts that affect model behavior (weights, preprocessing, thresholds) must be versioned together as atomic unit." Recommends manifest file linking artifacts with provenance metadata (training data hash, code commit, hyperparameters).

2. **(Scaler-Model Coupling Failure Example):** papers/mlops_production/Essential_MLOps_Data_Science_Horizons_2023_Data_Science_Horizons_Final_2023.pdf, p.67-73 - Reproducibility case study where model accuracy dropped 12% due to mismatched scaler version: "Old model + new scaler created train-serve skew... predictions no longer calibrated correctly."

3. **(Evolution Package Structure):** docs/output_1801_2026-01-18.md, p.2216-2350 (Pair 08 Q2) - Defines 7-artifact evolution package with manifest.json linking model, scaler, thresholds, baseline, config, report. All artifacts share single evolution_id.

4. **(Fine-Tuning Strategy):** docs/thesis/FINE_TUNING_STRATEGY.md, p.115-212 - Fine-tuning policy: ONE-TIME, SUPERVISED, GATED. Mixed data strategy (30% old + 70% new) to prevent catastrophic forgetting. Gating criteria: improvement 5% AND absolute accuracy 65%.

5. **(DVC+Git+MLflow Synchronization):** docs/output_1801_2026-01-18.md, p.2358-2390 (Pair 08 Q2) - Three-location storage for redundancy and access patterns: DVC for large binaries (model), Git for configs (scaler, thresholds), MLflow for metadata (metrics, tags).

---

### Q2) Propose exact file layout + naming for scaler versions and linking to model versions + MLflow runs.

**Decision:** Implement **evolution-centric file layout** where each evolution package is a self-contained directory with timestamp-based evolution_id as primary key, scaler files use semantic versioning (scaler_v{major}.{minor}_{context}), and MLflow runs link to evolution packages via tags. File structure: models/evolution_packages/{evolution_id}/ contains all artifacts with standardized names (scaler_config.json, model.keras), models/active/ symlink points to current production evolution, and models/checkpoints/ stores pre-deployment snapshots. Each scaler version is documented in manifest.json with parent lineage, and MLflow runs use tags (evolution_id, scaler_version, model_version) to create bidirectional linking between filesystem and experiment tracker.

---

#### Version Layout: Evolution-Centric Structure

**Core Design Principle: Evolution ID as Primary Key**

All artifacts within an evolution package share a single timestamp-based evolution_id:

```
evolution_id = "evolution_YYYYMMDD_HHMMSS"
```

**Example:**
- Initial training: evolution_20260106_091520
- Drift-triggered retrain: evolution_20260118_142530
- Fine-tuning on Garmin: evolution_20260125_163045_garmin

**Directory Structure:**

```
models/
 evolution_packages/                  # All versioned evolution packages
    evolution_20260106_091520/       # Initial training (v1)
       manifest.json                # Primary metadata (links all artifacts)
       model.keras                  # Model weights (5 MB)
       scaler_config.json           # Scaler params (scaler_v1.0_initial)
       monitoring_thresholds.yaml   # Confidence/drift/gating thresholds
       drift_baseline.json          # Training distribution statistics
       training_config.yaml         # Model architecture + hyperparameters
       evaluation_report.json       # Validation metrics + confusion matrix
   
    evolution_20260118_142530/       # Drift-triggered retraining (v2)
       manifest.json
       model.keras
       scaler_config.json           # scaler_v2.0_driftretrain (refit on augmented data)
       monitoring_thresholds.yaml
       drift_baseline.json
       training_config.yaml
       evaluation_report.json
   
    evolution_20260125_163045_garmin/  # Fine-tuning (v3)
       manifest.json
       model.keras
       scaler_config.json           # scaler_v3.0_garmin (refit on ADAM+Garmin)
       monitoring_thresholds.yaml   # Updated for Garmin distribution
       drift_baseline.json          # Garmin training baseline
       training_config.yaml         # Fine-tuning config (frozen layers, LR, etc.)
       evaluation_report.json
   
    evolution_20260201_103000_recalibration/  # Recalibration (no model retrain)
        manifest.json
        model.keras                  # Symlink to parent evolution's model (no retrain)
        scaler_config.json           # SAME as parent (no refit)
        monitoring_thresholds.yaml   # Updated thresholds only
        drift_baseline.json          # Updated baseline only
        training_config.yaml         # Copy from parent
        evaluation_report.json       # Re-evaluation report

 active/                              # Symlink to current production evolution
   -> evolution_packages/evolution_20260125_163045_garmin/

 checkpoints/                         # Pre-deployment snapshots for rollback
    pre_finetuning_20260125/         # Checkpoint before fine-tuning
       baseline_metrics.json        # Production metrics before change
       (copy of active evolution at checkpoint time)
   
    pre_architecture_update_20260201/
        ...

 evolution_packages.dvc               # DVC tracking file for all evolution packages
```

**Key Design Decisions:**

1. **Flat namespace per evolution**: All artifacts in same directory, no subdirectories
   - Simplifies loading: load_model("models/active/model.keras")
   - No path confusion: scaler is always {evolution_id}/scaler_config.json

2. **Standardized filenames**: All evolutions use same artifact names
   - model.keras (not model_v2.keras or ine_tuned_model.keras)
   - scaler_config.json (not scaler_v2.json)
   - Version info is in manifest.json, not filename

3. **Symlink for active deployment**: models/active/ always points to current production
   - Inference code loads from models/active/model.keras (stable path)
   - Atomic deployment: ln -sf evolution_20260125_163045_garmin models/active
   - Instant rollback: ln -sf evolution_20260118_142530 models/active

4. **Checkpoints for safety**: Pre-deployment snapshots enable clean rollback
   - Before fine-tuning, copy ctive/ to checkpoints/pre_finetuning_YYYYMMDD/
   - If fine-tuning fails, restore from checkpoint

---

#### Scaler Naming and Versioning

**Scaler Version Schema:**

Within scaler_config.json, use semantic versioning with context:

```json
{
  "version": "scaler_v{major}.{minor}_{context}",
  "scaler_mean": [3.22, 1.28, -3.53, 0.60, 0.23, 0.09],
  "scaler_scale": [6.57, 4.35, 3.24, 49.93, 14.81, 14.17],
  "fit_on": "training_data_description",
  "n_samples": 385326,
  "timestamp": "2026-01-25T16:30:45",
  "parent_scaler_version": "scaler_v2.0_driftretrain"
}
```

**Version Number Rules:**

- **Major version (v{major}.X):** Increments when scaler is REFIT (new mean/std)
  - v1.0  v2.0: Scaler refit during retraining
  - v2.0  v3.0: Scaler refit during fine-tuning

- **Minor version (vX.{minor}):** Increments for metadata updates (no refit)
  - v1.0  v1.1: Added it_on field, same mean/std
  - v1.1  v1.2: Updated timestamp, same parameters

- **Context suffix (_{context}):** Describes training scenario
  - _initial: First training on original data
  - _driftretrain: Refit after drift-triggered retraining
  - _garmin: Refit for Garmin device adaptation
  - _recalibration: Baseline update (usually same scaler as parent)

**Examples:**

| Evolution ID | Scaler Version | Trigger | Notes |
|--------------|----------------|---------|-------|
| evolution_20260106_091520 | scaler_v1.0_initial | Initial training | Fit on all_users_data_labeled.csv (n=385326) |
| evolution_20260118_142530 | scaler_v2.0_driftretrain | Drift (PSI>0.25) | Refit on original + 500 labeled production samples |
| evolution_20260125_163045_garmin | scaler_v3.0_garmin | Fine-tuning | Refit on 30% ADAM + 70% Garmin (n=1500) |
| evolution_20260201_103000_recalibration | scaler_v3.0_garmin | Recalibration | Same as parent (no refit), only baseline updated |

**Scaler Lineage Tracking:**

Manifest.json includes parent lineage:

```json
{
  "evolution_id": "evolution_20260125_163045_garmin",
  "evolution_type": "fine_tuning",
  "parent_evolution_id": "evolution_20260118_142530",
  "artifacts": {
    "scaler": {
      "file": "scaler_config.json",
      "version": "scaler_v3.0_garmin",
      "parent_version": "scaler_v2.0_driftretrain",
      "refit": true,
      "fit_on": "combined_ADAM_30pct_Garmin_70pct",
      "n_samples_fitted": 1500
    },
    "model": {
      "file": "model.keras",
      "version": "model_v3_finetuned",
      "parent_version": "model_v2",
      "training_mode": "fine_tuning",
      "frozen_layers": 10
    }
  }
}
```

This creates **scaler genealogy tree**:
```
scaler_v1.0_initial (n=385326)
   scaler_v2.0_driftretrain (n=385826, added 500 labeled prod samples)
       scaler_v3.0_garmin (n=1500, 30% old + 70% Garmin)
```

---

#### Linking to Model Versions

**Model Version Schema:**

Models also use semantic versioning within manifest.json:

```json
{
  "artifacts": {
    "model": {
      "file": "model.keras",
      "version": "model_v3_finetuned",
      "architecture": "1D-CNN-BiLSTM",
      "parent_version": "model_v2",
      "parameters": 499131,
      "training_epochs": 20,
      "coupled_scaler_version": "scaler_v3.0_garmin"
    }
  }
}
```

**Critical field: coupled_scaler_version**

This field **explicitly links** model to scaler, preventing mismatches:

```python
# Loading with coupling validation
manifest = json.load(open("models/active/manifest.json"))
expected_scaler_version = manifest["artifacts"]["model"]["coupled_scaler_version"]

scaler_config = json.load(open("models/active/scaler_config.json"))
actual_scaler_version = scaler_config["version"]

if actual_scaler_version != expected_scaler_version:
    raise ValueError(
        f"Scaler version mismatch!\n"
        f"  Model expects: {expected_scaler_version}\n"
        f"  Scaler loaded: {actual_scaler_version}\n"
        f"  Evolution package may be corrupted."
    )
```

---

#### MLflow Integration

**Tag-Based Linking:**

MLflow runs use tags to link to filesystem evolution packages:

```python
with mlflow.start_run(run_name="evolution_20260125_163045_garmin"):
    # Provenance tags (link to filesystem)
    mlflow.set_tag("evolution_id", "evolution_20260125_163045_garmin")
    mlflow.set_tag("evolution_type", "fine_tuning")
    mlflow.set_tag("parent_evolution_id", "evolution_20260118_142530")
    mlflow.set_tag("git_commit", "a3f2d8b")
    
    # Version tags (explicit versions)
    mlflow.set_tag("model_version", "model_v3_finetuned")
    mlflow.set_tag("scaler_version", "scaler_v3.0_garmin")
    mlflow.set_tag("baseline_version", "baseline_20260125")
    
    # Context tags (what changed)
    mlflow.set_tag("scaler_refit", "true")
    mlflow.set_tag("scaler_fit_on", "ADAM_30pct_Garmin_70pct")
    mlflow.set_tag("model_retrained", "true")
    mlflow.set_tag("training_mode", "fine_tuning")
    
    # Parameters
    mlflow.log_params({
        "learning_rate": 1e-4,
        "frozen_layers": 10,
        "epochs": 20,
        "old_domain_ratio": 0.3,
        "new_domain_ratio": 0.7
    })
    
    # Metrics
    mlflow.log_metrics({
        "val_accuracy": 0.72,
        "baseline_accuracy": 0.145,
        "improvement": 0.575
    })
    
    # Artifacts (upload key files)
    mlflow.log_artifact("models/evolution_packages/evolution_20260125_163045_garmin/scaler_config.json")
    mlflow.log_artifact("models/evolution_packages/evolution_20260125_163045_garmin/model.keras")
    mlflow.log_artifact("models/evolution_packages/evolution_20260125_163045_garmin/manifest.json")
```

**Querying MLflow by Scaler Version:**

```python
import mlflow

# Find all runs using scaler_v3.0_garmin
runs = mlflow.search_runs(
    filter_string="tags.scaler_version = 'scaler_v3.0_garmin'",
    order_by=["start_time DESC"]
)

print(f"Found {len(runs)} runs with scaler_v3.0_garmin:")
for idx, run in runs.iterrows():
    print(f"  {run['tags.evolution_id']}: accuracy={run['metrics.val_accuracy']:.3f}")

# Find all runs where scaler was refit
refit_runs = mlflow.search_runs(
    filter_string="tags.scaler_refit = 'true'",
    order_by=["start_time DESC"]
)
```

**Bidirectional Linking:**

```
Filesystem                              MLflow Tracking Server
                       
evolution_20260125_163045_garmin/       Run ID: abc123
 manifest.json                        Tags:
    mlflow_run_id: "abc123" > evolution_id: evolution_20260125_163045_garmin
 scaler_config.json                      scaler_version: scaler_v3.0_garmin
    version: scaler_v3.0_garmin > model_version: model_v3_finetuned
 model.keras                          Artifacts:
                                              scaler_config.json (uploaded)
                                              model.keras (uploaded)
```

**Manifest.json includes MLflow run ID:**

```json
{
  "evolution_id": "evolution_20260125_163045_garmin",
  "mlflow_run_id": "abc123",
  "mlflow_experiment_id": "1",
  "mlflow_tracking_uri": "mlruns/"
}
```

**Benefits:**
- From filesystem, can find MLflow run: mlflow.get_run(manifest["mlflow_run_id"])
- From MLflow UI, can find filesystem: 	ags["evolution_id"] gives directory name
- Audit trail: Every evolution has corresponding MLflow run with full metadata

---

#### Rollback Rules

**Rollback Scenarios:**

| Scenario | Trigger | Rollback Target | Command |
|----------|---------|----------------|---------|
| **Fine-tuning failed** | Validation accuracy < baseline | Pre-tuning checkpoint | cp -r models/checkpoints/pre_finetuning_20260125/* models/active/ |
| **Production degradation** | Accuracy dropped >10% after deploy | Previous evolution | ln -sf models/evolution_packages/evolution_20260118_142530 models/active |
| **Data corruption** | Model/scaler files corrupted | DVC restore | cd models/evolution_packages && dvc checkout evolution_20260125_163045_garmin.dvc |
| **Git revert needed** | Bad commit deployed | Git checkout + DVC | git checkout <prev-commit> && dvc checkout |

**Rollback Procedure (Production Degradation):**

```bash
# Step 1: Identify current and target evolutions
current=
echo "Current: "

# Check MLflow for previous evolution
target="evolution_20260118_142530"  # Example: previous version
echo "Rolling back to: "

# Step 2: Validate target evolution exists
if [ ! -d "models/evolution_packages/" ]; then
    echo "ERROR: Target evolution not found!"
    exit 1
fi

# Step 3: Create checkpoint of current state (for potential re-rollforward)
cp -r models/active models/checkpoints/pre_rollback_

# Step 4: Atomic rollback (update symlink)
ln -sf "evolution_packages/" models/active

# Step 5: Verify rollback
readlink models/active  # Should show target evolution

# Step 6: Test inference with rollback evolution
python src/run_inference.py --input data/raw/test_sample.csv --output data/prepared/predictions/rollback_test.json

# Step 7: Log rollback event to MLflow
python -c "
import mlflow
with mlflow.start_run(run_name='rollback_to_'):
    mlflow.set_tag('action', 'rollback')
    mlflow.set_tag('from_evolution', '')
    mlflow.set_tag('to_evolution', '')
    mlflow.set_tag('reason', 'production_accuracy_degradation')
    mlflow.log_metric('rollback_timestamp', )
"

echo "Rollback complete:  -> "
```

**Rollback Safety Guarantees:**

1. **Atomic operation**: Symlink update is single filesystem operation (no partial state)
2. **Immediate effect**: Next inference run uses rollback evolution
3. **No data loss**: All evolutions preserved in evolution_packages/
4. **Audit trail**: MLflow logs rollback event with reason and timestamp
5. **Re-rollforward**: Can restore current evolution from checkpoint if needed

---

#### Repo Actions

**Required Implementations:**

1. **Create scripts/create_evolution_package.py**

```python
#!/usr/bin/env python3
\"\"\"
Create new evolution package with standardized structure.
Usage: python scripts/create_evolution_package.py --type fine_tuning --parent evolution_20260118_142530
\"\"\"
import argparse
from pathlib import Path
from datetime import datetime
import json
import yaml

def create_evolution_package(
    evolution_type: str,
    parent_evolution_id: str = None,
    context: str = ""
):
    # Generate evolution_id
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    evolution_id = f"evolution_{timestamp}"
    if context:
        evolution_id += f"_{context}"
    
    # Create directory
    evo_path = Path(f"models/evolution_packages/{evolution_id}")
    evo_path.mkdir(parents=True, exist_ok=True)
    
    # Initialize manifest
    manifest = {
        "evolution_id": evolution_id,
        "evolution_type": evolution_type,
        "timestamp": datetime.now().isoformat(),
        "parent_evolution_id": parent_evolution_id,
        "git_commit": None,  # Fill during deployment
        "mlflow_run_id": None,  # Fill during training
        "artifacts": {
            "model": {"file": "model.keras", "version": None},
            "scaler": {"file": "scaler_config.json", "version": None, "refit": False},
            "thresholds": {"file": "monitoring_thresholds.yaml", "version": None},
            "baseline": {"file": "drift_baseline.json", "version": None},
            "config": {"file": "training_config.yaml"},
            "report": {"file": "evaluation_report.json"}
        }
    }
    
    with open(evo_path / "manifest.json", "w") as f:
        json.dump(manifest, f, indent=2)
    
    print(f"Created evolution package: {evolution_id}")
    print(f"  Path: {evo_path}")
    print(f"  Manifest initialized")
    
    return evolution_id, evo_path

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--type", required=True, choices=["retraining", "recalibration", "fine_tuning", "architecture_update"])
    parser.add_argument("--parent", help="Parent evolution ID")
    parser.add_argument("--context", default="", help="Context suffix for evolution ID")
    args = parser.parse_args()
    
    create_evolution_package(args.type, args.parent, args.context)
```

2. **Update scripts/validate_evolution.py**

Add scaler-model coupling validation:
```python
def validate_scaler_model_coupling(evo_path: Path):
    \"\"\"Verify scaler and model versions are coupled correctly.\"\"\"
    manifest = json.load(open(evo_path / "manifest.json"))
    scaler_config = json.load(open(evo_path / "scaler_config.json"))
    
    # Check 1: Manifest declares scaler version
    manifest_scaler_version = manifest["artifacts"]["scaler"]["version"]
    if not manifest_scaler_version:
        raise ValueError(f"Manifest missing scaler version")
    
    # Check 2: Scaler config has matching version
    scaler_config_version = scaler_config.get("version")
    if scaler_config_version != manifest_scaler_version:
        raise ValueError(
            f"Scaler version mismatch:\n"
            f"  Manifest: {manifest_scaler_version}\n"
            f"  Scaler config: {scaler_config_version}"
        )
    
    # Check 3: Model declares coupled scaler version
    model_coupled_scaler = manifest["artifacts"]["model"].get("coupled_scaler_version")
    if model_coupled_scaler != scaler_config_version:
        raise ValueError(
            f"Model-scaler coupling broken:\n"
            f"  Model expects: {model_coupled_scaler}\n"
            f"  Scaler is: {scaler_config_version}"
        )
    
    print(f"Scaler-model coupling validated")
    print(f"  Evolution: {manifest['evolution_id']}")
    print(f"  Scaler: {scaler_config_version}")
    print(f"  Model: {manifest['artifacts']['model']['version']}")
```

3. **Create docs/versioning_scheme.md**

Document the complete versioning scheme:
- Evolution ID format and examples
- Scaler semantic versioning rules
- Model versioning rules
- MLflow tagging conventions
- Querying examples
- Rollback procedures

4. **Update MLflow logging in src/mlflow_tracking.py**

Add helper method for evolution tracking:
```python
def log_evolution(self, evolution_id: str, manifest_path: Path):
    \"\"\"Log evolution package to MLflow with standardized tags.\"\"\"
    manifest = json.load(open(manifest_path))
    
    # Set evolution tags
    mlflow.set_tag("evolution_id", evolution_id)
    mlflow.set_tag("evolution_type", manifest["evolution_type"])
    mlflow.set_tag("parent_evolution_id", manifest.get("parent_evolution_id"))
    
    # Set version tags
    mlflow.set_tag("model_version", manifest["artifacts"]["model"]["version"])
    mlflow.set_tag("scaler_version", manifest["artifacts"]["scaler"]["version"])
    mlflow.set_tag("baseline_version", manifest["artifacts"]["baseline"]["version"])
    
    # Set context tags
    mlflow.set_tag("scaler_refit", str(manifest["artifacts"]["scaler"]["refit"]).lower())
    
    # Upload artifacts
    mlflow.log_artifact(str(manifest_path))
    mlflow.log_artifact(str(manifest_path.parent / "scaler_config.json"))
    
    # Save MLflow run ID back to manifest
    manifest["mlflow_run_id"] = mlflow.active_run().info.run_id
    with open(manifest_path, "w") as f:
        json.dump(manifest, f, indent=2)
```

---

#### CITATIONS

1. **(Evolution Package Structure):** docs/output_1801_2026-01-18.md, p.2216-2280 (Pair 08 Q2) - Defines evolution package with 7 co-versioned artifacts in single directory. Recommends timestamp-based evolution_id as primary key.

2. **(MLflow Experiment Tracking):** papers/mlops_production/Building-Scalable-MLOps-Optimizing-Machine-Learning-Deployment-and-Operations.pdf, p.89-102 - MLflow best practices: use tags for version linking (model_version, data_version tags), upload artifacts to runs, create bidirectional linking between filesystem and tracking server.

3. **(Semantic Versioning for ML):** papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.120-135 - Versioning scheme for ML artifacts: major version for retraining, minor for config changes. Recommends parent lineage tracking in manifest file.

4. **(Atomic Deployment):** papers/mlops_production/Essential_MLOps_Data_Science_Horizons_2023_Data_Science_Horizons_Final_2023.pdf, p.67-73 - Symlink-based deployment for atomic updates: single filesystem operation prevents partial state. Enables instant rollback without file copying.

5. **(Rollback Strategies):** papers/mlops_production/Resilience-aware MLOps for AI-based medical diagnostic system  2024.pdf, p.18-25 - Rollback procedures for production ML systems: maintain checkpoints, validate target evolution, log rollback events, test after rollback. Emphasizes audit trail for regulatory compliance.

---

**Summary Table:**

| Aspect | Implementation | Example | Purpose |
|--------|----------------|---------|---------|
| **Evolution ID** | Timestamp-based with context suffix | evolution_20260125_163045_garmin | Primary key for all artifacts in package |
| **Directory Structure** | Flat namespace per evolution | evolution_packages/{id}/*.{json,yaml,keras} | Simplifies loading, prevents path confusion |
| **Scaler Versioning** | Semantic (v{major}.{minor}_{context}) | scaler_v3.0_garmin | Tracks refit (major) vs metadata changes (minor) |
| **Model Versioning** | Semantic (model_v{major}_{context}) | model_v3_finetuned | Aligned with scaler major version |
| **Scaler-Model Coupling** | coupled_scaler_version field in manifest | model_v3 expects scaler_v3.0_garmin | Prevents mismatched deployments |
| **Active Deployment** | Symlink to current evolution | models/active -> evolution_20260125_163045_garmin/ | Atomic deployment, stable paths |
| **MLflow Linking** | Tags (evolution_id, scaler_version, model_version) | filter: tags.scaler_version = 'scaler_v3.0_garmin' | Bidirectional filesystem  tracking |
| **Manifest** | manifest.json with provenance + lineage | parent_evolution_id, mlflow_run_id, git_commit | Audit trail, rollback target identification |
| **Checkpoints** | Pre-deployment snapshots | models/checkpoints/pre_finetuning_20260125/ | Rollback safety, validation failures |
| **DVC Tracking** | evolution_packages.dvc | models/evolution_packages.dvc | Large file versioning (model weights) |
| **Git Tracking** | Configs + manifest | git add evolution_packages/{id}/*.{json,yaml} | Config versioning, code-artifact linking |
| **Rollback** | Symlink update + MLflow log | ln -sf evolution_20260118_142530 models/active | Instant restore, audit trail |

---



## Pair 12 — Gravity Removal

**Context:** Accelerometer measures total acceleration = body movement + gravity (~9.81 m/s). Gravity appears differently based on device orientation. Need to decide: remove gravity or keep it?

---

### **Q1: How does gravity appear in accelerometer signals, and does it affect all axes (X/Y/Z)?**

**Decision:** Gravity affects **ALL three axes** (X, Y, Z) based on device **orientation**, not just Z. The gravity vector (~9.81 m/s pointing downward toward Earth) projects onto each axis proportionally to the angle between that axis and vertical. For a smartwatch:
- **Flat horizontal (Z vertical):** Az  9.8, Ax  0, Ay  0
- **Tilted 45:** Gravity distributed across 2-3 axes
- **Wrist naturally worn:** Gravity varies with arm position (writing: Z-dominant, arm down: Y-dominant, arm raised: mixed)

**Detailed Explanation:**

#### **1. Physics of Gravity in Accelerometer**

An accelerometer measures **proper acceleration** (non-gravitational forces) in its local reference frame. When at rest, it detects gravity as a constant upward force (Newton's third law: sensor pushes against Earth).

**Mathematical Model:**
`
A_measured = A_body + A_gravity
`

Where:
- A_body = dynamic motion (what we want for HAR)
- A_gravity = constant 9.81 m/s vector pointing "down" in Earth frame

**Key Insight from Repo Code:**  
From [src/sensor_data_pipeline.py](src/sensor_data_pipeline.py#L732):
> "Gravity is a constant (DC) component at ~9.81 m/s. Human movement is dynamic (varying frequency). High-pass filter removes low-frequency components (gravity)."

From [notebooks/exploration/gravity_removal_demo.ipynb](notebooks/exploration/gravity_removal_demo.ipynb#L18):
> "Accelerometer measures: Body movement + Gravity  
> Gravity: ~9.81 m/s constant force pointing DOWN (toward Earth)  
> Problem: Gravity 'pollutes' our activity recognition data"

---

#### **2. Orientation-Dependent Distribution Across Axes**

Gravity projects onto X/Y/Z based on device rotation:

**Scenario 1: Watch Lying Flat (Production Data)**
`
Orientation: Z-axis vertical, X/Y horizontal
Result:      Az = -9.83 m/s, Ax  0, Ay  0
Evidence:    notebooks/exploration/gravity_removal_demo.ipynb shows production Az mean = -9.83
`

**Scenario 2: Wrist Naturally Worn (Training Data)**
`
Orientation: Dynamic (arm swings, rotates during activities)
Result:      Gravity distributed: Az = -3.42 m/s (partial), Ax/Ay have components
Evidence:    Training data shows Az = -3.42 (gravity pre-removed or varied orientations)
`

**Proof from Repo Evidence:**  
From [notebooks/exploration/gravity_removal_demo.ipynb](notebooks/exploration/gravity_removal_demo.ipynb#L2326):

| Data Source | Az Mean | Gravity Status | Notes |
|-------------|---------|----------------|-------|
| Production (Your Data) | **-9.83 m/s** |  PRESENT | Phone flat, Z-axis pointing down |
| Training (Research Paper) | -3.42 m/s |  REMOVED | Varied orientations, gravity filtered |
| Sample Dataset | -4.15 m/s |  REMOVED | Similar to training data |
| Anxiety-Pocket (phone vertical) | **+9.29 m/s** |  PRESENT | Phone Z-axis points UP |
| Anxiety-Wrist (smartwatch) | -0.34 m/s |  REMOVED | Pre-processed wearable data |

**Critical Observation:**  
Sign difference (+9.29 vs -9.83) shows **same gravity magnitude, opposite orientations**. This proves gravity affects **direction-dependent projection** onto axes, not absolute values.

---

#### **3. Why All Axes Are Affected (Not Just Z)**

**Misconception:** "Gravity only affects Z-axis (vertical)"  
**Reality:** Gravity affects whichever axis aligns with vertical direction.

From [docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md](docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md#L28):
> "Left vs. right wrist sensors have **mirrored coordinate frames**:  
> - Left wrist: X-axis points toward body, Y toward elbow  
> - Right wrist: X-axis points away from body, Y toward elbow  
> - Same gesture produces **different sensor signatures** depending on which wrist"

**Example: Hand Tapping Activity**
`
Watch on table (Az-dominant):
  Ax = 0.1  0.5 m/s    small motion
  Ay = -0.2  0.8 m/s   small motion  
  Az = -9.8  2.0 m/s   gravity + tapping motion

Watch on wrist (gravity distributed):
  Ax = -2.1  3.5 m/s   direct tapping force + gravity component
  Ay = 1.8  2.9 m/s    wrist rotation + gravity component
  Az = -5.4  4.1 m/s   partial gravity (arm not vertical)
`

**When Axis Distribution Changes:**
- **Idle/Sitting:** Arm position determines which axis captures gravity
- **Writing:** Wrist tilted ~30, Z gets 85% of gravity
- **Typing:** Wrist flat, Z gets 100% of gravity  
- **Walking:** Arm swings, gravity shifts between Y and Z
- **Eating:** Wrist rotates, gravity cycles through all axes

---

#### **4. Frequency Domain Explanation**

From [src/preprocess_data.py](src/preprocess_data.py#L248):
`python
class GravityRemover:
    """
    Remove gravity component from accelerometer data using high-pass filter.
    Apply AFTER unit conversion (expects m/s input).
    
    - Gravity = LOW frequency signal (constant, never changes)
    - Body movement = HIGH frequency signal (varies with activity)
    - High-pass filter (0.3 Hz cutoff) = keeps movement, removes gravity
    """
`

**Why 0.3 Hz Cutoff?**
- Gravity: 0 Hz (DC component, static)
- Slow orientation changes: 0.01-0.2 Hz (arm repositioning)
- Body movement: 0.5-20 Hz (walking ~2 Hz, hand gestures 3-10 Hz)

**Standard Practice:**  
From [src/sensor_data_pipeline.py](src/sensor_data_pipeline.py#L729):
> "Uses a Butterworth high-pass filter following the approach from the **UCI HAR dataset (Anguita et al., 2013)**.  
> Cutoff at 0.3 Hz captures gravity while preserving movement."

---

#### **5. Repo Evidence: Gravity Detection and Removal Implementation**

**Automatic Gravity Detection:**  
From [scripts/post_inference_monitoring.py](scripts/post_inference_monitoring.py) drift reports:
`json
{
  "gravity_check": {
    "az_mean": -1.95,
    "expected": -9.8,
    "within_tolerance": false
  },
  "type": "gravity_anomaly",
  "message": "Az mean (-1.95) differs from expected gravity (-9.8)",
  "severity": "HIGH"
}
`

**Gravity Removal Process (3-Order Butterworth):**  
From [src/sensor_data_pipeline.py](src/sensor_data_pipeline.py#L820-845):
`python
def remove_gravity(self, acceleration_data: np.ndarray) -> np.ndarray:
    """Apply high-pass filter to remove gravity from acceleration data."""
    if not self.enable_gravity_removal:
        return acceleration_data
    
    # Handle 1D and 2D arrays
    if acceleration_data.ndim == 1:
        return filtfilt(self.b, self.a, acceleration_data)  # Zero-phase filtering
    else:
        return filtfilt(self.b, self.a, acceleration_data, axis=0)  # Apply to ALL axes
`

**Critical Code Note:**  
xis=0 applies filter to **each column independently**  filters Ax, Ay, Az separately  removes gravity component from ALL axes, not just Z.

---

#### **6. Domain Shift Impact**

From [docs/technical/root_cause_low_accuracy.md](docs/technical/root_cause_low_accuracy.md#L138-145):
> " Gravity Validation:  
> - Az mean in sensor_fused_50Hz.csv: -1001.56 milliG  
> - After conversion: -1001.56  0.00981 = **-9.825 m/s**   
> - This confirms data is from watch lying **flat on table** (not on wrist)"

**Problem:**
- Training data: Varied orientations (gravity distributed or pre-removed, Az  -3.4)
- Production data: Flat orientation (gravity concentrated in Z, Az  -9.8)
- Result: Model sees **constant offset**  biased predictions (100% "hand_tapping")

**Solution:**  
Remove gravity to normalize across orientations:
`
Before removal:  Az_prod = -9.8, Az_train = -3.4   6.4 m/s mismatch
After removal:   Az_prod  0.0,  Az_train  0.0    0.0 m/s mismatch
`

---

**CITATIONS (Pair 12, Q1):**

1. **(src/sensor_data_pipeline.py, lines 724-740):** GravityRemovalPreprocessor class with theory: "Gravity is a constant (DC) component at ~9.81 m/s. High-pass filter removes low-frequency components (gravity). Cutoff at 0.3 Hz captures gravity while preserving movement. Following UCI HAR dataset (Anguita et al., 2013)."

2. **(notebooks/exploration/gravity_removal_demo.ipynb, lines 1-30):** Gravity definition and production data diagnosis: "Accelerometer measures: Body movement + Gravity (~9.81 m/s constant force pointing DOWN). Production data: Az  -9.83 m/s (constant gravity signature). Training data: Az varied (different arm positions). Result: Model predicts 100% 'hand_tapping' (biased by gravity)."

3. **(notebooks/exploration/gravity_removal_demo.ipynb, lines 2326-2360):** Multi-source Az comparison table showing orientation-dependent gravity: Production (-9.83, flat), Training (-3.42, removed), Anxiety-Pocket (+9.29, vertical Z-up), Anxiety-Wrist (-0.34, removed). Proves sign difference = orientation difference.

4. **(docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md, lines 24-48):** Coordinate frame mirroring and axis distribution: "Left vs. right wrist sensors have mirrored coordinate frames. Same gesture produces different sensor signatures depending on which wrist. Signal asymmetry: direct accelerations (2-10 m/s) vs. indirect motion (~0.1-0.5 m/s)."

5. **(src/sensor_data_pipeline.py, lines 820-845):** Gravity removal implementation applying filtfilt to ALL axes (axis=0 parameter): "Zero-phase filtering. Handle 1D and 2D arrays. Apply filter to each column (axis)  removes gravity from Ax, Ay, Az independently."

6. **(reports/monitoring/*/drift_report.json, lines 1-30):** Automated gravity anomaly detection: "gravity_check: az_mean: -1.95, expected: -9.8, within_tolerance: false. Type: gravity_anomaly. Message: Az mean differs from expected gravity. Severity: HIGH." Shows monitoring framework validates gravity presence.

---

**Summary Table (Q1):**

| **Aspect** | **Details** | **Repo Evidence** |
|------------|-------------|-------------------|
| **Physics** | Gravity = constant 9.81 m/s vector in Earth frame | sensor_data_pipeline.py lines 732-735 |
| **Axes Affected** | ALL three (X, Y, Z) based on orientation | gravity removal applies axis=0 to all columns |
| **Distribution** | Flat: Az=-9.8, Ax/Ay0. Tilted: distributed across 2-3 axes | gravity_removal_demo.ipynb Az comparison table |
| **Frequency** | 0 Hz (DC component), removed by 0.3 Hz high-pass filter | preprocess_data.py GravityRemover class |
| **Detection** | Automated: \|Az_mean\| > 5 m/s = gravity present | drift_report.json gravity_check threshold |
| **Production Impact** | Az=-9.83 (flat table) vs Az=-3.42 (training, varied) | root_cause_low_accuracy.md gravity validation |
| **Orientation Dependency** | Sign flip: +9.29 (Z-up) vs -9.83 (Z-down) for same gravity | anxiety dataset pocket vs table comparison |
| **Wrist Placement** | Left/right wrist mirror coordinate frames  different gravity projection | HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md |
| **Removal Method** | 3rd-order Butterworth high-pass, 0.3 Hz cutoff, zero-phase filtfilt | sensor_data_pipeline.py remove_gravity() |
| **Standard Practice** | UCI HAR dataset (Anguita 2013) uses same 0.3 Hz cutoff | sensor_data_pipeline.py docstring |

---



## Pair 12 - Gravity Removal

**Context:** Accelerometer measures total acceleration = body movement + gravity (~9.81 m/s squared). Gravity appears differently based on device orientation. Need to decide: remove gravity or keep it?

---

### **Q1: How does gravity appear in accelerometer signals, and does it affect all axes (X/Y/Z)?**

**Decision:** Gravity affects **ALL three axes** (X, Y, Z) based on device **orientation**, not just Z. The gravity vector (~9.81 m/s squared pointing downward toward Earth) projects onto each axis proportionally to the angle between that axis and vertical. For a smartwatch:
- **Flat horizontal (Z vertical):** Az approximately plus-minus 9.8, Ax approximately 0, Ay approximately 0
- **Tilted 45 degrees:** Gravity distributed across 2-3 axes
- **Wrist naturally worn:** Gravity varies with arm position (writing: Z-dominant, arm down: Y-dominant, arm raised: mixed)

**Detailed Explanation:**

#### **1. Physics of Gravity in Accelerometer**

An accelerometer measures **proper acceleration** (non-gravitational forces) in its local reference frame. When at rest, it detects gravity as a constant upward force (Newton's third law: sensor pushes against Earth).

**Mathematical Model:**
Measured acceleration = Body acceleration + Gravity acceleration

Where:
- Body acceleration = dynamic motion (what we want for HAR)
- Gravity acceleration = constant plus-minus 9.81 m/s squared vector pointing "down" in Earth frame

**Key Insight from Repo Code:**  
From src/sensor_data_pipeline.py lines 732-735:
"Gravity is a constant (DC) component at approximately 9.81 m/s squared. Human movement is dynamic (varying frequency). High-pass filter removes low-frequency components (gravity)."

From notebooks/exploration/gravity_removal_demo.ipynb line 18:
"Accelerometer measures: Body movement + Gravity  
Gravity: approximately 9.81 m/s squared constant force pointing DOWN (toward Earth)  
Problem: Gravity 'pollutes' our activity recognition data"

---

#### **2. Orientation-Dependent Distribution Across Axes**

Gravity projects onto X/Y/Z based on device rotation:

**Scenario 1: Watch Lying Flat (Production Data)**
Orientation: Z-axis vertical, X/Y horizontal
Result: Az = -9.83 m/s squared, Ax approximately 0, Ay approximately 0
Evidence: notebooks/exploration/gravity_removal_demo.ipynb shows production Az mean = -9.83

**Scenario 2: Wrist Naturally Worn (Training Data)**
Orientation: Dynamic (arm swings, rotates during activities)
Result: Gravity distributed: Az = -3.42 m/s squared (partial), Ax/Ay have components
Evidence: Training data shows Az = -3.42 (gravity pre-removed or varied orientations)

**Proof from Repo Evidence:**  
From notebooks/exploration/gravity_removal_demo.ipynb line 2326:

| Data Source | Az Mean | Gravity Status | Notes |
|-------------|---------|----------------|-------|
| Production (Your Data) | **-9.83 m/s squared** | PRESENT | Phone flat, Z-axis pointing down |
| Training (Research Paper) | -3.42 m/s squared | REMOVED | Varied orientations, gravity filtered |
| Sample Dataset | -4.15 m/s squared | REMOVED | Similar to training data |
| Anxiety-Pocket (phone vertical) | **+9.29 m/s squared** | PRESENT | Phone Z-axis points UP |
| Anxiety-Wrist (smartwatch) | -0.34 m/s squared | REMOVED | Pre-processed wearable data |

**Critical Observation:**  
Sign difference (+9.29 vs -9.83) shows **same gravity magnitude, opposite orientations**. This proves gravity affects **direction-dependent projection** onto axes, not absolute values.

---

#### **3. Why All Axes Are Affected (Not Just Z)**

**Misconception:** "Gravity only affects Z-axis (vertical)"  
**Reality:** Gravity affects whichever axis aligns with vertical direction.

From docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md lines 24-48:
"Left vs. right wrist sensors have **mirrored coordinate frames**:  
- Left wrist: X-axis points toward body, Y toward elbow  
- Right wrist: X-axis points away from body, Y toward elbow  
- Same gesture produces **different sensor signatures** depending on which wrist"

**Example: Hand Tapping Activity**

Watch on table (Az-dominant):
  Ax = 0.1 plus-minus 0.5 m/s squared   (small motion)
  Ay = -0.2 plus-minus 0.8 m/s squared  (small motion)  
  Az = -9.8 plus-minus 2.0 m/s squared  (gravity + tapping motion)

Watch on wrist (gravity distributed):
  Ax = -2.1 plus-minus 3.5 m/s squared  (direct tapping force + gravity component)
  Ay = 1.8 plus-minus 2.9 m/s squared   (wrist rotation + gravity component)
  Az = -5.4 plus-minus 4.1 m/s squared  (partial gravity, arm not vertical)

**When Axis Distribution Changes:**
- **Idle/Sitting:** Arm position determines which axis captures gravity
- **Writing:** Wrist tilted approximately 30 degrees, Z gets 85 percent of gravity
- **Typing:** Wrist flat, Z gets 100 percent of gravity  
- **Walking:** Arm swings, gravity shifts between Y and Z
- **Eating:** Wrist rotates, gravity cycles through all axes

---

#### **4. Frequency Domain Explanation**

From src/preprocess_data.py lines 248-260:
class GravityRemover: Remove gravity component from accelerometer data using high-pass filter.
- Gravity = LOW frequency signal (constant, never changes)
- Body movement = HIGH frequency signal (varies with activity)
- High-pass filter (0.3 Hz cutoff) = keeps movement, removes gravity

**Why 0.3 Hz Cutoff?**
- Gravity: 0 Hz (DC component, static)
- Slow orientation changes: 0.01-0.2 Hz (arm repositioning)
- Body movement: 0.5-20 Hz (walking approximately 2 Hz, hand gestures 3-10 Hz)

**Standard Practice:**  
From src/sensor_data_pipeline.py line 729:
"Uses a Butterworth high-pass filter following the approach from the **UCI HAR dataset (Anguita et al., 2013)**.  
Cutoff at 0.3 Hz captures gravity while preserving movement."

---

#### **5. Repo Evidence: Gravity Detection and Removal Implementation**

**Automatic Gravity Detection:**  
From scripts/post_inference_monitoring.py drift reports:
gravity_check shows az_mean: -1.95, expected: -9.8, within_tolerance: false
Type: gravity_anomaly with message "Az mean (-1.95) differs from expected gravity (-9.8)" and severity HIGH

**Gravity Removal Process (3-Order Butterworth):**  
From src/sensor_data_pipeline.py lines 820-845:
Function remove_gravity applies high-pass filter to remove gravity from acceleration data.
If gravity removal disabled, returns data unchanged.
For 1D arrays: applies filtfilt with zero-phase filtering
For 2D arrays: applies filtfilt with axis=0 to filter each column independently

**Critical Code Note:**  
axis=0 applies filter to **each column independently**, filters Ax, Ay, Az separately, removes gravity component from ALL axes, not just Z.

---

#### **6. Domain Shift Impact**

From docs/technical/root_cause_low_accuracy.md lines 138-145:
"Gravity Validation:  
- Az mean in sensor_fused_50Hz.csv: -1001.56 milliG  
- After conversion: -1001.56 times 0.00981 = **-9.825 m/s squared**  
- This confirms data is from watch lying **flat on table** (not on wrist)"

**Problem:**
- Training data: Varied orientations (gravity distributed or pre-removed, Az approximately -3.4)
- Production data: Flat orientation (gravity concentrated in Z, Az approximately -9.8)
- Result: Model sees **constant offset**, biased predictions (100 percent "hand_tapping")

**Solution:**  
Remove gravity to normalize across orientations:
Before removal: Az_prod = -9.8, Az_train = -3.4 leads to 6.4 m/s squared mismatch
After removal: Az_prod approximately 0.0, Az_train approximately 0.0 leads to 0.0 m/s squared mismatch

---

**CITATIONS (Pair 12, Q1):**

1. **(src/sensor_data_pipeline.py, lines 724-740):** GravityRemovalPreprocessor class with theory: "Gravity is a constant (DC) component at approximately 9.81 m/s squared. High-pass filter removes low-frequency components (gravity). Cutoff at 0.3 Hz captures gravity while preserving movement. Following UCI HAR dataset (Anguita et al., 2013)."

2. **(notebooks/exploration/gravity_removal_demo.ipynb, lines 1-30):** Gravity definition and production data diagnosis: "Accelerometer measures: Body movement + Gravity (approximately 9.81 m/s squared constant force pointing DOWN). Production data: Az approximately -9.83 m/s squared (constant gravity signature). Training data: Az varied (different arm positions). Result: Model predicts 100 percent 'hand_tapping' (biased by gravity)."

3. **(notebooks/exploration/gravity_removal_demo.ipynb, lines 2326-2360):** Multi-source Az comparison table showing orientation-dependent gravity: Production (-9.83, flat), Training (-3.42, removed), Anxiety-Pocket (+9.29, vertical Z-up), Anxiety-Wrist (-0.34, removed). Proves sign difference equals orientation difference.

4. **(docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md, lines 24-48):** Coordinate frame mirroring and axis distribution: "Left vs. right wrist sensors have mirrored coordinate frames. Same gesture produces different sensor signatures depending on which wrist. Signal asymmetry: direct accelerations (plus-minus 2-10 m/s squared) vs. indirect motion (approximately 0.1-0.5 m/s squared)."

5. **(src/sensor_data_pipeline.py, lines 820-845):** Gravity removal implementation applying filtfilt to ALL axes (axis=0 parameter): "Zero-phase filtering. Handle 1D and 2D arrays. Apply filter to each column (axis), removes gravity from Ax, Ay, Az independently."

6. **(reports/monitoring/drift_report.json, lines 1-30):** Automated gravity anomaly detection: "gravity_check with az_mean: -1.95, expected: -9.8, within_tolerance: false. Type: gravity_anomaly with message Az mean differs from expected gravity and severity HIGH." Shows monitoring framework validates gravity presence.

---

**Summary Table (Q1):**

| **Aspect** | **Details** | **Repo Evidence** |
|------------|-------------|-------------------|
| **Physics** | Gravity = constant plus-minus 9.81 m/s squared vector in Earth frame | sensor_data_pipeline.py lines 732-735 |
| **Axes Affected** | ALL three (X, Y, Z) based on orientation | gravity removal applies axis=0 to all columns |
| **Distribution** | Flat: Az=-9.8, Ax/Ay approximately 0. Tilted: distributed across 2-3 axes | gravity_removal_demo.ipynb Az comparison table |
| **Frequency** | 0 Hz (DC component), removed by 0.3 Hz high-pass filter | preprocess_data.py GravityRemover class |
| **Detection** | Automated: absolute value of Az_mean greater than 5 m/s squared = gravity present | drift_report.json gravity_check threshold |
| **Production Impact** | Az=-9.83 (flat table) vs Az=-3.42 (training, varied) | root_cause_low_accuracy.md gravity validation |
| **Orientation Dependency** | Sign flip: +9.29 (Z-up) vs -9.83 (Z-down) for same gravity | anxiety dataset pocket vs table comparison |
| **Wrist Placement** | Left/right wrist mirror coordinate frames leading to different gravity projection | HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md |
| **Removal Method** | 3rd-order Butterworth high-pass, 0.3 Hz cutoff, zero-phase filtfilt | sensor_data_pipeline.py remove_gravity function |
| **Standard Practice** | UCI HAR dataset (Anguita 2013) uses same 0.3 Hz cutoff | sensor_data_pipeline.py docstring |

---




### **Q2: When does gravity removal help vs harm for smartwatch HAR? Is it necessary? Provide ablation plan.**

**Decision:** Gravity removal is **HIGHLY RECOMMENDED but not strictly necessary**—helps when train/production orientations differ, harms when they match. For this repo: **REMOVE GRAVITY** because training data has varied/removed gravity (Az=-3.4) while production is flat-oriented (Az=-9.8), creating 6.4 m/s squared domain shift. However, implement as **YAML-configurable toggle** for ablation testing (enable_gravity_removal: true/false) to empirically validate impact on accuracy, confidence, and drift metrics.

**Detailed Explanation:**

#### **1. When Gravity Removal HELPS**

**Scenario A: Orientation Mismatch (Your Case)**

Training conditions:
- Watch worn on wrist during activities
- Arm moves naturally (varied orientations)
- Az mean = -3.42 m/s squared (gravity distributed or pre-removed)

Production conditions:
- Watch mostly flat on desk or consistent wrist position
- Limited orientation variation
- Az mean = -9.83 m/s squared (full gravity in Z-axis)

**Problem:** Model trained on Az approximately -3.4, sees Az approximately -9.8 in production
- Constant +6.4 m/s squared offset acts as **distribution shift**
- Model interprets gravity as "high-intensity activity"
- Result: 100 percent "hand_tapping" predictions (from repo logs)

**Solution Effect:**  
From notebooks/exploration/gravity_removal_demo.ipynb lines 490-540:
Before removal: Az mean = -8.39 m/s squared (gravity present)
After removal: Az mean = 0.13 m/s squared (gravity removed)
Change: 8.52 m/s squared shift toward training distribution

**Benefits:**
1. **Orientation Invariance:** Same activity recognized regardless of wrist angle
2. **Cross-Device Generalization:** Garmin to Apple Watch transfer works
3. **Reduced Drift:** Baseline drift metrics stabilize (no gravity fluctuations)
4. **Improved Confidence:** Softmax scores less biased by constant offset

---

**Scenario B: Cross-Position HAR (Multi-Sensor Systems)**

From docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md lines 57-79:
"Cross-Position Heterogeneity: Variability when device is on different body parts (e.g., wrist vs. pocket)."

Example from anxiety dataset (notebooks/exploration/gravity_removal_demo.ipynb lines 2385-2478):
- Pocket sensor: Az = +9.29 m/s squared (phone vertical, Z-up)
- Wrist sensor: Az = -0.34 m/s squared (smartwatch, gravity removed)
- Same user, same time, 9.6 m/s squared difference due to placement

**Without Removal:** Model cannot fuse pocket+wrist features (contradictory gravity)  
**With Removal:** Both normalized to Az approximately 0, fusion works

---

**Scenario C: Multi-User Studies (Dominant vs Non-Dominant Hand)**

From docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md lines 24-35:
"Left wrist: X-axis points toward body, Y toward elbow  
Right wrist: X-axis points away from body, Y toward elbow  
Same gesture produces **different sensor signatures** depending on which wrist"

**Impact:**
- Left wrist tapping: Gravity projects to -Ax (toward body)
- Right wrist tapping: Gravity projects to +Ax (away from body)
- Without removal: Ax sign difference breaks model
- With removal: Ax sign reflects actual motion direction

---

#### **2. When Gravity Removal HARMS**

**Scenario A: Orientation-Consistent Training and Production**

If training data collected with watch flat on table AND production also flat:
- Training Az = -9.8 m/s squared, Production Az = -9.8 m/s squared
- Gravity is a **consistent feature**, not noise
- Model learns: "Az approximately -10 = idle, Az approximately -15 = walking downstairs"
- Removing gravity: Loses vertical acceleration information

**Example (Hypothetical):**
Activity: Walking downstairs
- With gravity: Az = -9.8 (gravity) - 3.0 (downward motion) = -12.8 m/s squared
- Without gravity: Az = -3.0 m/s squared
- Model trained WITH gravity expects -12.8, sees -3.0 after removal = confused

---

**Scenario B: Activities Requiring Absolute Orientation**

**Fall Detection:**
- Fall = sudden change in orientation (horizontal to vertical)
- Gravity vector rotation is the SIGNAL, not noise
- Removing gravity: Cannot detect orientation change

**Posture Estimation:**
- Standing: Az approximately -9.8 (vertical)
- Lying down: Ax or Ay approximately -9.8 (horizontal)
- Removing gravity: All postures look identical (Az/Ax/Ay approximately 0)

**Your Repo Activities (Hand Scratching, Knuckle Cracking):**
- Do NOT require absolute orientation
- Only care about wrist motion dynamics (frequency, amplitude)
- Gravity removal: Safe and beneficial

---

**Scenario C: Low Signal-to-Noise Activities**

**Idle/Sitting:**
- Body motion: approximately 0.1-0.5 m/s squared (minimal)
- Gravity: 9.8 m/s squared (dominant)
- Without removal: Az = -9.8 plus-minus 0.2 (clear signal)
- With removal: Az = 0.0 plus-minus 0.2 (noisy, hard to distinguish from sensor noise)

**Your Case:** NOT a concern because:
- Activities have strong motion: hand_scratching (Ax = plus-minus 5 m/s squared)
- Idle already challenging (model predicts 100 percent hand_tapping regardless)

---

#### **3. Necessity Assessment for This Repo**

From notebooks/exploration/gravity_removal_demo.ipynb lines 2326-2360:

| Data Source | Az Mean | Gravity Status | Production Accuracy |
|-------------|---------|----------------|---------------------|
| Production (Current) | -9.83 m/s squared | PRESENT | 14-15 percent (biased) |
| Training (Baseline) | -3.42 m/s squared | REMOVED/VARIED | 87.1 percent (F1) |

**Mismatch Magnitude:** 6.4 m/s squared (65 percent of gravity!)

**Expected Impact of Removal:**
1. **Accuracy:** +15-25 percentage points (hypothesis)
   - Training F1 = 0.871 on varied-orientation data
   - Production F1 = 0.14-0.15 with flat-orientation data
   - Removing gravity: Aligns distributions

2. **Confidence:** +0.2-0.4 softmax score improvement
   - Current: Mean confidence = 0.45 (low, model uncertain)
   - Expected: Mean confidence = 0.65-0.85 (model recognizes patterns)

3. **Drift Metrics:** PSI drops from 0.3-0.5 to below 0.1
   - Current: HIGH drift on Az channel (constant offset)
   - After removal: LOW drift (only motion variance matters)

**Is It Necessary?**
- **Strictly necessary:** NO (model can theoretically learn constant offset)
- **Practically essential:** YES (6.4 m/s squared shift is 2x larger than typical motion)
- **MLOps necessary:** YES (enables orientation-invariant monitoring)

---

#### **4. Ablation Plan: Empirical Validation**

**Goal:** Quantify gravity removal impact on accuracy, confidence, drift, and embedding metrics.

**Design: 2x2 Factorial Experiment**

| Experiment | Training Data | Production Data | Hypothesis |
|------------|---------------|-----------------|------------|
| **Baseline (Current)** | Varied orientation (Az=-3.4) | Flat (Az=-9.8) | Low accuracy (14-15 percent) |
| **Exp 1: Remove Prod Only** | Varied (Az=-3.4) | Flat REMOVED (Az approximately 0) | Medium accuracy (50-60 percent) |
| **Exp 2: Keep Both** | Flat (Az=-9.8, retrain) | Flat (Az=-9.8) | High accuracy IF retrained (80 percent +) |
| **Exp 3: Remove Both** | Varied REMOVED (Az approximately 0) | Flat REMOVED (Az approximately 0) | Highest (85 percent +, orientation-invariant) |

**Exp 1 Tests:** Does removing production gravity alone fix mismatch?  
**Exp 2 Tests:** Can model adapt to flat orientation with retraining?  
**Exp 3 Tests:** Does removing gravity from both sides maximize generalization?

---

**Implementation Steps:**

**Step 1: Config Toggle (Already Exists)**  
From config/pipeline_config.yaml lines 15-18:
enable_gravity_removal: false (current)
gravity_filter:
  cutoff_hz: 0.3
  order: 3

**Change to:** enable_gravity_removal: true

**Step 2: Create Experiment Tracking Script**

File: scripts/ablation_gravity_removal.py

Purpose:
- Run inference with gravity ON/OFF
- Log metrics to MLflow with tag gravity_removed: true/false
- Compare accuracy, confidence, drift, embedding distance

**Pseudocode:**
For experiment in ['baseline', 'gravity_on', 'gravity_off']:
  Set config enable_gravity_removal
  Run preprocess_data.py
  Run run_inference.py
  Run post_inference_monitoring.py
  Log to MLflow:
    - metrics.accuracy (if labeled subset available)
    - metrics.mean_confidence
    - metrics.drift_psi_az
    - metrics.embedding_mmd
    - tags.gravity_removed
    - tags.experiment_name

**Step 3: Evaluation Metrics (4 Categories)**

**A. Classification Metrics (Requires Labeled Subset)**
- Accuracy, Precision, Recall, F1 per class
- Confusion matrix comparison (baseline vs gravity_removed)
- Class-wise confidence distribution

**B. Confidence Metrics (Unlabeled Data)**
- Mean softmax score (higher = more confident)
- Entropy distribution (lower = sharper predictions)
- Percentage predictions above 0.8 confidence threshold

**C. Drift Metrics (Layer 3 Monitoring)**
- PSI per channel (Az should drop significantly)
- Wasserstein distance (production vs baseline)
- Kolmogorov-Smirnov statistic

**D. Embedding Metrics (Layer 4 Monitoring)**
- Maximum Mean Discrepancy (MMD) between train/prod embeddings
- Cosine similarity of feature distributions
- t-SNE visualization: Do clusters separate better?

---

**Step 4: Comparison Plots (6 Visualizations)**

**Plot 1: Az Distribution Comparison**
- Before removal: Histogram with mean=-9.8, peak at -10
- After removal: Histogram with mean=0, peak at 0
- Overlay training data histogram (Az=-3.4 or 0)

**Plot 2: Confidence Score Distribution**
- Baseline: Mean=0.45, most predictions 0.3-0.6 (uncertain)
- Gravity removed: Mean=0.70, most predictions 0.6-0.9 (confident)

**Plot 3: Per-Class Accuracy Comparison**
- Bar chart: Each activity (hand_scratching, sitting, etc.)
- Blue bars: Baseline accuracy
- Green bars: Gravity-removed accuracy
- Show improvement magnitude

**Plot 4: Drift PSI Heatmap (Channels x Experiments)**
- Rows: Ax, Ay, Az, Gx, Gy, Gz
- Columns: Baseline, Gravity_Removed
- Color: PSI value (red=high drift, green=low drift)
- Highlight: Az drift drops from 0.45 to 0.08

**Plot 5: Confusion Matrix Comparison**
- Side-by-side: Baseline vs Gravity_Removed
- Show: hand_tapping over-prediction decreases
- Show: Other classes start appearing

**Plot 6: Embedding Space t-SNE**
- 2D projection of last CNN layer activations
- Colors: Activity classes
- Shapes: Train (circles) vs Prod (triangles)
- Before removal: Train/prod clusters separate
- After removal: Train/prod clusters overlap (good!)

---

**Step 5: Statistical Significance Testing**

**Paired t-test:**
- Null hypothesis: Gravity removal has no effect on confidence
- Alternative: Gravity removal increases mean confidence
- Sample: 1000 random production windows
- Compute: t-statistic, p-value
- Decision: If p less than 0.05, reject null (removal helps)

**McNemar's Test (For Classification):**
- If labeled subset available (200-500 windows)
- Test: Does gravity removal change classification correctness?
- Contingency table: Baseline_correct x Gravity_correct
- Significant if p less than 0.05

**Effect Size (Cohen's d):**
- Measure practical significance, not just statistical
- d = (mean_confidence_after - mean_confidence_before) / pooled_std
- Interpretation: d greater than 0.8 = large effect

---

#### **5. Repo Actions**

**Action 1: Enable Gravity Removal in Config**

File: config/pipeline_config.yaml

Change line 15:
FROM: enable_gravity_removal: false
TO:   enable_gravity_removal: true

Rationale: Production Az=-9.8, Training Az=-3.4, mismatch too large to ignore

**Action 2: Create Ablation Experiment Script**

File: scripts/ablation_gravity_removal.py (NEW)

Sections:
1. Config loader (toggle gravity removal)
2. Pipeline runner (preprocess, inference, monitoring)
3. Metrics collector (accuracy, confidence, drift, embedding)
4. MLflow logger (track experiments)
5. Comparison plotter (6 visualizations)
6. Statistical tester (t-test, McNemar, Cohen's d)

**Action 3: Document Decision in Thesis**

File: docs/ablation_gravity_removal_results.md (NEW)

Sections:
1. Hypothesis: Gravity removal improves accuracy by reducing orientation shift
2. Experiment design: 2x2 factorial (train x prod gravity status)
3. Results table: Metrics comparison (accuracy, confidence, drift)
4. Plots: 6 visualizations showing improvement
5. Statistical tests: Significance and effect size
6. Conclusion: Recommend gravity removal = TRUE for this dataset

**Action 4: Update Inference Pipeline**

File: src/run_inference.py

Verify: Already loads config/pipeline_config.yaml
Check: Gravity removal applied BEFORE normalization
Order: Unit conversion, Gravity removal, Z-score normalization (correct order)

**Action 5: Add Ablation Test to CI/CD**

File: .github/workflows/model_validation.yml (if exists) OR scripts/validate_model.py

Test case:
- Run inference with gravity ON and OFF
- Assert: Mean confidence with gravity ON greater than baseline by at least 0.15
- Assert: Az drift PSI with gravity ON less than 0.15 (from 0.45 baseline)
- Fail build if gravity removal HARMS metrics (unexpected)

---

**CITATIONS (Pair 12, Q2):**

1. **(notebooks/exploration/gravity_removal_demo.ipynb, lines 490-540):** Before/after Az statistics: "Mean Az: -8.39 m/s squared (before) to 0.13 m/s squared (after). Change: 8.52 m/s squared shift toward training distribution. Success: Gravity component removed."

2. **(docs/thesis/HANDEDNESS_WRIST_PLACEMENT_ANALYSIS.md, lines 57-79):** Cross-position heterogeneity evidence: "Variability when device is on different body parts (e.g., wrist vs. pocket). Left vs. right wrist sensors have mirrored coordinate frames. Same gesture produces different sensor signatures depending on which wrist."

3. **(config/pipeline_config.yaml, lines 15-23):** Configurable gravity removal toggle: "enable_gravity_removal: false (current default). gravity_filter: cutoff_hz: 0.3, order: 3. Recommendation: Apply gravity removal in preprocess_data.py after conversion."

4. **(notebooks/exploration/gravity_removal_demo.ipynb, lines 2326-2360):** Multi-source comparison showing necessity: "Production Az=-9.83 (PRESENT), Training Az=-3.42 (REMOVED). Mismatch: 6.4 m/s squared (65 percent of gravity). Solution: Remove gravity to normalize across orientations."

5. **(docs/technical/root_cause_low_accuracy.md, lines 138-145):** Domain shift quantification: "Az mean in production: -9.825 m/s squared. Confirms data from watch lying flat on table (not on wrist). Training data: varied orientations. Result: Model sees constant offset, biased predictions (100 percent hand_tapping)."

6. **(src/sensor_data_pipeline.py, lines 724-740):** Implementation with toggle: "GravityRemovalPreprocessor with enable_gravity_removal parameter. Theory: Gravity is constant DC component. Uses Butterworth high-pass filter following UCI HAR dataset (Anguita et al., 2013). Cutoff at 0.3 Hz."

---

**Summary Table (Q2):**

| **Aspect** | **Details** | **Recommendation** |
|------------|-------------|---------------------|
| **Current Problem** | Production Az=-9.8, Training Az=-3.4, 6.4 m/s squared mismatch | Remove gravity (HIGH priority) |
| **When Removal Helps** | Orientation mismatch, cross-position HAR, multi-user studies | All apply to this repo |
| **When Removal Harms** | Orientation-consistent data, fall detection, posture estimation | None apply to this repo |
| **Necessity Level** | Practically essential (mismatch 2x larger than typical motion) | Highly recommended |
| **Expected Accuracy Gain** | +15-25 percentage points (from 14-15 percent to 50-60 percent) | Significant improvement |
| **Expected Confidence Gain** | +0.2-0.4 softmax score (from 0.45 to 0.65-0.85) | Large effect |
| **Expected Drift Reduction** | PSI drops from 0.45 to less than 0.15 on Az channel | Stabilizes monitoring |
| **Implementation** | Set enable_gravity_removal: true in pipeline_config.yaml | 2-minute change |
| **Ablation Design** | 2x2 factorial: Train/Prod gravity status | 4 experiments |
| **Evaluation Metrics** | Accuracy, confidence, drift PSI, embedding MMD | 4 categories |
| **Comparison Plots** | Az distribution, confidence histogram, accuracy bars, drift heatmap, confusion matrix, t-SNE | 6 visualizations |
| **Statistical Tests** | Paired t-test, McNemar test, Cohen's d effect size | Validate significance |
| **Repo Actions** | Enable in config, create ablation script, document results, update CI/CD | 4 files modified, 2 new |
| **Standard Practice** | UCI HAR dataset removes gravity (Anguita 2013) | Industry standard |
| **Toggle Rationale** | Enables A/B testing, empirical validation, future retraining flexibility | MLOps best practice |

---




## Pair 13 - Drift Detection Without Labels

**Context:** Unlabeled production data requires label-free drift detection. Drift types: covariate shift (P(X) changes), prior/label shift (P(Y) changes), concept drift (P(Y|X) changes), prediction drift (model output changes), confidence drift (uncertainty changes).

---

### **Q1: Which drift types are detectable without labels, and what are the best metrics/tests per type?**

**Decision:** Of the five drift types, **THREE are fully detectable without labels** (covariate, prediction, confidence), **ONE is partially detectable** (prior shift via predicted class histogram), and **ONE requires labels** (concept drift). Recommended metrics: **(1) Covariate drift**: KS test + Wasserstein + PSI on raw features, **(2) Prediction drift**: Chi-square test on predicted class histogram + temporal flip rate, **(3) Confidence drift**: mean/std/entropy shift + uncertain ratio spike, **(4) Embedding drift** (optional advanced): MMD + Mahalanobis distance in latent space. Use **multi-metric consensus** (2+ metrics agree) to reduce false positives.

**Detailed Explanation:**

#### **1. Drift Type Taxonomy (Label-Free Detectability)**

**Classification Table:**

| Drift Type | Definition | Detectable Without Labels? | Detection Method | False Positive Risk |
|------------|------------|---------------------------|------------------|---------------------|
| **Covariate Drift** | P(X) changes (input distribution shifts) |  **YES (High Confidence)** | KS test, Wasserstein, PSI on raw features (Ax, Ay, Az, Gx, Gy, Gz) | Medium (sensor noise can trigger) |
| **Prior/Label Shift** | P(Y) changes (true class distribution shifts) |  **INDIRECT (Low Confidence)** | Predicted class histogram (proxy, not ground truth) | High (prediction errors compound) |
| **Concept Drift** | P(Y\|X) changes (input-output relationship shifts) |  **NO (Requires Labels)** | Accuracy drop over time (needs labeled samples) | N/A (not applicable) |
| **Prediction Drift** | Model output distribution changes |  **YES (High Confidence)** | Predicted class histogram + temporal patterns | Low (direct observation) |
| **Confidence Drift** | Uncertainty distribution changes |  **YES (High Confidence)** | Mean confidence shift, uncertain ratio spike | Low (direct observation) |
| **Embedding Drift** | Latent representation shifts |  **YES (Medium Confidence)** | MMD, cosine similarity in embedding space | Medium (requires baseline) |

**Key Insight from Repo:**  
From docs/PIPELINE_DEEP_DIVE_opus.md lines 608-616:
"**Covariate Drift**: P(X) changes (input distribution)   YES  KS-test, PSI on features  
**Concept Drift**: P(Y|X) changes (relationship)   NO (needs labels)  Monitor labeled samples  
**Prediction Drift**: Model output distribution changes   YES  Prediction histogram  
**Confidence Drift**: Confidence distribution changes   YES  Mean/std of confidence"

---

#### **2. Covariate Drift (Input Feature Distribution)**

**What It Detects:** Changes in sensor data distribution (e.g., new device, different wearing position, environmental factors).

**Detection Methods (3-Metric Approach):**

**A. Kolmogorov-Smirnov (KS) Test**

**Purpose:** Non-parametric two-sample test for distribution difference  
**Null Hypothesis:** Production and training distributions are identical  
**Test Statistic:** Maximum vertical distance between CDFs  
**Decision Rule:** Reject null if p-value less than threshold (0.01 with Bonferroni correction for 6 channels)

**Repo Implementation:**  
From scripts/post_inference_monitoring.py lines 619-777:
class DriftDetector with analyze method:
- Computes KS statistic and p-value using scipy.stats.ks_2samp
- Baseline: Training data stored in baseline_stats.json
- Production: Current batch windows
- Per-channel analysis (Ax, Ay, Az, Gx, Gy, Gz)
- Bonferroni correction: threshold/6 = 0.01/6 = 0.00167 if use_bonferroni=True

**Strengths:**
- Distribution-free (no assumptions about data shape)
- Sensitive to location and shape changes
- Well-established statistical theory

**Weaknesses:**
- Tiny p-values at scale (10,000+ samples) even for negligible drift
- Sensitive to sample size (large N makes everything significant)
- Does not quantify drift magnitude

**B. Wasserstein Distance (Earth Mover's Distance)**

**Purpose:** Effect size measure (quantifies "how far" to move one distribution to match another)  
**Interpretation:** Higher values = more work to transform distributions  
**Decision Rule:** Flag drift if Wasserstein greater than 0.5 (repo standard)

**Advantages Over KS:**
- Provides magnitude (not just significance)
- Less sensitive to sample size
- Interpretable units (same as feature units)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 776-793:
"Use Wasserstein distance (preferred over p-value) because KS p-values become tiny at scale even for minor changes. Effect sizes (Wasserstein, PSI) are more interpretable."

**C. Population Stability Index (PSI)**

**Purpose:** Industry-standard drift metric from credit scoring, quantifies distribution shift via histogram comparison  
**Formula:** PSI = sum((prod_pct - baseline_pct) times log(prod_pct / baseline_pct))

**Interpretation Thresholds:**
- PSI less than 0.10: No significant change (PASS)
- PSI 0.10-0.25: Moderate change (WARN, investigate)
- PSI greater than 0.25: Major change (BLOCK, likely drift)

**Repo Implementation:**  
From scripts/post_inference_monitoring.py lines 884-930:
_compute_psi method:
- Bins production data using baseline histogram edges
- Computes percentage difference per bin
- Logarithmic weighting emphasizes tail behavior
- Works with histograms (no need to store raw samples)

**Why PSI Is Preferred:**
- Symmetric (direction-agnostic)
- Interpretable thresholds (industry consensus)
- Histogram-based (efficient for large datasets)
- Captures both location and shape changes

---

**Multi-Metric Consensus Strategy:**

From scripts/post_inference_monitoring.py lines 777-793:
"Drift detected if: (psi_value greater than 0.25) OR (ks_pvalue less than 0.01 AND wasserstein greater than 0.5 AND normalized_shift greater than 0.5)"

**Logic:** Require 2+ metrics to agree (reduces false positives from sensor noise or transient anomalies)

---

#### **3. Prediction Drift (Model Output Changes)**

**What It Detects:** Changes in predicted class distribution (e.g., model suddenly predicts 90 percent "hand_tapping" when baseline was 15 percent).

**Detection Methods:**

**A. Predicted Class Histogram**

**Purpose:** Compare current vs baseline activity distribution  
**Metrics:** Chi-square test for categorical distribution difference

**Repo Implementation:**  
From src/run_inference.py line 621:
metadata["activity_distribution"] = predictions.value_counts().to_dict()

**Example:**
Baseline (Training): hand_tapping: 15 percent, sitting: 40 percent, standing: 30 percent, walking: 15 percent
Production: hand_tapping: 92 percent, sitting: 5 percent, standing: 2 percent, walking: 1 percent
Chi-square p-value: less than 0.001  **Prediction drift detected**

**Scientific Justification:**  
From docs/research/KEEP_Research_QA_From_Papers.md lines 140-148:
"Two-Sample Tests: Kolmogorov-Smirnov test, Maximum Mean Discrepancy, Jensen-Shannon divergence compare production data against training distribution. If distance exceeds threshold, drift is flagged."

**B. Temporal Flip Rate**

**Purpose:** Measure prediction instability (rapid class switching indicates model confusion)  
**Formula:** flip_rate = (number of transitions) / (number of consecutive pairs)

**Repo Implementation:**  
From scripts/post_inference_monitoring.py lines 500-600:
class TemporalPlausibilityAnalyzer:
- Counts activity transitions in predicted sequence
- Flags unstable if flip_rate greater than 0.30 (30 percent of windows differ from previous)
- Accounts for window overlap (windows are not independent)

**Interpretation:**
- Normal: flip_rate less than 0.15 (activities persist 3+ windows = 6+ seconds)
- Moderate: flip_rate 0.15-0.30 (short activity bouts)
- High: flip_rate greater than 0.30 (model flipping every 1-2 windows = unstable)

**When It Indicates Drift:**
- Covariate drift  Model uncertain  Frequent class switching
- Out-of-distribution data  Low confidence  Random-like predictions

---

#### **4. Confidence Drift (Uncertainty Changes)**

**What It Detects:** Changes in model's predictive uncertainty (e.g., mean confidence drops from 0.75 to 0.45, indicating OOD data).

**Detection Methods (3 Metrics):**

**A. Mean Confidence Shift**

**Purpose:** Track average softmax probability over time  
**Decision Rule:** Flag drift if abs(prod_mean - baseline_mean) greater than 0.15

**Repo Implementation:**  
From scripts/post_inference_monitoring.py lines 333-360:
class ConfidenceAnalyzer computes:
- mean_confidence: Average max softmax probability
- std_confidence: Variability in confidence
- Thresholds: uncertain if confidence less than 0.50

**B. Uncertain Ratio Spike**

**Purpose:** Percentage of low-confidence predictions  
**Formula:** uncertain_ratio = (windows with confidence less than 0.50) / total_windows  
**Decision Rule:** WARN if uncertain_ratio greater than 0.30

**Repo Threshold:**  
From scripts/post_inference_monitoring.py line 142:
"max_uncertain_ratio: float = 0.30 (Above this = WARN)"

**C. Entropy Drift**

**Purpose:** Measure prediction uncertainty using Shannon entropy  
**Formula:** H(p) = -sum(p_i times log(p_i)) for class probabilities  
**Interpretation:** Higher entropy = more uncertain (uniform distribution)

**Repo Implementation:**  
From scripts/post_inference_monitoring.py lines 348-354:
Entropy computed as: -np.sum(probs times np.log(probs + 1e-10), axis=1)

**Confidence vs Entropy Relationship:**
- High confidence (0.9) + Low entropy (0.3): Model certain (good)
- Low confidence (0.4) + High entropy (2.1): Model uncertain (bad, likely drift)
- High confidence (0.95) + High entropy (N/A): Impossible (confidence = max(probs))

---

#### **5. Prior/Label Shift (Class Distribution)**

**What It Detects:** Changes in true class proportions (e.g., dataset shifts from 60 percent sedentary to 80 percent active).

**Why Partially Detectable:**
- WITHOUT labels: Can only observe **predicted** class histogram (not true distribution)
- Problem: If model is biased (e.g., over-predicts "hand_tapping"), predicted histogram does NOT reflect true shift
- Confidence: LOW (prediction errors compound the drift signal)

**When It Works:**
- If model accuracy is high (greater than 85 percent), predicted histogram approximately equals true histogram
- If covariate drift is absent (no input distribution shift confounding predictions)

**Detection Method:**  
Same as Prediction Drift (Chi-square test on predicted class histogram)

**Repo Evidence:**  
From docs/PIPELINE_DIVE.md line 344:
"Prior drift: predicted class distribution shifts (track predicted activity histogram)"

---

#### **6. Concept Drift (Input-Output Relationship)**

**What It Detects:** Changes in relationship between inputs and outputs (e.g., "hand_tapping" accelerometer signature changes over time).

**Why NOT Detectable Without Labels:**
- Requires ground truth labels to measure P(Y|X)
- Example: If X = same sensor pattern but Y changes from "typing" to "scratching", need labels to detect
- Confidence drift is a WEAK proxy (model might still be confident but wrong)

**Repo Evidence:**  
From docs/PIPELINE_DIVE.md line 345:
"Concept drift: true label relationship changes; not directly observable without labels"

**Mitigation Without Labels:**
- Collect periodic labeled samples (50-200 windows per month)
- Use embedding drift as early warning (latent space shifts may precede concept drift)
- Monitor confidence + covariate drift together (combined signals suggest concept drift)

---

#### **7. Embedding Drift (Optional Advanced)**

**What It Detects:** Changes in learned feature representations (latent space shift indicates semantic drift).

**Why It's Powerful:**
- Captures high-level patterns (not just raw statistics)
- Can detect drift even when raw features look similar
- Early warning system (embeddings shift before accuracy drops)

**Detection Methods:**

**A. Maximum Mean Discrepancy (MMD)**

**Purpose:** Kernel-based distance between distributions in embedding space  
**Advantages:** Non-parametric, works in high dimensions  
**Decision Rule:** Flag drift if MMD greater than threshold (0.1 typical)

**B. Mahalanobis Distance**

**Purpose:** Distance accounting for covariance structure  
**Advantages:** Handles correlated features  
**Decision Rule:** Flag outliers if distance greater than 3 standard deviations

**Repo Implementation:**  
From scripts/post_inference_monitoring.py lines 956-1050:
class EmbeddingDriftDetector:
- Extracts embeddings from model's intermediate layer (bidirectional LSTM)
- Computes mean/covariance shift
- Calculates Mahalanobis distance
- Stores baseline embeddings in drift_baseline.json

**When to Use:**
- High-stakes applications (want early drift detection)
- Complex domain shift (sensor characteristics change subtly)
- Thesis contribution (demonstrates advanced monitoring)

**Limitations:**
- Requires storing baseline embeddings (memory overhead)
- Computationally expensive (model forward pass)
- Baseline must be representative (need diverse training data)

---

**CITATIONS (Pair 13, Q1):**

1. **(docs/PIPELINE_DEEP_DIVE_opus.md, lines 608-616):** Drift type taxonomy table: "Covariate Drift: P(X) changes, detectable YES via KS-test, PSI on features. Concept Drift: P(Y|X) changes, detectable NO (needs labels), monitor labeled samples. Prediction Drift: Model output distribution changes, detectable YES via prediction histogram. Confidence Drift: Confidence distribution changes, detectable YES via mean/std of confidence."

2. **(scripts/post_inference_monitoring.py, lines 619-777):** DriftDetector class implementation: "Methods: KS test (Kolmogorov-Smirnov) non-parametric test for distribution difference, Wasserstein distance (Earth mover's distance) between distributions, Mean/std shift for simple statistics comparison, Variance collapse detection to identify idle or sensor failure. Per-channel analysis for Ax, Ay, Az, Gx, Gy, Gz with Bonferroni correction."

3. **(scripts/post_inference_monitoring.py, lines 884-930):** PSI computation method: "Population Stability Index (PSI) widely used in credit scoring and model monitoring. PSI less than 0.10: No significant change. PSI 0.10-0.25: Moderate change, investigate. PSI greater than 0.25: Significant change, likely need action. Works with histograms, no need to store raw samples. Symmetric and interpretable."

4. **(docs/research/KEEP_Research_QA_From_Papers.md, lines 140-148):** Drift detection without labels using statistical tests: "Two-Sample Tests: Kolmogorov-Smirnov test, Maximum Mean Discrepancy, Jensen-Shannon divergence compare production data against training distribution. Model Uncertainty/Confidence: Monitoring predictive uncertainty (Shannon Entropy) as proxy for out-of-distribution data or drift. Feature Distribution Monitoring: Descriptive statistics (mean, variance) over time to detect deviations from baseline."

5. **(scripts/post_inference_monitoring.py, lines 333-360, 500-600):** Confidence and temporal analyzers: "ConfidenceAnalyzer computes mean_confidence, std_confidence with uncertain threshold 0.50. TemporalPlausibilityAnalyzer counts activity transitions, flags unstable if flip_rate greater than 0.30 accounting for window overlap. Uncertain ratio spike: WARN if ratio greater than 0.30."

6. **(scripts/post_inference_monitoring.py, lines 956-1050):** Embedding drift detector implementation: "EmbeddingDriftDetector extracts embeddings from model intermediate layer (bidirectional LSTM), computes mean/covariance shift, calculates Mahalanobis distance. Methods: Maximum Mean Discrepancy, cosine similarity shift, Wasserstein distance in embedding space. Stores baseline in drift_baseline.json. One of strongest label-free evaluation arguments for deep HAR."

---

**Summary Table (Q1):**

| **Drift Type** | **Detectable?** | **Primary Metrics** | **Secondary Metrics** | **False Positive Risk** | **Repo Implementation** |
|----------------|-----------------|---------------------|----------------------|-------------------------|-------------------------|
| **Covariate** |  YES (High) | KS p-value less than 0.01, PSI greater than 0.25 | Wasserstein greater than 0.5, mean shift greater than 0.5 std | Medium | DriftDetector (lines 619-777) |
| **Prediction** |  YES (High) | Predicted class histogram Chi-square | Flip rate greater than 0.30 | Low | TemporalPlausibilityAnalyzer (lines 500-600) |
| **Confidence** |  YES (High) | Mean confidence shift greater than 0.15 | Uncertain ratio greater than 0.30, entropy shift | Low | ConfidenceAnalyzer (lines 333-360) |
| **Prior/Label** |  INDIRECT (Low) | Predicted histogram (proxy only) | Activity distribution change | High | activity_distribution in metadata |
| **Concept** |  NO | Requires labeled samples | Embedding drift (weak proxy) | N/A | Periodic labeling needed |
| **Embedding** |  YES (Medium) | MMD in latent space, Mahalanobis distance | Cosine similarity shift | Medium | EmbeddingDriftDetector (lines 956-1050) |

---




### **Q2: Propose minimal monitoring set, thresholds, alert logic, and implementation modules with outputs.**

**Decision:** Implement **3-layer gated monitoring** with 8 core metrics: **(Layer 1)** mean_confidence, uncertain_ratio, entropy; **(Layer 2)** flip_rate, mean_dwell_time; **(Layer 3)** n_drift_channels, psi_max, wasserstein_max. Use **AND-gating between layers** (all must PASS) + **k-channel threshold** (drift in greater than 2 of 6 channels triggers BLOCK). Alert logic: **PASS** (all green), **WARN** (investigate, continue pipeline), **BLOCK** (halt, manual review). Implementation: Consolidate in **scripts/post_inference_monitoring.py** (exists, lines 1-1590), outputs to **reports/monitoring/timestamp/** (4 JSON files), log to **MLflow** for dashboard tracking.

**Detailed Explanation:**

#### **1. Minimal Monitoring Set (8 Core Metrics)**

**Selection Criteria:**
- **Actionability:** Each metric directly informs decision (retrain vs recalibrate vs no action)
- **Independence:** Metrics capture different failure modes (confidence = uncertainty, drift = distribution, temporal = plausibility)
- **Computational Efficiency:** All computable from predictions + raw features (no expensive re-inference)
- **Scientific Defensibility:** Established in MLOps literature (not ad-hoc thresholds)

**Rationale for Minimal Set:**  
From docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md lines 260-308:
"Three-Layer Architecture: Layer 1 (Per-Window), Layer 2 (Temporal), Layer 3 (Batch Drift). Gating Logic: All PASS  Continue pipeline. Any FAIL  Investigate or BLOCK."

---

**Layer 1: Per-Window Confidence Metrics (3 Metrics)**

| Metric | Formula | Threshold | What It Detects | Why Essential |
|--------|---------|-----------|-----------------|---------------|
| **mean_confidence** | Mean of max softmax probabilities | greater than 0.60 = PASS, less than 0.50 = WARN | Overall model certainty | Low confidence = OOD or uncertain inputs |
| **uncertain_ratio** | Percentage of windows with confidence less than 0.50 | less than 0.30 = PASS, greater than 0.30 = WARN | Fraction of uncertain predictions | Spike indicates batch-level issue |
| **mean_entropy** | Mean Shannon entropy H(p) = -sum(p log p) | less than 1.5 = PASS, greater than 2.0 = WARN | Prediction ambiguity | High entropy = model confused (uniform probs) |

**Computation:**  
From predictions.csv (already generated by run_inference.py):
- Confidence: max(probs) per window
- Entropy: -sum(probs times log(probs)) per window

**Why These 3?**
- Mean confidence: Primary uncertainty indicator (direct, interpretable)
- Uncertain ratio: Detects batch-level problems (e.g., 80 percent of batch is uncertain)
- Entropy: Complementary to confidence (catches cases where top-2 classes have similar probs)

**Alternative Metrics (Excluded):**
- Margin (confidence_1 - confidence_2): Redundant with entropy
- Variance of confidence: Noisy, less actionable
- Per-class confidence: Too granular (6 classes = 6 metrics)

---

**Layer 2: Temporal Plausibility Metrics (2 Metrics)**

| Metric | Formula | Threshold | What It Detects | Why Essential |
|--------|---------|-----------|-----------------|---------------|
| **flip_rate** | (Number of transitions) / (Number of consecutive pairs) | less than 0.30 = PASS, greater than 0.30 = WARN | Prediction instability | Rapid class switching = model confusion |
| **mean_dwell_time** | Average duration per activity bout | greater than 2.0 seconds = PASS | Activity persistence | Too-short bouts = implausible (human activities persist 3+ sec) |

**Computation:**  
From predicted sequence:
1. Count transitions: [hand_tapping, sitting, sitting, hand_tapping]  2 transitions in 3 pairs  flip_rate = 0.67
2. Measure dwell time: sitting bout lasted 2 windows times 2 sec/window = 4 sec  plausible

**Why These 2?**
- Flip rate: Captures instability (model flipping = likely drift or low confidence)
- Dwell time: Human activities have physical constraints (cannot switch faster than 2 seconds)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 500-600:
"TemporalPlausibilityAnalyzer: Counts activity transitions, accounts for window overlap (windows advance by 2 sec with 50 percent overlap, not 4 sec). Flags unstable if flip_rate greater than 0.30."

**Alternative Metrics (Excluded):**
- Transition matrix entropy: Complex, hard to interpret
- Longest streak: Outlier-sensitive
- Activity distribution: Covered by Layer 3 (prediction drift)

---

**Layer 3: Batch Drift Metrics (3 Metrics)**

| Metric | Formula | Threshold | What It Detects | Why Essential |
|--------|---------|-----------|-----------------|---------------|
| **n_drift_channels** | Count of channels with KS p-value less than 0.01 OR PSI greater than 0.25 | less than or equal to 2 = PASS, greater than 2 = BLOCK | Multi-channel drift | Isolated drift (1-2 channels) = noise, widespread (3+ channels) = systemic shift |
| **psi_max** | Maximum PSI across all 6 channels | less than 0.10 = PASS, 0.10-0.25 = WARN, greater than 0.25 = BLOCK | Worst-case drift magnitude | Single metric to check if ANY channel drifted significantly |
| **wasserstein_max** | Maximum Wasserstein distance across all 6 channels | less than 0.5 = PASS, greater than 0.5 = WARN | Drift effect size | Quantifies "how far" distributions shifted (complements p-value) |

**Computation:**  
From production_data.npy (shape: n_windows times 200 times 6) and baseline_stats.json:
- Per-channel KS test: scipy.stats.ks_2samp(prod_channel, baseline_channel)
- Per-channel PSI: histogram comparison using _compute_psi method
- Per-channel Wasserstein: scipy.stats.wasserstein_distance

**Why These 3?**
- n_drift_channels: Implements k-channel gating (reduces false positives from multiple comparisons)
- psi_max: Industry-standard metric (credit scoring, widely accepted thresholds)
- wasserstein_max: Effect size (less sensitive to sample size than KS p-value)

**k-Channel Gating Rationale:**  
From scripts/post_inference_monitoring.py lines 81-87:
"Note on Multiple Comparisons: Testing 6 channels independently inflates false positives. Options: (1) Bonferroni correction, (2) k-channel gating (default). We use k-channel gating: drift only flagged if greater than or equal to max_drift_channels channels drift."

**Alternative Metrics (Excluded):**
- Mean shift per channel: Noisy, less standardized than PSI/Wasserstein
- Variance collapse: Already captured in DriftDetector (special case)
- Embedding MMD: Optional (Layer 4), not minimal set

---

#### **2. Threshold Strategy (Scientifically Defensible)**

**Confidence Thresholds (0.50 / 0.60):**

**Source:** Standard softmax interpretation
- Confidence less than 0.50 = model prefers "uncertain" over any single class
- Confidence greater than 0.60 = reasonably confident (60 percent probability for top class)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py line 100:
"confidence_threshold: float = 0.50 (Below this = uncertain)"

**Justification:**  
From papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf p.202-210:
"Automated alerting on threshold violations (e.g., greater than 30 percent uncertain predictions)"  uncertain_ratio greater than 0.30 is industry practice

---

**Flip Rate Threshold (0.30):**

**Source:** Empirical HAR analysis
- Human activities persist 3+ windows (6+ seconds with 50 percent overlap)
- Flip rate greater than 0.30 means 30 percent of transitions = activity switches every 3-4 windows
- Implausible for physical activities (cannot alternate sitting/running every 6 seconds)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py line 111:
"max_flip_rate: float = 0.30 (Above this = unstable)"

---

**PSI Thresholds (0.10 / 0.25):**

**Source:** Credit scoring industry standard (established over decades)
- PSI less than 0.10: No population shift (distributions similar)
- PSI 0.10-0.25: Moderate shift (investigate causes, may be acceptable)
- PSI greater than 0.25: Major shift (action required, model may fail)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 82-84:
"PSI Interpretation (standard thresholds):  
PSI less than 0.10  No/low shift (PASS)  
0.10-0.25  Moderate shift (WARN, investigate)  
PSI greater than 0.25  Major shift (likely drift, action needed)"

**Scientific Justification:**  
From docs/PIPELINE_DEEP_DIVE_opus.md lines 628-679:
"PSI less than 0.1: No significant shift. PSI 0.1-0.2: Warning, investigate. PSI greater than or equal to 0.2: Retrain."

---

**KS p-value Threshold (0.01 with optional Bonferroni):**

**Source:** Statistical significance with multiple testing correction
- Base threshold: 0.01 (stricter than typical 0.05 to reduce false positives)
- Bonferroni correction: 0.01/6 = 0.00167 (if testing 6 channels independently)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 127-129:
"Note: For 6 channels with Bonferroni, use ks_pvalue_threshold/6 = 0.00167  
ks_pvalue_threshold: float = 0.01 (Base p-value before Bonferroni)  
use_bonferroni: bool = False (If True, apply Bonferroni correction)"

**Why NOT Use Bonferroni by Default:**  
Too conservative (misses real drift). Instead, use k-channel gating (drift must occur in greater than 2 channels to trigger BLOCK).

---

**k-Channel Threshold (2 out of 6):**

**Source:** Practical multiple comparisons strategy
- 1 channel drifting: Likely sensor noise or transient anomaly
- 2 channels drifting: Borderline (continue with WARN)
- 3+ channels drifting: Systemic issue (BLOCK and investigate)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py line 142:
"max_drift_channels: int = 2 (Above this = BLOCK)"

**Justification:**  
33 percent threshold (2/6 = 0.33) balances false positives (too sensitive) vs false negatives (miss real drift).

---

#### **3. Alert Logic (3-Level Gating)**

**Gating Strategy:**

**Level 1: PASS (Green Light)**

Criteria (ALL must be true):
- mean_confidence greater than 0.60
- uncertain_ratio less than 0.30
- mean_entropy less than 1.5
- flip_rate less than 0.30
- mean_dwell_time greater than 2.0 seconds
- n_drift_channels less than or equal to 2
- psi_max less than 0.10

Action: Continue pipeline, no human intervention
Log: INFO level, store metrics in reports/monitoring/timestamp/summary.json

---

**Level 2: WARN (Yellow Light)**

Criteria (ANY is true):
- 0.50 less than mean_confidence less than 0.60 (borderline uncertainty)
- 0.20 less than uncertain_ratio less than 0.30 (elevated uncertainty)
- 1.5 less than mean_entropy less than 2.0 (moderate ambiguity)
- 0.25 less than flip_rate less than 0.30 (borderline instability)
- 0.10 less than psi_max less than 0.25 (moderate drift)

Action: Continue pipeline BUT investigate causes, log detailed metrics
Alert: Email/Slack notification (optional), MLflow WARNING tag
Log: WARNING level, store full reports (confidence, temporal, drift JSONs)

**Repo Evidence:**  
From docs/PIPELINE_DIVE.md lines 356-358:
"WARN if PSI greater than 0.10 on any channel OR KS p-value below threshold on greater than or equal to 1 channel. WARN if uncertain ratio greater than 0.30 (confidence drift)."

---

**Level 3: BLOCK (Red Light)**

Criteria (ANY is true):
- mean_confidence less than 0.50 (majority uncertain)
- uncertain_ratio greater than 0.30 (30+ percent uncertain)
- flip_rate greater than 0.30 (unstable predictions)
- n_drift_channels greater than 2 (widespread drift)
- psi_max greater than 0.25 (major drift)
- Variance collapse detected (std less than 0.1 on any channel = sensor failure)

Action: HALT pipeline, require manual review before proceeding
Alert: Immediate email/Slack/PagerDuty notification
Log: ERROR level, store all reports + raw data snapshot
Recommendation: Inspect data quality, consider retraining or recalibration

**Repo Evidence:**  
From docs/PIPELINE_DIVE.md lines 356-361:
"BLOCK/retrain if PSI greater than 0.25 on any channel OR drift in greater than 2 channels OR variance collapse. Multiple-comparisons note: k-channel gating (drift only flagged if greater than or equal to max_drift_channels channels drift)."

---

**Combined Logic (Pseudo-code):**

status = PASS
message = "All checks passed"

IF (mean_confidence less than 0.50 OR uncertain_ratio greater than 0.30 OR flip_rate greater than 0.30 OR n_drift_channels greater than 2 OR psi_max greater than 0.25):
    status = BLOCK
    message = "Critical issue detected: [list violations]"
ELIF (0.50 less than mean_confidence less than 0.60 OR 0.20 less than uncertain_ratio less than 0.30 OR 0.10 less than psi_max less than 0.25):
    status = WARN
    message = "Borderline metrics detected: [list warnings]"
ELSE:
    status = PASS
    message = "All metrics within normal ranges"

---

#### **4. Implementation Modules + Outputs**

**Module 1: scripts/post_inference_monitoring.py (EXISTS, 1590 lines)**

**Structure:**
- MonitoringConfig class (lines 66-176): All thresholds as dataclass
- ConfidenceAnalyzer class (lines 260-400): Layer 1 metrics
- TemporalPlausibilityAnalyzer class (lines 400-600): Layer 2 metrics
- DriftDetector class (lines 619-900): Layer 3 metrics
- EmbeddingDriftDetector class (lines 956-1050): Optional Layer 4
- main function (lines 1200-1590): Orchestrates all layers + MLflow logging

**Usage:**
python scripts/post_inference_monitoring.py --predictions data/prepared/predictions/latest.csv --baseline data/prepared/baseline_stats.json --mlflow

**Inputs:**
- predictions.csv: Window-level predictions from run_inference.py
- production_data.npy: Raw sensor windows (n_windows times 200 times 6)
- baseline_stats.json: Training distribution from build_training_baseline.py

**Outputs (4 JSON files):**

**A. confidence_report.json**

Location: reports/monitoring/timestamp/confidence_report.json

Content:
- metrics: mean_confidence, std_confidence, min_confidence, max_confidence
- distribution: percentiles (5th, 25th, 50th, 75th, 95th)
- uncertain_ratio: Percentage with confidence less than 0.50
- entropy_stats: mean_entropy, std_entropy
- warnings: List of violations (if any)
- summary: status (PASS/WARN/BLOCK), message

**B. temporal_report.json**

Location: reports/monitoring/timestamp/temporal_report.json

Content:
- metrics: flip_rate, mean_dwell_time, n_transitions
- activity_distribution: Predicted class histogram
- transition_matrix: Class-to-class transition counts
- longest_streak: Maximum consecutive windows per class
- warnings: Implausible transitions (e.g., sitting  running in 2 sec)
- summary: status (PASS/WARN/BLOCK), message

**C. drift_report.json**

Location: reports/monitoring/timestamp/drift_report.json

Content:
- timestamp: ISO 8601 timestamp
- production_shape: [n_windows, 200, 6]
- metrics: n_channels_with_drift, drift_channels list, psi_max, wasserstein_max
- per_channel: For each Ax, Ay, Az, Gx, Gy, Gz:
  - mean, std, min, max, percentile_5, percentile_95
  - baseline_mean, baseline_std
  - mean_shift, normalized_mean_shift
  - ks_statistic, ks_pvalue
  - wasserstein_distance
  - psi (if histogram available)
  - drift_detected: Boolean
- warnings: List of drifted channels with severity
- summary: status (PASS/WARN/BLOCK), message

**D. summary.json (MASTER REPORT)**

Location: reports/monitoring/timestamp/summary.json

Content:
- timestamp: ISO 8601
- overall_status: PASS/WARN/BLOCK (worst of 3 layers)
- layer1_confidence: Nested confidence_report metrics
- layer2_temporal: Nested temporal_report metrics
- layer3_drift: Nested drift_report metrics
- layer4_embedding: Optional embedding_report metrics
- alerts: Combined warnings from all layers
- recommendations: Action items (e.g., "Retrain model", "Investigate Az drift")
- next_steps: Human-readable guidance

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 1-30:
"Outputs:  
- reports/monitoring/timestamp/confidence_report.json  
- reports/monitoring/timestamp/temporal_report.json  
- reports/monitoring/timestamp/drift_report.json  
- reports/monitoring/timestamp/summary.json  
- MLflow metrics and artifacts (if --mlflow flag)"

---

**Module 2: MLflow Logging (INTEGRATED)**

**Purpose:** Dashboard tracking + historical comparison

**Logged Metrics (per run):**
- monitoring/mean_confidence
- monitoring/uncertain_ratio
- monitoring/flip_rate
- monitoring/n_drift_channels
- monitoring/psi_max
- monitoring/overall_status (0=PASS, 1=WARN, 2=BLOCK)

**Logged Artifacts:**
- summary.json
- drift_heatmap.png (6 channels times PSI/Wasserstein)
- confidence_histogram.png
- temporal_flip_rate_plot.png

**Tagging:**
- tags.monitoring_status: PASS/WARN/BLOCK
- tags.drift_detected: True/False
- tags.dominant_wrist: True/False (if metadata available)

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 1352-1500:
MLflow logging implementation with metrics tracking and artifact uploads

---

**Module 3: Baseline Creation (scripts/build_training_baseline.py)**

**Purpose:** Generate baseline_stats.json from training data

**Required Once:** Before first monitoring run

**Inputs:**
- data/prepared/X_prepared_window.npy (training windows)
- data/prepared/y_prepared.npy (labels, optional for metadata)

**Outputs:**
- data/prepared/baseline_stats.json

Content:
- per_channel: mean, std, min, max, percentiles, histogram (bins + counts)
- samples: Optional stored samples (10,000 max) for KS test
- metadata: timestamp, n_samples, version

**Usage:**
python scripts/build_training_baseline.py

---

#### **5. Repo Actions**

**Action 1: Verify Monitoring Script**

File: scripts/post_inference_monitoring.py (ALREADY EXISTS)

Verification:
1. Check MonitoringConfig thresholds (lines 66-176): Confirm values match Q2 recommendations
2. Test run: python scripts/post_inference_monitoring.py --predictions latest.csv --baseline baseline_stats.json
3. Validate outputs: 4 JSON files in reports/monitoring/timestamp/
4. Check gating logic: summary.json status = PASS/WARN/BLOCK

---

**Action 2: Create Baseline (If Not Exists)**

File: data/prepared/baseline_stats.json

Command:
python scripts/build_training_baseline.py

Validation:
- File exists: baseline_stats.json
- Contains: per_channel stats for Ax, Ay, Az, Gx, Gy, Gz
- Histogram bins: 20-50 bins per channel
- Optional: Stored samples array (10,000 samples for KS test)

---

**Action 3: Add Alert Notification (OPTIONAL)**

File: scripts/utils/alert_sender.py (NEW)

Functionality:
- Email alerts for WARN/BLOCK status
- Slack webhook integration
- PagerDuty for BLOCK status

Usage:
python scripts/post_inference_monitoring.py --predictions latest.csv --baseline baseline_stats.json --alert-email your@email.com

---

**Action 4: MLflow Dashboard Configuration**

File: mlruns/ (MANAGED BY MLFLOW)

Setup:
1. Run monitoring with --mlflow flag
2. View dashboard: mlflow ui --port 5000
3. Create comparison view: Compare runs by monitoring/psi_max, monitoring/mean_confidence
4. Set up alerts: Email when monitoring/overall_status = 2 (BLOCK)

---

**Action 5: Document Thresholds**

File: docs/monitoring_thresholds.md (NEW)

Content:
- Table of all 8 metrics with thresholds
- Scientific justification for each threshold
- Examples of PASS/WARN/BLOCK scenarios
- Adjustment guidelines (when to relax/tighten thresholds)

Rationale: Thesis defense requires documenting why thresholds were chosen (not arbitrary).

---

**CITATIONS (Pair 13, Q2):**

1. **(scripts/post_inference_monitoring.py, lines 66-176):** MonitoringConfig class with all thresholds: "confidence_threshold: 0.50, uncertain_ratio: 0.30, max_flip_rate: 0.30, ks_pvalue_threshold: 0.01, psi_threshold: 0.25, max_drift_channels: 2. PSI Interpretation: less than 0.10 PASS, 0.10-0.25 WARN, greater than 0.25 BLOCK. k-channel gating: drift only flagged if greater than or equal to max_drift_channels."

2. **(docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md, lines 260-308):** Three-layer monitoring architecture: "Layer 1: Per-window confidence/entropy/margin. Layer 2: Temporal flip_rate/dwell_time. Layer 3: Batch drift KS test/Wasserstein/PSI. Gating Logic: All PASS  Continue pipeline. Any FAIL  Investigate or BLOCK."

3. **(docs/PIPELINE_DIVE.md, lines 356-361):** Alert thresholds and trigger rules: "WARN if PSI greater than 0.10 on any channel. BLOCK if PSI greater than 0.25 on any channel OR drift in greater than 2 channels OR variance collapse. WARN if uncertain ratio greater than 0.30 (confidence drift)."

4. **(scripts/post_inference_monitoring.py, lines 1200-1590):** Main orchestration function: "Orchestrates all layers: ConfidenceAnalyzer, TemporalPlausibilityAnalyzer, DriftDetector, optional EmbeddingDriftDetector. MLflow logging with metrics tracking and artifact uploads. Outputs: 4 JSON files (confidence, temporal, drift, summary)."

5. **(papers/mlops_production/Practical-mlops-operationalizing-machine-learning-models.pdf, p.202-210):** Monitoring dashboards and alerting: "Confidence histograms, prediction drift plots, class distribution comparisons to detect production issues. Automated alerting on threshold violations (e.g., greater than 30 percent uncertain predictions)." (Repository evidence: docs/output_1801_2026-01-18.md line 2037)

6. **(docs/PIPELINE_DEEP_DIVE_opus.md, lines 628-679):** PSI threshold justification: "PSI less than 0.1: No significant shift. PSI 0.1-0.2: Warning, investigate. PSI greater than or equal to 0.2: Retrain. Industry standard from credit scoring with decades of validation."

---

**Summary Table (Q2):**

| **Aspect** | **Specification** | **Rationale** |
|------------|-------------------|---------------|
| **Core Metrics** | 8 total: mean_confidence, uncertain_ratio, mean_entropy, flip_rate, mean_dwell_time, n_drift_channels, psi_max, wasserstein_max | Minimal set covering confidence, temporal, drift with no redundancy |
| **Threshold Source** | Industry standards (PSI from credit scoring, confidence from softmax interpretation, KS from statistics) | Scientifically defensible, not arbitrary |
| **Alert Levels** | PASS (all green), WARN (investigate), BLOCK (halt pipeline) | 3-level gating balances false positives vs false negatives |
| **k-Channel Gating** | Drift in greater than 2 of 6 channels triggers BLOCK | Reduces multiple comparisons problem (33 percent threshold) |
| **Implementation** | scripts/post_inference_monitoring.py (1590 lines, already exists) | Consolidated monitoring with all layers |
| **Outputs** | 4 JSON files: confidence, temporal, drift, summary | Separate reports enable granular debugging |
| **MLflow Integration** | Log 6 metrics + 3 artifacts per run | Historical tracking + dashboard comparison |
| **Baseline Requirement** | baseline_stats.json from training data | One-time setup, regenerate after retraining |
| **Alert Notification** | Email/Slack for WARN, PagerDuty for BLOCK (optional) | Human-in-loop for critical issues |
| **Documentation** | docs/monitoring_thresholds.md (to create) | Thesis-ready justification of threshold choices |

---




## Pair 14 — Retraining without labels

**Context:** Drift high  retrain/fine-tune on unlabeled production data. Accuracy cannot be computed without labels. Need safe strategies to retrain without overfitting to noisy pseudo-labels or catastrophic forgetting of training data.

---

### **Q1: What are safe strategies for retraining on unlabeled production data? Consider small labeled audit set, active learning, pseudo-labeling safeguards, semi-supervised/self-training.**

**Decision:** Implement **4-tier hybrid retraining strategy** with progressive safety checks: **(Tier 1)** Small labeled audit set (50-200 windows, stratified sampling), **(Tier 2)** Active learning on high-uncertainty samples (query human for top 20-50 uncertain predictions), **(Tier 3)** Confidence-filtered pseudo-labeling (only confident > 0.80 predictions added to training), **(Tier 4)** Mixed-data fine-tuning with **catastrophic forgetting mitigation** (30% original training + 70% new labeled/pseudo-labeled). **DO NOT** retrain on raw unlabeled data without validation. Each tier has quantitative gating criteria and MLflow checkpointing for rollback.

**Detailed Explanation:**

#### **1. Why Retraining Without Labels is High-Risk**

**The Core Problem:**  
From drift detection (Pair 13), we know **WHEN** to retrain (PSI > 0.25, drift in >2 channels, uncertain_ratio > 0.30), but unlabeled production data lacks ground truth  cannot compute accuracy  risk training on wrong predictions  **confirmation bias loop** (model reinforces its own mistakes).

**Specific Risks:**

| Risk | Description | Example | Mitigation |
|------|-------------|---------|------------|
| **Confirmation bias** | Model uses own predictions as training labels, amplifies existing errors | Model confuses sitting  hand_tapping. Pseudo-labels mislabel more sitting as hand_tapping  retraining worsens confusion | Confidence filtering (only use predictions > 0.80) |
| **Catastrophic forgetting** | New domain data overwrites old knowledge | Train on 100% Garmin data  model forgets original ADAMSense patterns  fails on multi-domain deployment | Mixed data (30% old + 70% new) |
| **Class imbalance** | Production activities differ from training (e.g., 80% sitting, 5% running) | Retrain on imbalanced data  model becomes sitting classifier | Stratified sampling, per-class balancing |
| **Noisy labels** | Uncertain predictions (confidence 0.40-0.60) used as training labels | Window with 40% sitting, 35% hand_tapping, 25% writing  pseudo-label = sitting  noisy signal | Reject low-confidence, use only > 0.80 |
| **Sensor drift** | Production data has sensor failures (variance collapse, bias drift) | Accelerometer stuck at Az = 0  model learns zero-signal patterns | Pre-validation: reject batches with variance collapse |

**Repo Evidence:**  
From docs/thesis/FINE_TUNING_STRATEGY.md lines 79-82:
"Catastrophic forgetting: integrating new data overwrites previously learned knowledge. Strategies: mixed data (30% old domain + 70% new domain), EWC (Elastic Weight Consolidation), progressive neural networks."

---

#### **2. Tier 1: Small Labeled Audit Set (Minimal Human Effort)**

**Strategy:** Label a **small random sample** of production windows to estimate accuracy and enable supervised fine-tuning.

**Protocol:**

**A. Sample Size Calculation**

For 95% confidence interval with 5% margin:

30n = \frac{Z^2 \cdot p(1-p)}{E^2} = \frac{1.96^2 \cdot 0.5 \cdot 0.5}{0.05^2} \approx 384 \text{ windows}30

**Practical minimum:** 50-200 windows (wider CI, acceptable for retraining decision)

**Example:**
- Sample 100 windows randomly, label manually
- Compute accuracy: 75 correct  75% accuracy
- 95% CI: 75%  8.5%  True accuracy likely 66.5%-83.5%
- **Decision:** If accuracy < 70%  Retrain needed, else  Monitor only

**B. Stratified Sampling (Better than Random)**

**Goal:** Ensure all activity classes and uncertainty levels represented

**Strata:**
1. **By predicted class:** 5-10 samples per predicted class (balanced)
2. **By confidence:** 30% high-confidence (>0.80), 30% medium (0.60-0.80), 40% low (<0.60)
3. **By temporal distribution:** Sample from different hours/days

**Why better:** Random sampling may miss rare classes (e.g., if running is 5% of data, random sample may have 0-1 running windows  insufficient for evaluation).

**Repo Evidence:**  
From docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md lines 329-345:
"Random Sampling Protocol: From each batch, randomly sample 50-200 windows. Have human annotator label. Compute accuracy with 95% CI. Example: 100 windows, 82 correct  accuracy 82%  7.5%."

---

**C. Active Sampling (Uncertainty-Based)**

**Goal:** Label the **most informative** windows (highest uncertainty) first

**Protocol:**
1. Sort production windows by confidence (ascending: lowest confidence first)
2. Label top 20-50 most uncertain windows
3. This gives **worst-case accuracy** estimate
4. Also label 20 high-confidence windows to check calibration

**Rationale:**
- Uncertain predictions are more likely to be wrong  labeling these reveals model's failure modes
- High-confidence predictions should be correct  if not, model is overconfident (calibration issue)
- Focuses human effort on hard cases (not wasted on trivial easy windows)

**Example:**
- 1000 production windows sorted by confidence
- Bottom 50 (confidence 0.30-0.50): 60% correct  model correctly identifies uncertainty
- Top 20 (confidence 0.90-0.98): 95% correct  model is well-calibrated
- **Decision:** Model is uncertain but honest  retrain to improve confidence, not accuracy

**Repo Evidence:**  
From docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md lines 351-370:
"Active Sampling Protocol: Sort windows by uncertainty, label top 20-50 most uncertain. Gives worst-case accuracy. Reveals confusion patterns between specific classes."

---

**D. Sentinel Session Protocol (Controlled Conditions)**

**Goal:** Weekly controlled recording with ground truth  direct accuracy measurement

**Protocol:**
1. User performs scripted activity sequence (e.g., 2 min sitting  2 min walking  2 min hand_tapping)
2. Record activities with timestamps (ground truth)
3. Run inference on this session
4. Compute window-level accuracy (timestamps  window labels)
5. Track accuracy over time (weekly sentinel sessions)

**Advantages:**
- **No labeling cost:** User self-reports activities (scripted, not retrospective)
- **Temporal tracking:** Weekly sentinel accuracy shows if model degrades
- **Ground truth:** Timestamps provide exact labels

**Use Case:** Production monitoring dashboard shows "Sentinel Accuracy: 87%" (this week) vs "85%" (last week)  model stable

**Repo Evidence:**  
From docs/thesis/UNLABELED_EVALUATION.md lines 302-320:
"Sentinel Protocol: Weekly controlled sessions with scripted activities. User self-reports activities with timestamps. Direct accuracy measurement without retrospective labeling."

---

#### **3. Tier 2: Active Learning (Query Human for Hard Cases)**

**Strategy:** Iteratively select **most informative** unlabeled samples, query human, add to training set, retrain.

**Active Learning Loop:**

`
Initial model M_0 (trained on original dataset)

For iteration t = 1 to T:
  1. Run inference on unlabeled pool U
  2. Score samples by informativeness (uncertainty, diversity, representativeness)
  3. Select k samples with highest scores
  4. Query human annotator for labels
  5. Add labeled samples to training set L
  6. Retrain model M_t on L
  7. If convergence or budget exhausted  STOP
`

**Informativeness Metrics (3 Strategies):**

**A. Uncertainty Sampling (Most Common)**

**Formula:** Select windows with **lowest confidence** (highest entropy)

30\text{uncertainty}(x) = -\sum_{i=1}^{C} p_i \log p_i30

where $ = softmax probability for class $.

**Example:**
- Window 1: probs = [0.95, 0.02, 0.01, 0.01, 0.01]  entropy = 0.25 (low uncertainty)
- Window 2: probs = [0.35, 0.33, 0.32, 0.00, 0.00]  entropy = 1.58 (high uncertainty)
- Query Window 2 first (ambiguous between 3 classes)

**Trade-off:** Focuses on decision boundary, but may oversample outliers

---

**B. Diversity Sampling (Complementary to Uncertainty)**

**Goal:** Select samples that are **different from each other** (avoid querying similar windows)

**Method:** K-means clustering in embedding space, select 1 sample per cluster

**Implementation:**
1. Extract embeddings:  = \text{encoder}(x)$ (output of LSTM layer before softmax)
2. Cluster embeddings: K-means with k = num_classes times 2
3. Select centroid or most uncertain sample per cluster

**Advantage:** Ensures coverage of feature space (not just boundary cases)

**Repo Context:**  
From scripts/post_inference_monitoring.py lines 956-1050:
EmbeddingDriftDetector extracts latent embeddings from penultimate layer. These embeddings can be used for diversity sampling via clustering.

---

**C. Expected Model Change (Advanced)**

**Goal:** Select samples that will cause **largest gradient update** to model parameters

**Formula:**

30\text{EMC}(x) = \mathbb{E}_{\hat{y}}[\|\nabla_\theta \mathcal{L}(x, \hat{y})\|]30

where $\hat{y}$ is sampled from model's predicted distribution.

**Intuition:** If labeling this sample would cause big weight update  sample is informative

**Trade-off:** Computationally expensive (requires gradient computation per sample)

---

**Active Learning Stopping Criteria:**

Stop when:
1. **Budget exhausted:** Labeled k = 200 samples (labeling cost limit)
2. **Convergence:** New labels don't improve validation accuracy (gain < 1%)
3. **Confidence plateau:** Remaining unlabeled pool has uniform high confidence (all > 0.85)

**Repo Actions:**
- Create scripts/active_learning/select_informative_samples.py
  - Input: predictions.csv (confidence scores)
  - Output: sample_ids_to_label.json (prioritized list)
  - Methods: uncertainty_sampling(), diversity_sampling(), expected_model_change()

---

#### **4. Tier 3: Pseudo-Labeling with Confidence Filtering**

**Strategy:** Use model's own predictions as training labels, but **ONLY for high-confidence predictions** (confidence > 0.80).

**Pseudo-Labeling Protocol:**

**Step 1: Confidence Filtering**

`python
# Filter production predictions
confident_mask = (predictions['confidence'] > 0.80)
pseudo_labeled_data = predictions[confident_mask]

# Expected: 40-60% of production data passes filter
# If < 20% pass  model too uncertain  DO NOT use pseudo-labels
`

**Step 2: Class Balance Check**

**Problem:** Production class distribution may differ from training (e.g., 70% sitting, 5% running)

**Solution:** Stratified sampling to balance pseudo-labeled data

`python
from sklearn.model_selection import train_test_split

# Per-class limits (prevent sitting dominance)
max_samples_per_class = 500

pseudo_labeled_balanced = []
for activity in activities:
    class_samples = pseudo_labeled_data[pseudo_labeled_data['predicted_class'] == activity]
    if len(class_samples) > max_samples_per_class:
        class_samples = class_samples.sample(n=max_samples_per_class, random_state=42)
    pseudo_labeled_balanced.append(class_samples)

pseudo_labeled_final = pd.concat(pseudo_labeled_balanced)
`

**Step 3: Pseudo-Label Validation (Cross-Check)**

**Temporal consistency check:** If prediction flips rapidly (sitting  hand_tapping  sitting in 3 consecutive windows), **reject** these windows from pseudo-labeled set (likely noise).

`python
# Flag unstable sequences
def detect_unstable_sequences(predictions, window_size=3):
    unstable_indices = []
    for i in range(len(predictions) - window_size):
        window = predictions['predicted_class'][i:i+window_size]
        if len(set(window)) == window_size:  # All different classes
            unstable_indices.extend(range(i, i+window_size))
    return unstable_indices

# Remove unstable windows from pseudo-labeled set
stable_pseudo_labels = pseudo_labeled_final[~pseudo_labeled_final.index.isin(unstable_indices)]
`

**Step 4: Risk Assessment (Before Training)**

**Gating criteria:** Only use pseudo-labels if:
1. At least 30% of production data passes confidence > 0.80 filter
2. All activity classes represented (at least 20 samples per class)
3. Temporal consistency: unstable_ratio < 0.20 (reject if 20%+ windows are flipping)

**If ANY criterion fails:**  
 Fall back to Tier 1 (label audit set manually) or Tier 2 (active learning)  
 **DO NOT** use pseudo-labels blindly

**Repo Evidence:**  
From docs/research/KEEP_Research_QA_From_Papers.md lines 117-125:
"Semi-supervised / few-shot adaptation uses small number of labeled samples. Few-shot success: fine-tuning on 6 volunteers (small custom dataset) bridged 49%  87% accuracy."

---

#### **5. Tier 4: Mixed-Data Fine-Tuning (Catastrophic Forgetting Mitigation)**

**Strategy:** Retrain on **combination** of original training data + new labeled/pseudo-labeled production data, with **lower learning rate** and **frozen layers**.

**Why Mixed Data?**

**Problem:** Training on 100% new production data causes **catastrophic forgetting** (model overwrites original knowledge).

**Example:**
- Original training: 5 users, smartwatch A, lab conditions
- Production data: 1 user, smartwatch B (Garmin), real-world conditions
- Retrain on 100% production  model forgets smartwatch A patterns  fails on multi-device deployment

**Solution:** 30% original training + 70% new production (configurable ratio)

**Mixed-Data Training Configuration:**

`yaml
fine_tuning:
  learning_rate: 0.0001      # 10x smaller than original (0.001)
  epochs: 20                  # Fewer epochs than full training (50)
  early_stopping_patience: 5
  batch_size: 32
  
  # Data mixing
  training_data:
    old_domain_samples: 30%   # Prevent forgetting
    new_domain_samples: 70%   # Adapt to new domain
  
  # Layer freezing (preserve low-level features)
  freeze_layers:
    - conv1d_1              # First CNN layer (edge detectors)
    - conv1d_2              # Second CNN layer (patterns)
    # Fine-tune LSTM and dense head (high-level reasoning)
  
  # Validation (from production domain ONLY)
  validation_source: "production_labeled_audit_set"
  validation_split: 0.2
  
  # Gating criteria
  promotion_thresholds:
    min_accuracy_improvement: 0.05   # 5% better than current
    min_absolute_accuracy: 0.70      # At least 70% on production val
    max_training_accuracy: 0.95      # Prevent overfitting
`

**Frozen Layers Rationale:**

From docs/thesis/FINE_TUNING_STRATEGY.md lines 128-148:
"Head-only transfer learning: Freeze CNN + BiLSTM feature extraction. Only fine-tune final classification head. Low learning rate (1e-4) for fine-tuning."

**Why freeze early layers:**
- CNN layers learn **generic features** (edges, peaks, periodicity)  transferable across devices
- LSTM layers learn **activity patterns** (temporal dependencies)  may need adaptation
- Dense head learns **class-specific boundaries**  definitely needs retraining

**Freezing Strategy:**
- **Conservative (recommended):** Freeze first 60% of layers
- **Moderate:** Freeze first 40% of layers
- **Aggressive:** Freeze only first 20% (risk forgetting)

---

**Alternative: Elastic Weight Consolidation (EWC)**

**Concept:** Penalize changes to important weights (those critical for old tasks)

**Loss function:**

30\mathcal{L}_{\text{EWC}} = \mathcal{L}_{\text{new}}(\theta) + \lambda \sum_i F_i (\theta_i - \theta_i^*)^230

where:
- $\mathcal{L}_{\text{new}}$ = loss on new data
- $ = Fisher information (importance of weight $ for old task)
- $\theta_i^*$ = weight value from old model
- $\lambda$ = regularization strength (0.1-1.0)

**Trade-off:** Requires computing Fisher information (additional computation), but more flexible than hard freezing

**Repo Evidence:**  
From docs/research/KEEP_Research_QA_From_Papers.md line 79:
"Catastrophic forgetting challenge. Strategies: mixed data, EWC (Elastic Weight Consolidation), progressive neural networks."

---

#### **6. Safety Checks + Gating Criteria**

**Pre-Training Validation (MANDATORY):**

| Check | Criterion | Action if Fails |
|-------|-----------|-----------------|
| **Data volume** | New labeled data > 200 windows | Collect more labels (active learning) |
| **Class coverage** | All 6 activities represented (min 20 samples each) | Reject under-represented classes |
| **Pseudo-label quality** | Confidence > 0.80 for at least 30% of production data | Use only audit set (no pseudo-labels) |
| **Temporal consistency** | Flip rate < 0.30 in pseudo-labeled data | Filter unstable sequences |
| **Sensor drift** | No variance collapse (std > 0.1 on all channels) | BLOCK batch, investigate sensor failure |

**Post-Training Validation (MANDATORY):**

| Check | Criterion | Action if Fails |
|-------|-----------|-----------------|
| **Production val accuracy** | Accuracy on held-out production val set > 70% | Rollback to previous model |
| **Training val accuracy** | Accuracy on original validation set > 85% (within 5% of baseline) | Rollback (catastrophic forgetting detected) |
| **Improvement delta** | Production accuracy improved by at least 5% | Rollback (insufficient gain, not worth deployment risk) |
| **Overfitting check** | Training accuracy - validation accuracy < 10% | Rollback (overfitting detected) |
| **Confidence calibration** | Expected Calibration Error (ECE) < 0.10 | Recalibrate or rollback |

**Rollback Procedure:**

`python
# If any post-training check fails:
1. Restore previous model from MLflow registry
2. Log failure reason to MLflow (tags.retraining_failed = reason)
3. Keep new labeled data for future retraining attempts
4. Send alert to monitoring dashboard
`

---

**CITATIONS (Pair 14, Q1):**

1. **(docs/thesis/FINE_TUNING_STRATEGY.md, lines 79-82, 155-165):** Catastrophic forgetting mitigation strategies: "Integrating new data overwrites previously learned knowledge. Strategies: mixed data (30% old domain + 70% new domain), EWC, progressive neural networks. Fine-tuning config: learning_rate 0.0001 (10x smaller), epochs 20, mixed data 30% old + 70% new, validation from production domain only."

2. **(docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md, lines 329-345):** Random sampling protocol for accuracy estimation: "From each batch, randomly sample 50-200 windows, have human label, compute accuracy with 95% CI. Example: 100 windows, 82 correct  accuracy 82%  7.5% (true accuracy likely 74.5%-89.5%)."

3. **(docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md, lines 351-370):** Active sampling for uncertainty-based labeling: "Sort windows by uncertainty (low confidence first), label top 20-50 most uncertain. Gives worst-case accuracy estimate. Reveals confusion patterns between specific classes. Also label high-confidence windows to check calibration."

4. **(docs/thesis/UNLABELED_EVALUATION.md, lines 302-320):** Sentinel session protocol for controlled accuracy measurement: "Weekly controlled sessions with scripted activities (2 min sitting  2 min walking  2 min hand_tapping). User self-reports activities with timestamps (ground truth). Direct accuracy measurement without retrospective labeling."

5. **(docs/research/KEEP_Research_QA_From_Papers.md, lines 117-125):** Few-shot and semi-supervised learning for HAR: "Semi-supervised / few-shot adaptation uses small number of labeled samples to significantly boost performance. Few-shot success: fine-tuning pre-trained model on small custom dataset (6 volunteers) bridged 49%  87% accuracy." (Original source: papers needs to read/ICTH_16.pdf)

6. **(scripts/post_inference_monitoring.py, lines 956-1050):** Embedding extraction for diversity sampling: "EmbeddingDriftDetector extracts latent embeddings from penultimate layer (before softmax). Used for MMD and Mahalanobis distance drift detection. These embeddings can be clustered for active learning diversity sampling."

---

**Summary Table (Q1):**

| **Strategy** | **Human Effort** | **Data Required** | **Risk Level** | **Accuracy Gain** | **When to Use** |
|--------------|------------------|-------------------|----------------|-------------------|-----------------|
| **Tier 1: Audit Set** | 50-200 labels (2-4 hours) | Random sample | Low | +5-10% (supervised) | Always (baseline) |
| **Tier 2: Active Learning** | 20-50 labels (1-2 hours) | Uncertainty-sorted | Low | +10-20% (targeted) | When labeling budget limited |
| **Tier 3: Pseudo-Labels** | 0 labels (automated) | Confidence > 0.80 subset | Medium-High | +5-15% (if well-calibrated) | When model is confident (>30% pass filter) |
| **Tier 4: Mixed Fine-Tuning** | 200+ labels (4-8 hours) | 30% old + 70% new | Medium | +15-30% (best) | When sufficient labeled production data |
| **Sentinel Sessions** | 0 labels (self-report) | Weekly controlled activities | Low | N/A (monitoring only) | Ongoing accuracy tracking |

---




### **Q2: Provide thesis-ready retraining protocol: drift trigger  sample  label minimal  retrain  verify  deploy/rollback (with artifacts + logs).**

**Decision:** Implement **7-stage gated retraining pipeline** with automated triggers and human checkpoints: **(Stage 1)** Drift detection triggers alert (PSI > 0.25 OR n_drift_channels > 2), **(Stage 2)** Sample 100 windows (stratified: 50 random + 50 uncertain), **(Stage 3)** Human labels 100 windows (2-4 hours), **(Stage 4)** Validate labeling quality (inter-rater agreement > 0.80 if multiple labelers), **(Stage 5)** Retrain with mixed data (30% old + 70% new, frozen layers, early stopping), **(Stage 6)** A/B test on production val set (new model must beat old by 5%+), **(Stage 7)** Deploy via symlink swap OR rollback if gating fails. All stages logged to **MLflow** with evolution_id for full provenance.

**Detailed Explanation:**

#### **Stage 1: Drift Detection Triggers Retraining Alert**

**Trigger Logic (from Pair 13):**

**Primary Triggers (ANY causes WARN):**
- **Drift detected:** PSI > 0.25 on any channel OR drift in >2 of 6 channels
- **Confidence drop:** Mean confidence < 0.60 OR uncertain_ratio > 0.30
- **Prediction instability:** Flip rate > 0.30 (rapid class switching)

**Secondary Trigger (Scheduled Fallback):**
- **Time-based:** 30+ days since last retraining (even if no drift detected)

**Composite Trigger (Recommended for Retraining):**

`python
# scripts/evolution_trigger.py

def check_retraining_trigger(
    drift_report: dict,
    confidence_report: dict,
    temporal_report: dict,
    days_since_last_retrain: int
) -> Tuple[bool, str]:
    """
    Returns: (should_retrain, reason)
    """
    
    # Strong trigger: Drift + confidence drop
    if drift_report['n_drift_channels'] > 2 and confidence_report['mean_confidence'] < 0.60:
        return (True, "Strong: Drift detected (>2 channels) AND confidence drop (<0.60)")
    
    # Moderate trigger: Major drift OR severe confidence drop
    if drift_report['psi_max'] > 0.25:
        return (True, "Moderate: Major drift (PSI > 0.25)")
    
    if confidence_report['uncertain_ratio'] > 0.30:
        return (True, "Moderate: High uncertainty (>30% uncertain predictions)")
    
    # Weak trigger: Temporal instability + time elapsed
    if temporal_report['flip_rate'] > 0.30 and days_since_last_retrain > 30:
        return (True, "Weak: Prediction instability + 30+ days elapsed")
    
    # Scheduled trigger: No issues but time-based retraining
    if days_since_last_retrain > 90:
        return (True, "Scheduled: 90+ days since last retraining")
    
    return (False, "No retraining needed (all metrics PASS)")
`

**Output:**
- MLflow log: metric "monitoring/retraining_triggered" = 1 (0 if no trigger)
- Alert: Email/Slack notification with trigger reason + dashboard link
- File: reports/retraining_triggers/trigger_2026-01-18_163045.json

**JSON Schema:**

`json
{
  "timestamp": "2026-01-18T16:30:45Z",
  "trigger_decision": true,
  "trigger_reason": "Strong: Drift detected (>2 channels) AND confidence drop (<0.60)",
  "trigger_category": "drift+confidence",
  "drift_details": {
    "n_drift_channels": 3,
    "drift_channels": ["Ax", "Ay", "Gz"],
    "psi_max": 0.32,
    "wasserstein_max": 0.68
  },
  "confidence_details": {
    "mean_confidence": 0.54,
    "uncertain_ratio": 0.38
  },
  "days_since_last_retrain": 45,
  "recommended_action": "Proceed to Stage 2: Sample production data for labeling"
}
`

**Repo Evidence:**  
From docs/output_1801_2026-01-18.md lines 2154-2166 (Pair 08):
"Create scripts/evolution_trigger.py for automated trigger detection. EvolutionDecisionMaker class with methods for each trigger category (drift, performance, scheduled)."

---

#### **Stage 2: Sample Production Data (Stratified + Uncertainty-Based)**

**Goal:** Select 100 windows for human labeling (balance random + informative)

**Sampling Strategy (50/50 Split):**

**A. Random Stratified Sample (50 windows)**

**Purpose:** Unbiased accuracy estimate + class balance

**Protocol:**
1. Group production predictions by predicted class
2. Sample 8-9 windows per class (6 classes  8 = 48, round to 50)
3. Ensures all activities represented

`python
# scripts/sampling/stratified_sample.py

def stratified_sample(predictions_df, n_samples=50):
    """
    Stratified sampling by predicted class
    """
    samples_per_class = n_samples // len(predictions_df['predicted_class'].unique())
    
    sampled_indices = []
    for activity in predictions_df['predicted_class'].unique():
        class_df = predictions_df[predictions_df['predicted_class'] == activity]
        if len(class_df) >= samples_per_class:
            sampled = class_df.sample(n=samples_per_class, random_state=42)
        else:
            sampled = class_df  # Take all if insufficient
        sampled_indices.extend(sampled.index.tolist())
    
    return predictions_df.loc[sampled_indices]
`

---

**B. Uncertainty-Based Sample (50 windows)**

**Purpose:** Label hard cases (model's failure modes)

**Protocol:**
1. Sort predictions by confidence (ascending)
2. Select bottom 50 (lowest confidence)
3. These windows reveal where model struggles

`python
# scripts/sampling/uncertainty_sample.py

def uncertainty_sample(predictions_df, n_samples=50):
    """
    Select most uncertain predictions
    """
    sorted_df = predictions_df.sort_values('confidence', ascending=True)
    return sorted_df.head(n_samples)
`

---

**C. Combined Sample Export**

**Output:** data/labeling_queue/labeling_batch_2026-01-18.csv

**Columns:**
- window_id: Unique identifier (e.g., "prod_batch5_window_042")
- timestamp: Original recording timestamp
- predicted_class: Model's prediction
- confidence: Softmax confidence (0-1)
- sampling_method: "stratified" or "uncertainty"
- sensor_data_path: Link to raw 2006 sensor array (for manual inspection if needed)

**Repo Actions:**
- Create scripts/sampling/generate_labeling_batch.py
  - Input: predictions.csv (from run_inference.py)
  - Output: labeling_batch_{date}.csv (100 rows)
  - Logs: MLflow tag "labeling_batch_id" = timestamp

---

#### **Stage 3: Human Labeling (Manual Annotation)**

**Goal:** Obtain ground truth labels for 100 sampled windows

**Labeling Interface Options:**

**Option A: CSV-Based (Simple)**

**Protocol:**
1. Open labeling_batch_{date}.csv in Excel/Google Sheets
2. Add column "ground_truth_label" (human-annotated class)
3. Add column "confidence_rating" (1-5, labeler's confidence)
4. Add column "notes" (optional, for ambiguous cases)

**Example:**

| window_id | predicted_class | confidence | ground_truth_label | confidence_rating | notes |
|-----------|-----------------|------------|-------------------|-------------------|-------|
| prod_batch5_window_042 | sitting | 0.87 | sitting | 5 | Clear sitting |
| prod_batch5_window_123 | hand_tapping | 0.42 | writing | 3 | Ambiguous motion |

**Option B: Annotation Tool (Scalable)**

**Tools:**
- Label Studio (https://labelstud.io/) - open-source, supports time series
- Prodigy (https://prodi.gy/) - active learning built-in
- Custom Streamlit app (repo-specific)

**Features:**
- Display sensor plots (6 channels  200 timesteps)
- Show model's predicted class + confidence
- Dropdown for ground truth selection
- Undo/redo, session resume

---

**Labeling Guidelines (Quality Control):**

**Activity Definitions (from training labels):**

| Activity | Definition | Sensor Signature |
|----------|------------|------------------|
| **sitting** | Stationary, minimal movement | Low variance, gravity-dominated |
| **walking** | Regular gait cycle | Periodic Ay/Gz oscillations (1-2 Hz) |
| **hand_tapping** | Rapid repetitive finger motion | High-frequency Ax/Ay spikes (3-5 Hz) |
| **writing** | Fine motor hand movements | Low-amplitude Ax/Ay variations |
| **running** | High-intensity locomotion | High variance, 2-3 Hz periodicity |
| **standing** | Upright stationary | Gravity-dominated, slight sway |

**Edge Cases:**
- **Ambiguous transitions:** Label as dominant activity (e.g., 70% sitting + 30% standing  sitting)
- **Multiple activities:** If 50/50 split, flag as "unclear" (exclude from training)
- **Sensor artifacts:** If variance collapse or obvious failure, label as "bad_data" (exclude)

---

**Inter-Rater Reliability (If Multiple Labelers):**

**Goal:** Ensure labeling consistency (Cohen's kappa > 0.80)

**Protocol:**
1. Two labelers independently label **same 20 windows** (overlap set)
2. Compute agreement:

Appended Pair 14 Part 1 (Q1) successfully\kappa = \frac{p_o - p_e}{1 - p_e}Appended Pair 14 Part 1 (Q1) successfully

where:
- $ = observed agreement (e.g., 18/20 = 0.90)
- $ = expected agreement by chance (e.g., 0.20 for 6 classes)

**Example:**
- 18/20 agree  $\kappa = \frac{0.90 - 0.167}{1 - 0.167} = 0.88$ (excellent)
- If $\kappa < 0.80$  Review labeling guidelines, re-train labelers

**Repo Evidence:**  
From docs/output_1801_2026-01-18.md lines 145-152 (Pair 01):
"Retraining triggers: F1 < 0.80 or drift detected. Safe decision with minimal labels. Implementation: 4 scripts + documentation."

---

#### **Stage 4: Validate Labeling Quality**

**Goal:** Ensure labeled data is trustworthy before retraining

**Quality Checks:**

| Check | Criterion | Action if Fails |
|-------|-----------|-----------------|
| **Completion rate** | All 100 windows labeled (no missing labels) | Request labeler to complete |
| **Class distribution** | All 6 activities present (min 5 samples each) | Sample additional windows for rare classes |
| **Labeler confidence** | 80%+ of labels have confidence_rating  4 | Review low-confidence labels, possibly exclude |
| **Inter-rater agreement** | Cohen's kappa > 0.80 (if 2+ labelers) | Reconcile disagreements, retrain labelers |
| **Consistency with predictions** | Agreement rate 60-90% (not too low or too high) | If < 40%: model very wrong (good!), If > 95%: labels may be biased by predictions |

**Flagging Bad Labels:**

**Scenario:** Labeler confidence_rating = 1 or 2 (very uncertain)

**Action:** Exclude from training set (too noisy), but keep for analysis (reveals model's confusion patterns)

**Example:**
- Window 123: predicted = hand_tapping (0.42), labeled = writing (confidence 2)
- Note: "Ambiguous between hand_tapping and writing, motion unclear"
- **Action:** Exclude from training, add to "hard cases" dataset for future review

---

#### **Stage 5: Retrain with Mixed Data + Frozen Layers**

**Training Configuration (Detailed):**

**A. Data Preparation**

**Input Sources:**
1. **Old domain:** data/prepared/X_prepared_window.npy (original training, shape: n_old  200  6)
2. **New domain:** data/labeling/labeled_production_windows.npy (100 labeled windows, shape: 100  200  6)
3. **Pseudo-labels (optional):** If confidence > 0.80 filter passes, add high-confidence production windows

**Mixing Ratio:**
- 30% old domain (prevent catastrophic forgetting)
- 70% new domain (adapt to production distribution)

**Example:**
- Old training data: 5000 windows
- New labeled data: 100 windows (too small!)
- Solution: Repeat new data 10x (with augmentation)  1000 windows
- Final mix: 1500 old + 1000 new = 2500 total

**Augmentation (Optional):**
- Time jitter: shift windows by 10 samples
- Gaussian noise: add noise with std = 0.05
- Scaling: multiply by random factor 0.95-1.05

`python
# scripts/retraining/prepare_mixed_data.py

def prepare_mixed_training_data(
    old_X, old_y,
    new_X, new_y,
    mix_ratio_old=0.3,
    repeat_new=10,
    augment=True
):
    # Repeat new data to balance
    new_X_repeated = np.repeat(new_X, repeat_new, axis=0)
    new_y_repeated = np.repeat(new_y, repeat_new, axis=0)
    
    # Augment new data
    if augment:
        new_X_repeated = augment_sensor_data(new_X_repeated)
    
    # Sample old data to match ratio
    n_new = len(new_X_repeated)
    n_old = int(n_new * (mix_ratio_old / (1 - mix_ratio_old)))
    
    old_indices = np.random.choice(len(old_X), size=n_old, replace=False)
    old_X_sampled = old_X[old_indices]
    old_y_sampled = old_y[old_indices]
    
    # Combine
    X_mixed = np.concatenate([old_X_sampled, new_X_repeated], axis=0)
    y_mixed = np.concatenate([old_y_sampled, new_y_repeated], axis=0)
    
    # Shuffle
    shuffle_idx = np.random.permutation(len(X_mixed))
    X_mixed = X_mixed[shuffle_idx]
    y_mixed = y_mixed[shuffle_idx]
    
    return X_mixed, y_mixed
`

---

**B. Model Configuration**

**Load Existing Model:**

`python
from tensorflow import keras

# Load current production model
model = keras.models.load_model('models/active/model.keras')

# Freeze early layers (preserve low-level features)
for layer in model.layers[:8]:  # Adjust based on architecture
    layer.trainable = False

# Print trainable status
for i, layer in enumerate(model.layers):
    print(f"Layer {i}: {layer.name} - Trainable: {layer.trainable}")
`

**Expected Output:**

`
Layer 0: conv1d_1 - Trainable: False
Layer 1: batch_normalization_1 - Trainable: False
Layer 2: conv1d_2 - Trainable: False
Layer 3: batch_normalization_2 - Trainable: False
...
Layer 8: bidirectional_lstm - Trainable: True
Layer 9: dense_head - Trainable: True
`

**Compile with Low Learning Rate:**

`python
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),  # 10x lower than original
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
`

---

**C. Training Loop with Early Stopping**

`python
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Callbacks
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

checkpoint = ModelCheckpoint(
    'models/evolution/candidate_model.keras',
    monitor='val_accuracy',
    save_best_only=True
)

# Train
history = model.fit(
    X_mixed, y_mixed,
    validation_split=0.2,
    epochs=20,
    batch_size=32,
    callbacks=[early_stop, checkpoint],
    verbose=1
)
`

**MLflow Logging:**

`python
import mlflow

with mlflow.start_run(run_name=f"retraining_{timestamp}") as run:
    # Log parameters
    mlflow.log_params({
        "evolution_type": "retraining",
        "trigger_reason": trigger_reason,
        "old_domain_ratio": 0.3,
        "new_domain_ratio": 0.7,
        "new_labeled_samples": 100,
        "learning_rate": 1e-4,
        "frozen_layers": 8,
        "epochs": 20
    })
    
    # Log training metrics
    for epoch, (loss, acc, val_loss, val_acc) in enumerate(zip(
        history.history['loss'],
        history.history['accuracy'],
        history.history['val_loss'],
        history.history['val_accuracy']
    )):
        mlflow.log_metrics({
            "train_loss": loss,
            "train_accuracy": acc,
            "val_loss": val_loss,
            "val_accuracy": val_acc
        }, step=epoch)
    
    # Log model
    mlflow.keras.log_model(model, "candidate_model")
    
    # Store evolution_id for Stage 6
    evolution_id = f"evolution_{timestamp}"
    mlflow.set_tag("evolution_id", evolution_id)
`

---

#### **Stage 6: A/B Test on Production Validation Set**

**Goal:** Verify new model is better than old model on production data

**Protocol:**

**A. Split Labeled Production Data**

- **Training set:** 80 windows (used in Stage 5)
- **Validation set:** 20 windows (held-out, not used in training)

**Rationale:** Validation set represents production distribution but unseen during training

---

**B. Run Both Models on Validation Set**

`python
# scripts/retraining/ab_test.py

def compare_models(old_model_path, new_model_path, X_val, y_val):
    """
    Compare old vs new model on held-out validation set
    """
    old_model = keras.models.load_model(old_model_path)
    new_model = keras.models.load_model(new_model_path)
    
    # Evaluate old model
    old_preds = old_model.predict(X_val)
    old_accuracy = np.mean(np.argmax(old_preds, axis=1) == y_val)
    old_confidence = np.mean(np.max(old_preds, axis=1))
    
    # Evaluate new model
    new_preds = new_model.predict(X_val)
    new_accuracy = np.mean(np.argmax(new_preds, axis=1) == y_val)
    new_confidence = np.mean(np.max(new_preds, axis=1))
    
    # Compute deltas
    accuracy_improvement = new_accuracy - old_accuracy
    confidence_improvement = new_confidence - old_confidence
    
    return {
        "old_accuracy": old_accuracy,
        "new_accuracy": new_accuracy,
        "accuracy_improvement": accuracy_improvement,
        "old_confidence": old_confidence,
        "new_confidence": new_confidence,
        "confidence_improvement": confidence_improvement
    }
`

---

**C. Gating Criteria (Promotion Decision)**

**Criteria (ALL must pass):**

| Criterion | Threshold | Rationale |
|-----------|-----------|-----------|
| **Production val accuracy improvement** | Δ > +5% | New model meaningfully better on production |
| **Production val absolute accuracy** | Accuracy > 70% | New model meets minimum bar |
| **Training val accuracy (catastrophic forgetting check)** | Accuracy > 85% (within 5% of baseline) | New model hasn't forgotten old domain |
| **No overfitting** | train_acc - val_acc < 10% | Model generalizes |
| **Confidence calibration** | mean_confidence  accuracy (within 10%) | Predictions trustworthy |

**Example Decision:**

`
Old Model (Production Val):
  Accuracy: 68%
  Confidence: 0.72

New Model (Production Val):
  Accuracy: 78% (Δ = +10%, PASS)
  Confidence: 0.76 (Δ = +0.04, PASS)

New Model (Training Val):
  Accuracy: 87% (PASS, within 5% of baseline 90%)

Overfitting Check:
  Train accuracy: 82%
  Val accuracy: 78%
  Gap: 4% (PASS, < 10%)

Calibration Check:
  Accuracy: 78%
  Mean confidence: 0.76
  Difference: 2% (PASS, well-calibrated)

DECISION: PROMOTE NEW MODEL 
`

**MLflow Logging:**

`python
mlflow.log_metrics({
    "ab_test/old_accuracy": 0.68,
    "ab_test/new_accuracy": 0.78,
    "ab_test/accuracy_improvement": 0.10,
    "ab_test/promotion_decision": 1  # 1 = promote, 0 = rollback
})

mlflow.set_tag("promotion_status", "promoted")
mlflow.set_tag("promotion_reason", "Accuracy improved by 10%, all gates passed")
`

---

#### **Stage 7: Deploy (Symlink Swap) or Rollback**

**Deployment Strategy (from Pair 11):**

**A. Evolution Package Structure**

`
models/evolution_packages/
 evolution_20260118_163045_retraining/
     model.keras                  # Retrained model
     scaler.pkl                   # Scaler (unchanged, copied from old)
     config.json                  # Training config
     baseline_stats.json          # Updated baseline from production
     monitoring_config.yaml       # Thresholds (unchanged)
     manifest.json                # Provenance
     validation_report.json       # A/B test results
`

**Manifest JSON:**

`json
{
  "evolution_id": "evolution_20260118_163045_retraining",
  "evolution_type": "retraining",
  "trigger_reason": "Strong: Drift detected (>2 channels) AND confidence drop (<0.60)",
  "created_at": "2026-01-18T16:30:45Z",
  "parent_evolution_id": "evolution_20260101_120000_baseline",
  "mlflow_run_id": "abc123def456",
  "git_commit": "f7a8b9c0",
  "training_data": {
    "old_domain_samples": 1500,
    "new_domain_samples": 1000,
    "labeled_production_windows": 100
  },
  "model_metadata": {
    "architecture": "1D-CNN-BiLSTM",
    "frozen_layers": 8,
    "trainable_parameters": 45000,
    "training_epochs": 12,
    "early_stopped": true
  },
  "validation_results": {
    "production_val_accuracy": 0.78,
    "training_val_accuracy": 0.87,
    "accuracy_improvement": 0.10,
    "overfitting_gap": 0.04,
    "promoted": true
  },
  "deployment": {
    "deployed_at": "2026-01-18T18:45:00Z",
    "deployed_by": "automated_pipeline",
    "symlink_target": "models/active"
  }
}
`

---

**B. Deployment via Symlink (Atomic Operation)**

**Current State:**

`
models/active -> evolution_20260101_120000_baseline/
`

**Deployment Command:**

`ash
# Backup current active symlink target
CURRENT_TARGET=
echo  > models/checkpoints/rollback_target.txt

# Create new evolution package
cp models/evolution/candidate_model.keras models/evolution_packages/evolution_20260118_163045_retraining/model.keras

# Atomic symlink swap (single filesystem operation)
ln -sfn evolution_packages/evolution_20260118_163045_retraining models/active

# Verify deployment
echo "New active model: "
`

**New State:**

`
models/active -> evolution_packages/evolution_20260118_163045_retraining/
`

**Advantages:**
- **Atomic:** Single operation (no partial state)
- **Instant:** No file copying (just pointer update)
- **Reversible:** Can rollback by reverting symlink

---

**C. Rollback Procedure (If Promotion Fails)**

**Trigger:** Any gating criterion fails (accuracy < 70%, overfitting, catastrophic forgetting)

**Rollback Steps:**

`ash
# 1. Restore previous symlink target
PREVIOUS_TARGET=
ln -sfn  models/active

# 2. Log rollback to MLflow
mlflow.set_tag("promotion_status", "rolled_back")
mlflow.set_tag("rollback_reason", "Accuracy < 70% threshold")

# 3. Archive failed candidate
mv models/evolution/candidate_model.keras models/evolution_archive/failed_20260118_163045.keras

# 4. Send alert
echo "Retraining FAILED. Model rolled back to: " | mail -s "Retraining Alert" team@example.com
`

**MLflow Tracking:**

`python
with mlflow.start_run(run_id=evolution_run_id):
    mlflow.log_metric("deployment/rollback", 1)
    mlflow.log_param("rollback_reason", "Production val accuracy below 70%")
    mlflow.set_tag("deployment_status", "rolled_back")
`

---

#### **End-to-End Automation (Putting It All Together)**

**Orchestration Script:** scripts/retraining_pipeline.py

**Usage:**

`ash
# Automated retraining pipeline (triggered by monitoring)
python scripts/retraining_pipeline.py \
  --trigger-report reports/monitoring/latest/summary.json \
  --predictions data/prepared/predictions/latest.csv \
  --mlflow-experiment retraining \
  --auto-deploy  # Deploy if all gates pass
`

**Pipeline Stages (with Checkpoints):**

`python
# scripts/retraining_pipeline.py

def run_retraining_pipeline(
    trigger_report_path,
    predictions_path,
    mlflow_experiment,
    auto_deploy=False
):
    """
    End-to-end retraining pipeline with gating
    """
    
    # Stage 1: Check trigger
    trigger_decision, trigger_reason = check_retraining_trigger(trigger_report_path)
    if not trigger_decision:
        logger.info("No retraining needed")
        return
    
    logger.info(f"Retraining triggered: {trigger_reason}")
    
    # Stage 2: Sample production data
    labeling_batch = generate_labeling_batch(
        predictions_path,
        n_random=50,
        n_uncertain=50
    )
    labeling_batch.to_csv('data/labeling_queue/labeling_batch.csv')
    logger.info("Labeling batch created. PAUSED: Waiting for human labels...")
    
    # CHECKPOINT: Wait for human labeling
    # (Manual step: labeler adds ground_truth_label column)
    
    input("Press Enter after labeling is complete...")
    
    # Stage 3: Validate labeled data
    labeled_data = pd.read_csv('data/labeling_queue/labeling_batch.csv')
    if not validate_labeling_quality(labeled_data):
        logger.error("Labeling quality check failed. Aborting.")
        return
    
    # Stage 4: Prepare mixed training data
    X_mixed, y_mixed = prepare_mixed_training_data(
        old_X='data/prepared/X_prepared_window.npy',
        old_y='data/prepared/y_prepared.npy',
        new_X=labeled_data['sensor_data'],
        new_y=labeled_data['ground_truth_label'],
        mix_ratio_old=0.3
    )
    
    # Stage 5: Retrain with MLflow tracking
    with mlflow.start_run(experiment_id=mlflow_experiment):
        model = retrain_with_frozen_layers(X_mixed, y_mixed)
        mlflow.keras.log_model(model, "candidate_model")
        evolution_id = mlflow.active_run().info.run_id
    
    # Stage 6: A/B test
    ab_results = compare_models(
        old_model_path='models/active/model.keras',
        new_model_path='models/evolution/candidate_model.keras',
        X_val=X_val_production,
        y_val=y_val_production
    )
    
    # Gating decision
    should_promote = (
        ab_results['accuracy_improvement'] > 0.05 and
        ab_results['new_accuracy'] > 0.70 and
        ab_results['train_val_accuracy'] > 0.85
    )
    
    if should_promote:
        logger.info(" All gates passed. Promoting new model.")
        
        if auto_deploy:
            # Stage 7: Deploy
            deploy_evolution(evolution_id, model)
            logger.info(f"Deployed: models/active -> evolution_{evolution_id}")
        else:
            logger.info("Manual deployment required (--auto-deploy not set)")
    else:
        logger.warning(" Gating failed. Rolling back.")
        rollback_deployment(reason=f"Accuracy improvement {ab_results['accuracy_improvement']:.2f} < 0.05")
`

---

**CITATIONS (Pair 14, Q2):**

1. **(docs/output_1801_2026-01-18.md, lines 2154-2166, Pair 08):** Evolution trigger detection framework: "Create scripts/evolution_trigger.py for automated trigger detection. EvolutionDecisionMaker class with methods for each trigger category (drift, performance, scheduled). Drift triggers: PSI > 0.25, >2 channels drifting, confidence drop, flip rate > 0.30."

2. **(docs/output_1801_2026-01-18.md, lines 4900-5000, Pair 11):** Evolution package structure and symlink deployment: "Flat namespace per evolution: evolution_packages/{id}/*.{json,yaml,keras}. Active deployment via symlink: models/active  evolution_20260125_163045. Atomic deployment prevents partial state. Rollback via symlink revert."

3. **(docs/thesis/FINE_TUNING_STRATEGY.md, lines 155-195):** Mixed-data fine-tuning configuration to prevent catastrophic forgetting: "Fine-tuning config: learning_rate 0.0001 (10x smaller), epochs 20, mixed data 30% old + 70% new, validation from production domain only. Gating: min_accuracy_improvement 0.05, min_absolute_accuracy 0.65."

4. **(docs/output_1801_2026-01-18.md, lines 90-152, Pair 01):** Labeling audit plan with stratified sampling: "Minimum audit: 3-5 representative sessions (300-500 windows total). Stratified sampling: temporal diversity, feature distribution coverage, activity class balance, uncertainty-based prioritization. Enables evaluation + safe retraining decisions."

5. **(scripts/post_inference_monitoring.py, lines 1352-1500):** MLflow tracking integration for monitoring and retraining: "MLflow logging with metrics tracking (monitoring/mean_confidence, monitoring/n_drift_channels, monitoring/overall_status). Artifact uploads (summary.json, drift_heatmap.png). Tags: monitoring_status, drift_detected."

6. **(papers/mlops_production/MLOps A Step Forward to Enterprise Machine Learning 2023.pdf, p.12-19):** Closed-loop retraining in MLOps systems: "Monitoring detects degradation  triggers evolution  validation confirms improvement  deployment updates system. Closed-loop control ensures models adapt to changing distributions while maintaining reproducibility and auditability." (Referenced in docs/output_1801_2026-01-18.md Pair 08)

---

**Summary Table (Q2):**

| **Stage** | **Action** | **Inputs** | **Outputs** | **Human Involvement** | **Duration** |
|-----------|------------|------------|-------------|----------------------|--------------|
| **1. Trigger** | Check drift/confidence metrics | drift_report.json, confidence_report.json | trigger_decision.json | None (automated) | 5 min |
| **2. Sample** | Stratified + uncertainty sampling | predictions.csv | labeling_batch.csv (100 windows) | None (automated) | 10 min |
| **3. Label** | Human annotation | labeling_batch.csv | labeled_batch.csv (with ground_truth_label) | Full (labeler) | 2-4 hours |
| **4. Validate** | Quality checks (completion, inter-rater) | labeled_batch.csv | validation_report.json | None (automated) | 5 min |
| **5. Retrain** | Mixed-data fine-tuning with frozen layers | X_mixed, y_mixed | candidate_model.keras | None (automated) | 30-60 min |
| **6. A/B Test** | Compare old vs new on production val | candidate_model, X_val, y_val | ab_test_report.json | None (automated) | 5 min |
| **7. Deploy/Rollback** | Symlink swap OR revert | ab_test_report, gating decision | models/active symlink update | Manual approval (optional) | 2 min |

**Total Time (End-to-End):** 3-5 hours (mostly human labeling in Stage 3)

**Total Cost:** ~4 hours human labor (~\-200 if outsourced labeling)

**Expected Benefit:** +10-20% accuracy improvement on production data, reduced drift alerts

---




## Pair 15 — Uncertainty & OOD methods

**Context:** Alternatives beyond softmax confidence for uncertainty quantification and out-of-distribution (OOD) detection: entropy, margin, temperature scaling/ECE, energy score, embedding-distance OOD. Need practical methods for HAR deployment.

---

### **Q1: Recommend 2-3 realistic uncertainty/OOD methods for HAR in our repo; explain how each plugs into inference.**

**Decision:** Implement **3-tier uncertainty framework** progressing from simple to advanced: **(Tier 1) Entropy + Margin** (already computable from softmax, zero cost, immediate deployment), **(Tier 2) Temperature Scaling** (post-hoc calibration using validation set, fits single parameter T, improves confidence reliability, one-time setup), **(Tier 3) Embedding Distance OOD** (Mahalanobis distance from training prototypes, detects novel patterns, optional Layer 4 in monitoring). All three are **label-free at inference time** (calibration set needed once for Tier 2, training embeddings for Tier 3). Reject energy score (requires logits not saved by current pipeline) and conformal prediction (too complex for thesis scope).

**Detailed Explanation:**

#### **Why Go Beyond Softmax Confidence?**

**The Core Problem:**  
Softmax confidence = max(p, p, ..., p) only tells us the top class probability. It **cannot distinguish** between:
- **Confident correct:** [0.95, 0.02, 0.01, 0.01, 0.01, 0.00]  Confidence 0.95, likely sitting
- **Confident wrong:** [0.95, 0.02, 0.01, 0.01, 0.01, 0.00]  Confidence 0.95, but actually walking (overconfident mistake)
- **Uncertain ambiguous:** [0.35, 0.33, 0.32, 0.00, 0.00, 0.00]  Confidence 0.35, 3-way tie (sitting vs hand_tapping vs writing)

**What We Need:**
1. **Uncertainty quantification:** How uncertain is the model? (entropy, margin)
2. **Calibration:** Does 80% confidence actually mean 80% accuracy? (temperature scaling, ECE)
3. **OOD detection:** Is this input fundamentally different from training? (embedding distance, energy score)

**Repo Evidence:**  
From docs/PIPELINE_DEEP_DIVE_opus.md lines 281-289:
"Better alternatives to top-1 probability: Entropy (overall uncertainty), Margin (ambiguity between top classes), Temperature Scaling (post-hoc calibration), Energy Score (OOD detection). Recommended: Entropy YES, Margin YES, Temperature Scaling YES."

---

#### **Tier 1: Entropy + Margin (Already Implemented)**

**Method 1A: Shannon Entropy**

**What It Measures:** Overall uncertainty of probability distribution

**Formula:**

H(p) = -sum of p_i times log(p_i) for i=1 to k

where p_i = softmax probability for class i, k = number of classes (6 in our case).

**Interpretation:**
- H = 0: Deterministic (one class has p=1, others p=0)
- H = log(k): Maximum uncertainty (uniform distribution, all classes equally likely)
- For 6 classes: H_max = log(6) approximately 1.79

**Example Scenarios:**

| Scenario | Probabilities | Confidence | Entropy | Interpretation |
|----------|---------------|------------|---------|----------------|
| **Confident sitting** | [0.95, 0.02, 0.01, 0.01, 0.01, 0.00] | 0.95 | 0.25 | Low uncertainty, confident |
| **Ambiguous 3-way** | [0.35, 0.33, 0.32, 0.00, 0.00, 0.00] | 0.35 | 1.58 | High uncertainty, 3-way confusion |
| **Uniform (worst)** | [0.17, 0.17, 0.17, 0.16, 0.16, 0.17] | 0.17 | 1.79 | Maximum uncertainty, no clue |
| **Binary confusion** | [0.50, 0.50, 0.00, 0.00, 0.00, 0.00] | 0.50 | 0.69 | Moderate uncertainty, 2-way tie |

**Decision Thresholds:**

From scripts/post_inference_monitoring.py lines 348-354 and docs output Pair 13:
- Entropy less than 1.0: Confident (flag as GREEN)
- Entropy 1.0-2.0: Moderate uncertainty (flag as YELLOW)
- Entropy greater than 2.0: High uncertainty (flag as RED, review)

---

**Method 1B: Margin (Top1 - Top2)**

**What It Measures:** Ambiguity between top two classes

**Formula:**

Margin = p_top1 - p_top2

where p_top1 = highest probability, p_top2 = second-highest.

**Interpretation:**
- Margin greater than 0.5: Clear winner (top class dominates)
- Margin 0.2-0.5: Moderate separation
- Margin less than 0.2: Ambiguous (two classes competing)
- Margin less than 0.10: Very ambiguous (near tie)

**Example Scenarios:**

| Scenario | Probabilities | Top1 | Top2 | Margin | Interpretation |
|----------|---------------|------|------|--------|----------------|
| **Clear sitting** | [0.84, 0.12, 0.02, 0.01, 0.01, 0.00] | 0.84 | 0.12 | 0.72 | Clear decision |
| **Sitting vs standing** | [0.55, 0.40, 0.03, 0.01, 0.01, 0.00] | 0.55 | 0.40 | 0.15 | Ambiguous |
| **Perfect tie** | [0.50, 0.50, 0.00, 0.00, 0.00, 0.00] | 0.50 | 0.50 | 0.00 | Cannot decide |

**Why Margin Complements Entropy:**
- **Entropy detects multi-way confusion:** [0.25, 0.25, 0.25, 0.25, 0.00, 0.00]  H = 1.39 (high entropy), Margin = 0.00 (4-way tie)
- **Margin detects binary confusion:** [0.50, 0.50, 0.00, 0.00, 0.00, 0.00]  H = 0.69 (moderate entropy), Margin = 0.00 (reveals tie)

**Decision Thresholds:**

From docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md lines 278-290:
- Margin greater than 0.30: Confident (GREEN)
- Margin 0.10-0.30: Moderate (YELLOW)
- Margin less than 0.10: Ambiguous (RED, likely wrong or uncertain)

---

**How to Integrate into Inference (Already Done):**

**Location:** src/run_inference.py (predictions CSV already includes entropy and margin)

**Existing Columns:**
- confidence: Max softmax probability (already present)
- entropy: Shannon entropy (can be added if missing)
- margin: Top1 - Top2 difference (can be added if missing)

**Computation:**

`python
# In src/run_inference.py, after softmax computation

import numpy as np

def compute_uncertainty_metrics(probs):
    \"\"\"
    Compute entropy and margin from softmax probabilities.
    
    Args:
        probs: (n_windows, n_classes) array
    
    Returns:
        entropy: (n_windows,) array
        margin: (n_windows,) array
    \"\"\"
    # Entropy
    epsilon = 1e-10  # Avoid log(0)
    entropy = -np.sum(probs * np.log(probs + epsilon), axis=1)
    
    # Margin
    sorted_probs = np.sort(probs, axis=1)
    top1 = sorted_probs[:, -1]  # Highest
    top2 = sorted_probs[:, -2]  # Second-highest
    margin = top1 - top2
    
    return entropy, margin

# Usage in inference
probs = model.predict(X_production)  # (n_windows, 6)
confidence = np.max(probs, axis=1)
entropy, margin = compute_uncertainty_metrics(probs)

# Add to CSV output
df['confidence'] = confidence
df['entropy'] = entropy
df['margin'] = margin
df['is_uncertain'] = (confidence < 0.50) | (entropy > 2.0) | (margin < 0.10)
`

**Cost:** Virtually zero (already have probabilities, just arithmetic)

---

#### **Tier 2: Temperature Scaling (Post-Hoc Calibration)**

**What It Is:** A single-parameter post-processing method that makes confidence scores match accuracy.

**Problem:** Neural networks are often **overconfident** (predict 90% confidence but only 70% accuracy).

**Solution:** Scale logits by temperature T before softmax:

p_i_calibrated = exp(z_i / T) / sum of exp(z_j / T) for all j

where:
- z_i = logit (pre-softmax activation) for class i
- T = temperature parameter (T greater than 1 reduces confidence, T less than 1 increases confidence)
- T = 1 = no change (standard softmax)

**How It Works:**

**Step 1: Fit T on Validation Set (One-Time Setup)**

`python
# scripts/calibration/fit_temperature.py

import numpy as np
from scipy.optimize import minimize
from tensorflow.keras.models import load_model

def compute_nll_loss(T, logits, labels):
    \"\"\"
    Negative log-likelihood loss for temperature T.
    \"\"\"
    scaled_logits = logits / T
    probs = np.exp(scaled_logits) / np.sum(np.exp(scaled_logits), axis=1, keepdims=True)
    # Cross-entropy loss
    n_samples = len(labels)
    nll = -np.sum(np.log(probs[np.arange(n_samples), labels] + 1e-10)) / n_samples
    return nll

def fit_temperature(model_path, X_val, y_val):
    \"\"\"
    Fit temperature parameter on validation set.
    
    Args:
        model_path: Path to trained model
        X_val: Validation features (n_samples, 200, 6)
        y_val: Validation labels (n_samples,)
    
    Returns:
        T_optimal: Best temperature value
    \"\"\"
    model = load_model(model_path)
    
    # Extract logits (before final softmax activation)
    # Assuming model has a 'dense_head' layer as final layer
    logit_model = tf.keras.Model(
        inputs=model.input,
        outputs=model.get_layer('dense_head').output  # Pre-activation logits
    )
    
    logits = logit_model.predict(X_val, verbose=0)
    
    # Optimize T to minimize NLL loss
    result = minimize(
        compute_nll_loss,
        x0=1.0,  # Start with T=1 (no scaling)
        args=(logits, y_val),
        method='L-BFGS-B',
        bounds=[(0.1, 10.0)]  # T must be positive
    )
    
    T_optimal = result.x[0]
    
    # Validate improvement
    baseline_nll = compute_nll_loss(1.0, logits, y_val)
    calibrated_nll = compute_nll_loss(T_optimal, logits, y_val)
    
    print(f\"Baseline NLL (T=1.0): {baseline_nll:.4f}\")
    print(f\"Calibrated NLL (T={T_optimal:.2f}): {calibrated_nll:.4f}\")
    print(f\"Improvement: {baseline_nll - calibrated_nll:.4f}\")
    
    return T_optimal

# Usage
T = fit_temperature(
    model_path='models/active/model.keras',
    X_val=X_val,
    y_val=y_val
)

# Save T to config
import json
with open('models/active/calibration_config.json', 'w') as f:
    json.dump({'temperature': float(T)}, f)
`

**Step 2: Apply T at Inference Time**

`python
# In src/run_inference.py

import json

# Load temperature
with open('models/active/calibration_config.json', 'r') as f:
    T = json.load(f)['temperature']

# Get logits from model (NOT softmax probs)
logit_model = tf.keras.Model(
    inputs=model.input,
    outputs=model.get_layer('dense_head').output
)

logits = logit_model.predict(X_production)

# Apply temperature scaling
calibrated_probs = np.exp(logits / T) / np.sum(np.exp(logits / T), axis=1, keepdims=True)

# Use calibrated_probs instead of raw softmax
df['confidence'] = np.max(calibrated_probs, axis=1)
df['predicted_class'] = np.argmax(calibrated_probs, axis=1)
`

**Expected Improvement:**

**Before Calibration (T=1.0):**
- Confidence bins: [70-80%]  Accuracy 65% (overconfident by 5-15%)
- Expected Calibration Error (ECE): 0.12 (poor calibration)

**After Calibration (T=1.5 typical):**
- Confidence bins: [70-80%]  Accuracy 73% (well-calibrated within 7%)
- Expected Calibration Error (ECE): 0.05 (good calibration)

**Why It Works:** Temperature scaling preserves ranking (argmax unchanged), only adjusts confidence magnitudes  model still predicts same classes, but confidence scores are more trustworthy.

**Repo Evidence:**  
From docs/PIPELINE_DEEP_DIVE_opus.md lines 313-320:
\"Temperature Scaling (Calibration): Fit T on validation set (once), then apply in production. Makes confidence = actual accuracy. Requirement: Fit T offline using evaluate_predictions.py.\"

**Cost:** Negligible at inference (just divide logits by T before softmax), one-time setup (fit T on validation set, 5-10 minutes)

---

#### **Tier 3: Embedding Distance OOD (Mahalanobis Distance)**

**What It Is:** Measure how far production embeddings are from training distribution in latent space.

**Why It's Powerful:** Raw sensor features (Ax, Ay, Az, Gx, Gy, Gz) may look similar, but learned embeddings capture high-level patterns  detect semantic shift even when sensor stats pass drift tests.

**Method: Mahalanobis Distance**

**Formula:**

D_M(x) = sqrt of [(x - mu) transposed times inverse_cov times (x - mu)]

where:
- x = production embedding (extracted from LSTM layer)
- mu = mean training embedding
- inverse_cov = inverse covariance matrix of training embeddings

**Interpretation:**
- D_M less than 2: In-distribution (typical training pattern)
- 2 less than D_M less than 4: Borderline (monitor closely)
- D_M greater than 4: Out-of-distribution (novel pattern, likely to fail)

**How to Compute:**

**Step 1: Extract Baseline Embeddings (One-Time Setup)**

`python
# scripts/calibration/extract_training_embeddings.py

import numpy as np
from tensorflow.keras.models import Model, load_model

def extract_embeddings(model_path, X_train, layer_name='bidirectional'):
    \"\"\"
    Extract embeddings from intermediate layer.
    
    Args:
        model_path: Path to trained model
        X_train: Training data (n_samples, 200, 6)
        layer_name: Name of embedding layer (default: 'bidirectional' LSTM)
    
    Returns:
        embeddings: (n_samples, embedding_dim) array
    \"\"\"
    model = load_model(model_path)
    
    # Find target layer
    embedding_layer = None
    for layer in model.layers:
        if layer_name.lower() in layer.name.lower():
            embedding_layer = layer
            break
    
    if embedding_layer is None:
        raise ValueError(f\"Layer '{layer_name}' not found\")
    
    # Create embedding extractor
    embedding_model = Model(
        inputs=model.input,
        outputs=embedding_layer.output
    )
    
    # Extract in batches
    batch_size = 256
    embeddings = []
    for i in range(0, len(X_train), batch_size):
        batch = X_train[i:i+batch_size]
        batch_emb = embedding_model.predict(batch, verbose=0)
        
        # If LSTM output is (n_samples, timesteps, features), pool over time
        if len(batch_emb.shape) == 3:
            batch_emb = np.mean(batch_emb, axis=1)  # Global average pooling
        
        embeddings.append(batch_emb)
    
    return np.vstack(embeddings)

# Usage
embeddings = extract_embeddings(
    model_path='models/active/model.keras',
    X_train=X_train,
    layer_name='bidirectional'
)

# Compute statistics
mean_emb = np.mean(embeddings, axis=0)
cov_emb = np.cov(embeddings, rowvar=False)

# Save baseline
np.savez(
    'data/prepared/baseline_embeddings.npz',
    mean=mean_emb,
    cov=cov_emb,
    n_samples=len(embeddings)
)
`

**Step 2: Compute Mahalanobis Distance at Inference**

`python
# In scripts/post_inference_monitoring.py (already exists as Layer 4)

from scipy.spatial.distance import mahalanobis

def compute_mahalanobis_ood(prod_embeddings, baseline_path):
    \"\"\"
    Compute Mahalanobis distance for production embeddings.
    
    Args:
        prod_embeddings: (n_samples, embedding_dim)
        baseline_path: Path to baseline_embeddings.npz
    
    Returns:
        mahal_distances: (n_samples,) array of distances
    \"\"\"
    baseline = np.load(baseline_path)
    mean_emb = baseline['mean']
    cov_emb = baseline['cov']
    
    # Regularize covariance (prevent singular matrix)
    cov_inv = np.linalg.pinv(cov_emb + 1e-6 * np.eye(cov_emb.shape[0]))
    
    # Compute distances
    distances = []
    for emb in prod_embeddings:
        diff = emb - mean_emb
        dist = np.sqrt(diff @ cov_inv @ diff)
        distances.append(dist)
    
    return np.array(distances)

# Usage in monitoring
prod_emb = extract_embeddings(model_path, X_production)
mahal_dist = compute_mahalanobis_ood(prod_emb, 'data/prepared/baseline_embeddings.npz')

# Flag OOD samples
ood_threshold = 4.0
ood_mask = mahal_dist > ood_threshold
ood_ratio = np.mean(ood_mask)

print(f\"OOD samples: {100*ood_ratio:.1f}% (Mahalanobis distance > {ood_threshold})\")
`

**Repo Evidence:**  
From scripts/post_inference_monitoring.py lines 956-1050:
\"EmbeddingDriftDetector extracts embeddings from model intermediate layer (bidirectional LSTM), computes mean/covariance shift, calculates Mahalanobis distance. One of strongest label-free evaluation arguments for deep HAR.\"

**Cost:** Moderate (requires model forward pass for embeddings, matrix inversion for covariance), but optional (use only if Layer 1-3 monitoring insufficient)

---

#### **Method Comparison Table**

| Method | What It Detects | Data Required | Computation Cost | Integration Effort | When to Use |
|--------|-----------------|---------------|------------------|-------------------|-------------|
| **Entropy** | Multi-way confusion (uniform probs) | Softmax probs (free) | Negligible | Already done | Always (baseline) |
| **Margin** | Binary confusion (top2 close) | Softmax probs (free) | Negligible | Already done | Always (complements entropy) |
| **Temperature Scaling** | Overconfidence / underconfidence | Validation set (once) | Negligible at inference | 1-2 days (fit T, modify inference) | When calibration matters (high-stakes decisions) |
| **Mahalanobis Distance** | Semantic OOD (novel patterns) | Training embeddings (once) | Moderate (embedding extraction) | 2-3 days (extract embeddings, integrate) | When raw drift tests insufficient (Layer 4) |
| **Energy Score** | OOD detection (similar to Mahalanobis) | Training logits (once) | Negligible | NOT RECOMMENDED (logits not saved by current pipeline) | Skip (requires pipeline refactor) |
| **Conformal Prediction** | Prediction sets with coverage guarantees | Calibration set (labeled) | High (quantile computation) | NOT RECOMMENDED (too complex for thesis scope) | Skip (research topic, not production-ready) |

---

**CITATIONS (Pair 15, Q1):**

1. **(docs/PIPELINE_DEEP_DIVE_opus.md, lines 281-289):** Uncertainty method comparison: \"Better alternatives to top-1 probability: Entropy H(p) = -Σ p log(p) (overall uncertainty), Margin (p_top1 - p_top2) for ambiguity, Temperature Scaling p' = softmax(z/T) for post-hoc calibration (tune T on val set), Energy Score E = -Tlog Σ exp(z/T) for OOD detection. Recommended: Entropy YES, Margin YES, Temperature Scaling YES.\"

2. **(scripts/post_inference_monitoring.py, lines 348-354, 956-1050):** Existing entropy and embedding drift implementations: \"ConfidenceAnalyzer computes entropy H(p) = -sum(p_i times log(p_i)) with threshold >2.0 for high uncertainty. EmbeddingDriftDetector (Layer 4) extracts embeddings from bidirectional LSTM layer, computes Mahalanobis distance from training baseline, detects semantic drift in latent space.\"

3. **(docs/output_1801_2026-01-18.md, lines 1793-1852, Pair 07):** ECE calibration metric: \"Calibration (ECE) = Σ |acc_i - conf_i|  (n_i/N) measures confidence reliability for overconfidence detection. Implementation: src/evaluate_predictions.py::ClassificationEvaluator::compute_calibration() uses 10 confidence bins. Well-calibrated: ECE < 0.05. Poor calibration: ECE > 0.15.\"

4. **(docs/thesis/UNLABELED_EVALUATION.md, lines 63-75, 414-420):** Uncertainty metrics for unlabeled evaluation: \"Confidence & uncertainty metrics from softmax outputs: Max Probability (confidence), Entropy (overall uncertainty), Margin (top1 - top2 difference), Energy Score E = -log Σ exp(z_i) for OOD detection (requires logits). Papers used: NeurIPS-2020-energy-based-out-of-distribution-detection-Paper.pdf for Energy Score.\"

5. **(papers needs to read/When Does Optimizing a Proper Loss Yield Calibration.pdf, p.12-15):** Calibration theory for neural networks: \"Confidence, entropy, and margin provide complementary uncertainty signals. Systems should track all three to capture different failure modes (overconfidence vs confusion). Temperature scaling is a post-hoc calibration method that improves reliability of confidence scores without retraining model.\" (Referenced in docs/output_1801_2026-01-18.md line 1744)

6. **(docs/PIPELINE_DIVE.md, lines 260-280, 393-400):** Implementation status and next steps: \"Entropy and margin computed in post-inference monitoring (scripts/post_inference_monitoring.py:342, :348, :354). Embedding drift available as optional layer (:955). Temperature scaling: add calibration step in src/evaluate_predictions.py or new src/calibration.py. Next improvements: Add confidence calibration (temperature scaling), add energy score or embedding distance OOD signals.\"

---

**Summary Table (Q1):**

| **Tier** | **Methods** | **Measures** | **Data Required** | **Cost** | **Implementation** |
|----------|-------------|--------------|-------------------|----------|-------------------|
| **1 (Deployed)** | Entropy + Margin | Multi-way confusion, binary ambiguity | Softmax probs (free) | Zero | Already in scripts/post_inference_monitoring.py |
| **2 (Recommended)** | Temperature Scaling | Calibration (confidence reliability) | Validation set with labels (once) | Negligible | Fit T offline, apply in inference (1-2 days) |
| **3 (Optional)** | Mahalanobis Distance | Semantic OOD (novel patterns) | Training embeddings (once) | Moderate | Extract embeddings, compute distance (2-3 days) |

---




### **Q2: Define reference data, logging, and monitoring for these uncertainty/OOD methods—what baseline to store, what to log, how to monitor drift.**

**Decision:** Store **three baselines** for uncertainty methods: **(Baseline 1) calibration_config.json** containing temperature parameter T fitted on validation set (used by Tier 2), **(Baseline 2) baseline_embeddings.npz** containing training embedding statistics (mean, covariance, sample embeddings) for Mahalanobis distance (Tier 3), **(Baseline 3) training_confidence_stats.json** containing training-time distributions of entropy, margin, confidence for drift detection. **Logging strategy:** Add entropy, margin, Mahalanobis distance to predictions CSV (per-window level) and compute aggregate statistics (mean, std, percentiles) in batch reports logged to MLflow. **Monitoring approach:** Track **drift of uncertainty metrics themselves** using same 4-layer framework (Layer 1: confidence distribution shift, Layer 2: ECE degradation, Layer 3: OOD ratio increase, Layer 4: embedding drift), with alerts triggered when calibration degrades (ECE >0.15 after being <0.05) or OOD ratio exceeds 20%.

**Detailed Explanation:**

#### **Baseline 1: Calibration Configuration (Temperature Parameter)**

**What to Store:** Single temperature parameter T fitted on validation set.

**File:** models/active/calibration_config.json

**Schema:**

`json
{
  \"temperature\": 1.47,
  \"fit_date\": \"2025-01-15\",
  \"val_samples\": 2000,
  \"val_nll_before\": 0.82,
  \"val_nll_after\": 0.54,
  \"val_ece_before\": 0.12,
  \"val_ece_after\": 0.05,
  \"model_version\": \"v2025_01_10\",
  \"notes\": \"Fitted on 2000 validation samples with stratified class distribution\"
}
`

**Why These Fields:**
- 	emperature: Core parameter (required for inference)
- it_date: Traceability (when was T computed)
- al_samples: Sample size (ensure sufficient data for reliable T)
- al_nll_before/after: Validation improvement (NLL = negative log-likelihood, lower is better)
- al_ece_before/after: Calibration improvement (ECE = Expected Calibration Error, <0.05 is good)
- model_version: Which model this T belongs to (T must be refit if model retrained)
- 
otes: Human-readable context

**How to Generate:**

`python
# scripts/calibration/fit_temperature.py

import json
from datetime import datetime
from scipy.optimize import minimize
import numpy as np

def fit_and_save_temperature(model, X_val, y_val, output_path):
    \"\"\"
    Fit temperature on validation set and save config.
    \"\"\"
    # Extract logits
    logit_model = tf.keras.Model(
        inputs=model.input,
        outputs=model.get_layer('dense_head').output
    )
    logits = logit_model.predict(X_val, verbose=0)
    
    # Compute baseline metrics (T=1.0)
    baseline_nll = compute_nll_loss(1.0, logits, y_val)
    baseline_ece = compute_ece(logits, y_val, T=1.0)
    
    # Optimize T
    result = minimize(
        compute_nll_loss,
        x0=1.0,
        args=(logits, y_val),
        method='L-BFGS-B',
        bounds=[(0.1, 10.0)]
    )
    T_optimal = result.x[0]
    
    # Compute calibrated metrics
    calibrated_nll = compute_nll_loss(T_optimal, logits, y_val)
    calibrated_ece = compute_ece(logits, y_val, T=T_optimal)
    
    # Save config
    config = {
        'temperature': float(T_optimal),
        'fit_date': datetime.now().strftime('%Y-%m-%d'),
        'val_samples': len(y_val),
        'val_nll_before': float(baseline_nll),
        'val_nll_after': float(calibrated_nll),
        'val_ece_before': float(baseline_ece),
        'val_ece_after': float(calibrated_ece),
        'model_version': model.name,
        'notes': f'Fitted on {len(y_val)} validation samples'
    }
    
    with open(output_path, 'w') as f:
        json.dump(config, f, indent=2)
    
    print(f\"Temperature T={T_optimal:.2f} saved to {output_path}\")
    print(f\"ECE improvement: {baseline_ece:.4f} -> {calibrated_ece:.4f}\")
    
    return config

# Usage
config = fit_and_save_temperature(
    model=model,
    X_val=X_val,
    y_val=y_val,
    output_path='models/active/calibration_config.json'
)
`

**When to Regenerate:**
- After model retraining (T is model-specific)
- If ECE degrades >0.05 in monitoring (recalibration needed)
- If validation set distribution changes significantly

---

#### **Baseline 2: Training Embeddings (For Mahalanobis Distance)**

**What to Store:** Mean embedding, covariance matrix, sample embeddings from training set.

**File:** data/prepared/baseline_embeddings.npz

**Schema:**

`python
# NumPy compressed archive
baseline_embeddings.npz:
  - mean: (embedding_dim,) array, mean of training embeddings
  - cov: (embedding_dim, embedding_dim) array, covariance matrix
  - samples: (n_samples, embedding_dim) array, actual training embeddings (optional, for visualization)
  - layer_name: string, which layer embeddings came from (e.g., 'bidirectional')
  - n_training_samples: int, total training samples used
  - extraction_date: string, when baseline was created
`

**Example Content:**

`python
import numpy as np

baseline = np.load('data/prepared/baseline_embeddings.npz')

print(f\"Mean embedding shape: {baseline['mean'].shape}\")  # (128,)
print(f\"Covariance shape: {baseline['cov'].shape}\")       # (128, 128)
print(f\"Sample embeddings: {baseline['samples'].shape}\")  # (5000, 128)
print(f\"Layer: {baseline['layer_name']}\")                 # 'bidirectional'
print(f\"Training samples: {baseline['n_training_samples']}\")  # 5000
print(f\"Extraction date: {baseline['extraction_date']}\")  # '2025-01-10'
`

**How to Generate:**

`python
# scripts/calibration/extract_training_embeddings.py

import numpy as np
from datetime import datetime
from tensorflow.keras.models import Model, load_model

def extract_and_save_baseline_embeddings(model_path, X_train, output_path, layer_name='bidirectional'):
    \"\"\"
    Extract training embeddings and save baseline statistics.
    \"\"\"
    model = load_model(model_path)
    
    # Find embedding layer
    embedding_layer = None
    for layer in model.layers:
        if layer_name.lower() in layer.name.lower():
            embedding_layer = layer
            break
    
    if embedding_layer is None:
        raise ValueError(f\"Layer '{layer_name}' not found in model\")
    
    # Create embedding extractor
    embedding_model = Model(
        inputs=model.input,
        outputs=embedding_layer.output
    )
    
    print(f\"Extracting embeddings from layer: {embedding_layer.name}\")
    
    # Extract embeddings in batches
    batch_size = 256
    embeddings = []
    for i in range(0, len(X_train), batch_size):
        batch = X_train[i:i+batch_size]
        batch_emb = embedding_model.predict(batch, verbose=0)
        
        # If LSTM output is (n_samples, timesteps, features), pool over time
        if len(batch_emb.shape) == 3:
            batch_emb = np.mean(batch_emb, axis=1)  # Global average pooling
        
        embeddings.append(batch_emb)
        
        if (i // batch_size) % 10 == 0:
            print(f\"Processed {i}/{len(X_train)} samples...\")
    
    embeddings = np.vstack(embeddings)
    
    # Compute statistics
    mean_emb = np.mean(embeddings, axis=0)
    cov_emb = np.cov(embeddings, rowvar=False)
    
    # Save baseline
    np.savez(
        output_path,
        mean=mean_emb,
        cov=cov_emb,
        samples=embeddings,  # Save full samples for debugging
        layer_name=embedding_layer.name,
        n_training_samples=len(X_train),
        extraction_date=datetime.now().strftime('%Y-%m-%d')
    )
    
    print(f\"Baseline embeddings saved to {output_path}\")
    print(f\"Embedding dimension: {mean_emb.shape[0]}\")
    print(f\"Training samples: {len(embeddings)}\")
    
    return embeddings

# Usage
embeddings = extract_and_save_baseline_embeddings(
    model_path='models/active/model.keras',
    X_train=X_train,
    output_path='data/prepared/baseline_embeddings.npz',
    layer_name='bidirectional'
)
`

**When to Regenerate:**
- After model retraining (embeddings come from model)
- If embedding layer architecture changes
- If training data distribution changes significantly

**Storage Size:** For 5000 training samples with 128-dim embeddings: ~2.5 MB (samples) + 128 KB (mean/cov) = ~2.6 MB total

---

#### **Baseline 3: Training Confidence Statistics (For Drift Detection)**

**What to Store:** Distributions of entropy, margin, confidence from training set.

**File:** data/prepared/training_confidence_stats.json

**Schema:**

`json
{
  \"confidence\": {
    \"mean\": 0.78,
    \"std\": 0.15,
    \"p25\": 0.70,
    \"p50\": 0.82,
    \"p75\": 0.90,
    \"histogram\": {
      \"bins\": [0.0, 0.1, 0.2, ..., 1.0],
      \"counts\": [5, 12, 50, ...]
    }
  },
  \"entropy\": {
    \"mean\": 0.62,
    \"std\": 0.48,
    \"p25\": 0.25,
    \"p50\": 0.50,
    \"p75\": 0.85,
    \"histogram\": {
      \"bins\": [0.0, 0.2, 0.4, ..., 2.0],
      \"counts\": [120, 300, 250, ...]
    }
  },
  \"margin\": {
    \"mean\": 0.58,
    \"std\": 0.22,
    \"p25\": 0.45,
    \"p50\": 0.62,
    \"p75\": 0.78,
    \"histogram\": {
      \"bins\": [0.0, 0.1, 0.2, ..., 1.0],
      \"counts\": [10, 25, 80, ...]
    }
  },
  \"n_training_samples\": 5000,
  \"creation_date\": \"2025-01-10\",
  \"model_version\": \"v2025_01_10\"
}
`

**How to Generate:**

`python
# scripts/calibration/compute_training_confidence_stats.py

import json
import numpy as np
from tensorflow.keras.models import load_model

def compute_training_confidence_stats(model_path, X_train, output_path):
    \"\"\"
    Compute confidence statistics from training set predictions.
    \"\"\"
    model = load_model(model_path)
    
    # Predict on training set
    probs = model.predict(X_train, batch_size=256, verbose=0)
    
    # Compute metrics
    confidence = np.max(probs, axis=1)
    
    epsilon = 1e-10
    entropy = -np.sum(probs * np.log(probs + epsilon), axis=1)
    
    sorted_probs = np.sort(probs, axis=1)
    margin = sorted_probs[:, -1] - sorted_probs[:, -2]
    
    # Compute statistics
    def compute_stats(arr, n_bins=20):
        return {
            'mean': float(np.mean(arr)),
            'std': float(np.std(arr)),
            'p25': float(np.percentile(arr, 25)),
            'p50': float(np.percentile(arr, 50)),
            'p75': float(np.percentile(arr, 75)),
            'histogram': {
                'bins': np.histogram(arr, bins=n_bins)[1].tolist(),
                'counts': np.histogram(arr, bins=n_bins)[0].tolist()
            }
        }
    
    stats = {
        'confidence': compute_stats(confidence),
        'entropy': compute_stats(entropy),
        'margin': compute_stats(margin),
        'n_training_samples': len(X_train),
        'creation_date': datetime.now().strftime('%Y-%m-%d'),
        'model_version': model.name
    }
    
    # Save
    with open(output_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f\"Training confidence stats saved to {output_path}\")
    print(f\"Mean confidence: {stats['confidence']['mean']:.3f}  {stats['confidence']['std']:.3f}\")
    print(f\"Mean entropy: {stats['entropy']['mean']:.3f}  {stats['entropy']['std']:.3f}\")
    print(f\"Mean margin: {stats['margin']['mean']:.3f}  {stats['margin']['std']:.3f}\")
    
    return stats

# Usage
stats = compute_training_confidence_stats(
    model_path='models/active/model.keras',
    X_train=X_train,
    output_path='data/prepared/training_confidence_stats.json'
)
`

**Why This Baseline:** Enables drift detection of uncertainty metrics  if production entropy suddenly higher than training, model is more uncertain (possible distribution shift).

---

#### **Logging Strategy: What to Log and Where**

**Per-Window Level (Predictions CSV):**

Add uncertainty columns to src/run_inference.py output:

`python
# src/run_inference.py

# Existing columns
df['confidence'] = confidence
df['predicted_class'] = predicted_class
df['true_label'] = true_label  # If available

# NEW: Add uncertainty metrics
df['entropy'] = entropy
df['margin'] = margin
df['mahalanobis_distance'] = mahal_dist  # If Tier 3 enabled
df['is_uncertain'] = (entropy > 2.0) | (margin < 0.10)
df['is_ood'] = (mahal_dist > 4.0) if mahal_dist is not None else False

# Save to CSV
df.to_csv(f'outputs/predictions_{run_id}.csv', index=False)
`

**Example CSV Row:**

| window_id | predicted_class | confidence | entropy | margin | mahalanobis_distance | is_uncertain | is_ood |
|-----------|-----------------|------------|---------|--------|----------------------|-------------|--------|
| 42 | sitting | 0.84 | 0.52 | 0.68 | 1.8 | False | False |
| 43 | walking | 0.56 | 1.42 | 0.20 | 2.3 | True | False |
| 44 | unknown | 0.35 | 1.78 | 0.05 | 5.1 | True | True |

---

**Aggregate Level (Batch Reports to MLflow):**

Compute batch-level statistics and log to MLflow:

`python
# scripts/post_inference_monitoring.py

import mlflow

def log_uncertainty_metrics_to_mlflow(df, run_id):
    \"\"\"
    Log aggregate uncertainty metrics to MLflow.
    \"\"\"
    with mlflow.start_run(run_name=f\"uncertainty_report_{run_id}\"):
        # Scalar metrics
        mlflow.log_metric('mean_confidence', df['confidence'].mean())
        mlflow.log_metric('mean_entropy', df['entropy'].mean())
        mlflow.log_metric('mean_margin', df['margin'].mean())
        
        # Ratios
        mlflow.log_metric('uncertain_ratio', df['is_uncertain'].mean())
        mlflow.log_metric('ood_ratio', df['is_ood'].mean() if 'is_ood' in df else 0.0)
        
        # Percentiles
        mlflow.log_metric('entropy_p75', df['entropy'].quantile(0.75))
        mlflow.log_metric('entropy_p95', df['entropy'].quantile(0.95))
        mlflow.log_metric('margin_p25', df['margin'].quantile(0.25))
        
        # Distributions (as artifacts)
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        axes[0].hist(df['confidence'], bins=20, alpha=0.7, edgecolor='black')
        axes[0].set_title('Confidence Distribution')
        axes[0].set_xlabel('Confidence')
        
        axes[1].hist(df['entropy'], bins=20, alpha=0.7, edgecolor='black', color='orange')
        axes[1].set_title('Entropy Distribution')
        axes[1].set_xlabel('Entropy')
        
        axes[2].hist(df['margin'], bins=20, alpha=0.7, edgecolor='black', color='green')
        axes[2].set_title('Margin Distribution')
        axes[2].set_xlabel('Margin')
        
        plt.tight_layout()
        mlflow.log_figure(fig, 'uncertainty_distributions.png')
        plt.close()
        
        # Log CSV artifact
        mlflow.log_artifact(f'outputs/predictions_{run_id}.csv')
`

**MLflow UI View:**

`
Run: uncertainty_report_20250115_120000
Metrics:
  - mean_confidence: 0.72
  - mean_entropy: 0.95
  - mean_margin: 0.48
  - uncertain_ratio: 0.15 (15% of windows flagged as uncertain)
  - ood_ratio: 0.03 (3% of windows flagged as OOD)
  - entropy_p95: 1.82
Artifacts:
  - uncertainty_distributions.png
  - predictions_20250115_120000.csv
`

---

#### **Monitoring Strategy: How to Detect Drift of Uncertainty Metrics**

Use **same 4-layer framework** applied to uncertainty metrics themselves:

**Layer 1: Confidence Distribution Shift**

**What to Monitor:** Mean confidence, entropy, margin compared to training baseline.

**Implementation:**

`python
# scripts/post_inference_monitoring.py

def monitor_confidence_drift(prod_df, baseline_stats_path):
    \"\"\"
    Monitor drift of confidence metrics using training baseline.
    \"\"\"
    # Load baseline
    with open(baseline_stats_path, 'r') as f:
        baseline = json.load(f)
    
    # Compute production stats
    prod_confidence_mean = prod_df['confidence'].mean()
    prod_entropy_mean = prod_df['entropy'].mean()
    prod_margin_mean = prod_df['margin'].mean()
    
    # Compare to baseline
    confidence_drop = baseline['confidence']['mean'] - prod_confidence_mean
    entropy_increase = prod_entropy_mean - baseline['entropy']['mean']
    margin_drop = baseline['margin']['mean'] - prod_margin_mean
    
    # Alert thresholds
    alerts = []
    
    if confidence_drop > 0.15:
        alerts.append(f\"Confidence dropped by {confidence_drop:.3f} (baseline {baseline['confidence']['mean']:.3f} -> prod {prod_confidence_mean:.3f})\")
    
    if entropy_increase > 0.50:
        alerts.append(f\"Entropy increased by {entropy_increase:.3f} (baseline {baseline['entropy']['mean']:.3f} -> prod {prod_entropy_mean:.3f})\")
    
    if margin_drop > 0.15:
        alerts.append(f\"Margin dropped by {margin_drop:.3f} (baseline {baseline['margin']['mean']:.3f} -> prod {prod_margin_mean:.3f})\")
    
    return {
        'confidence_drop': confidence_drop,
        'entropy_increase': entropy_increase,
        'margin_drop': margin_drop,
        'alerts': alerts
    }
`

**Alert Triggers:**
- Confidence drop >0.15 (mean confidence drops from 0.78  0.63)
- Entropy increase >0.50 (mean entropy rises from 0.62  1.12)
- Margin drop >0.15 (mean margin drops from 0.58  0.43)

**What It Means:** Model is less confident / more confused  likely distribution shift.

---

**Layer 2: Calibration Drift (ECE Degradation)**

**What to Monitor:** Expected Calibration Error (ECE) increasing over time.

**Implementation:**

`python
# scripts/post_inference_monitoring.py

def monitor_calibration_drift(prod_df, calibration_config_path):
    \"\"\"
    Monitor ECE degradation over time.
    \"\"\"
    # Load baseline ECE (from calibration_config.json)
    with open(calibration_config_path, 'r') as f:
        config = json.load(f)
    
    baseline_ece = config['val_ece_after']  # ECE after temperature scaling
    
    # Compute production ECE (requires labels)
    # If labels available from human audit:
    if 'true_label' in prod_df.columns:
        prod_ece = compute_ece(prod_df['confidence'].values, prod_df['true_label'].values)
        
        ece_degradation = prod_ece - baseline_ece
        
        if ece_degradation > 0.05:
            alert = f\"ECE degraded by {ece_degradation:.4f} (baseline {baseline_ece:.4f} -> prod {prod_ece:.4f}). Recalibration recommended.\"
            return {'prod_ece': prod_ece, 'alert': alert}
    
    return {'prod_ece': None, 'alert': 'No labels available for ECE computation'}

def compute_ece(confidence, labels, n_bins=10):
    \"\"\"
    Compute Expected Calibration Error.
    \"\"\"
    bins = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(confidence, bins) - 1
    
    ece = 0.0
    for i in range(n_bins):
        mask = (bin_indices == i)
        if mask.sum() > 0:
            bin_confidence = confidence[mask].mean()
            bin_accuracy = (np.argmax(predictions[mask], axis=1) == labels[mask]).mean()
            bin_weight = mask.sum() / len(confidence)
            ece += bin_weight * abs(bin_accuracy - bin_confidence)
    
    return ece
`

**Alert Trigger:** ECE increases by >0.05 (e.g., from 0.05  0.10 or higher)

**What It Means:** Confidence scores no longer match accuracy  need to refit temperature parameter T.

---

**Layer 3: OOD Ratio Drift**

**What to Monitor:** Percentage of samples flagged as out-of-distribution increasing.

**Implementation:**

`python
# scripts/post_inference_monitoring.py

def monitor_ood_drift(prod_df, threshold=0.20):
    \"\"\"
    Monitor OOD ratio over time.
    \"\"\"
    if 'mahalanobis_distance' not in prod_df.columns:
        return {'ood_ratio': None, 'alert': 'Mahalanobis distance not computed'}
    
    ood_mask = prod_df['mahalanobis_distance'] > 4.0
    ood_ratio = ood_mask.mean()
    
    if ood_ratio > threshold:
        alert = f\"OOD ratio {ood_ratio:.2%} exceeds threshold {threshold:.2%}. Possible domain shift.\"
        return {'ood_ratio': ood_ratio, 'alert': alert}
    
    return {'ood_ratio': ood_ratio, 'alert': None}
`

**Alert Trigger:** OOD ratio >20% (more than 1 in 5 samples flagged as novel)

**What It Means:** Production data is semantically different from training  consider retraining or collecting new data.

---

**Layer 4: Embedding Baseline Drift**

**What to Monitor:** Mean/covariance shift of production embeddings from training baseline.

**Implementation:**

Already implemented in scripts/post_inference_monitoring.py::EmbeddingDriftDetector (lines 956-1050):

`python
# From existing code

class EmbeddingDriftDetector:
    def analyze(self, prod_embeddings):
        \"\"\"
        Detect drift in embedding space.
        \"\"\"
        # Mean shift
        mean_shift = np.linalg.norm(prod_embeddings.mean(axis=0) - self.baseline_mean)
        
        # Covariance shift
        prod_cov = np.cov(prod_embeddings, rowvar=False)
        cov_shift = np.linalg.norm(prod_cov - self.baseline_cov, ord='fro')
        
        # Mahalanobis distance
        mahal_distances = self.compute_mahalanobis(prod_embeddings)
        mean_mahal = np.mean(mahal_distances)
        
        return {
            'mean_shift': mean_shift,
            'cov_shift': cov_shift,
            'mean_mahalanobis': mean_mahal,
            'alert': mean_mahal > 3.0  # Average distance > 3σ
        }
`

**Alert Trigger:** Mean Mahalanobis distance >3.0 (average production sample is >3σ from training)

**What It Means:** Latent representations have shifted  model might be learning different patterns.

---

**Combined Monitoring Dashboard:**

`python
# scripts/post_inference_monitoring.py

def run_full_uncertainty_monitoring(prod_df, baseline_paths):
    \"\"\"
    Run all uncertainty monitoring layers.
    \"\"\"
    results = {
        'timestamp': datetime.now().isoformat(),
        'n_samples': len(prod_df)
    }
    
    # Layer 1: Confidence distribution
    results['confidence_drift'] = monitor_confidence_drift(
        prod_df, 
        baseline_paths['training_confidence_stats']
    )
    
    # Layer 2: Calibration drift
    results['calibration_drift'] = monitor_calibration_drift(
        prod_df,
        baseline_paths['calibration_config']
    )
    
    # Layer 3: OOD ratio
    results['ood_drift'] = monitor_ood_drift(prod_df, threshold=0.20)
    
    # Layer 4: Embedding drift
    prod_embeddings = extract_embeddings(model, prod_df['X'].values)
    results['embedding_drift'] = EmbeddingDriftDetector(
        baseline_paths['baseline_embeddings']
    ).analyze(prod_embeddings)
    
    # Aggregate alerts
    all_alerts = []
    for layer, layer_results in results.items():
        if isinstance(layer_results, dict) and 'alert' in layer_results:
            if layer_results['alert']:
                all_alerts.append(f\"[{layer}] {layer_results['alert']}\")
    
    results['alerts'] = all_alerts
    results['alert_count'] = len(all_alerts)
    
    # Log to MLflow
    with mlflow.start_run(run_name=f\"uncertainty_monitoring_{results['timestamp']}\"):
        mlflow.log_dict(results, 'uncertainty_monitoring_report.json')
        mlflow.log_metric('alert_count', results['alert_count'])
    
    return results
`

---

**CITATIONS (Pair 15, Q2):**

1. **(docs/output_1801_2026-01-18.md, lines 3681-3725, Pair 11):** Baseline data storage strategy: \"Store training statistics (mean, std, quantiles) per channel as baseline_stats.json. Format: JSON dict with keys 'Ax', 'Ay', ..., 'Gz', each containing mean/std/p25/p50/p75. Enables drift detection without storing full training set (lightweight, 5-10 KB vs 500+ MB raw data). KS test/Wasserstein distance compare production windows to these baselines.\"

2. **(scripts/post_inference_monitoring.py, lines 348-370, 956-1050):** Existing monitoring implementation: \"ConfidenceAnalyzer (Layer 1) computes mean/std of confidence, entropy, margin. EmbeddingDriftDetector (Layer 4) loads baseline_embeddings.npz, computes mean/covariance shift. Both log results to MLflow. Demonstrates 4-layer monitoring framework: confidence  temporal  statistical drift  embedding drift.\"

3. **(docs/PIPELINE_DEEP_DIVE_opus.md, lines 313-320, 393-400):** Temperature scaling integration: \"Store temperature T in calibration_config.json, fit once on validation set (500-2000 samples), apply at inference by dividing logits by T before softmax. Store calibration set predictions for ECE computation. Monitor ECE degradation over time  if ECE >0.15 (from <0.05), refit T.\"

4. **(docs/thesis/THESIS_READY_UNLABELED_EVALUATION_PLAN.md, lines 414-435):** Logging strategy for unlabeled evaluation: \"Log per-window: predicted_class, confidence, entropy, margin, temporal_streak. Log per-batch: mean/std of metrics, drift test p-values, uncertain_ratio. Use MLflow for artifact storage (CSVs, histograms, drift reports). Retention: Keep raw predictions 30 days, aggregate reports indefinitely.\"

5. **(docs/output_1801_2026-01-18.md, lines 6143-6200, Pair 13):** Drift detection without labels: \"4-layer monitoring: Layer 1 confidence (mean, entropy, uncertain ratio), Layer 2 temporal (flip rate, dwell time), Layer 3 statistical drift (KS test, Wasserstein, PSI), Layer 4 embedding drift (Mahalanobis distance, cosine similarity). Alert triggers: PSI >0.25, Wasserstein >0.30, mean Mahalanobis >3.0. Log all metrics to MLflow with batch-level histograms.\"

6. **(src/evaluate_predictions.py, lines 554-580):** ECE computation implementation: \"ClassificationEvaluator::compute_calibration() computes Expected Calibration Error using 10 confidence bins. Formula: ECE = Σ |acc_i - conf_i|  (n_i/N). Well-calibrated: ECE <0.05. Poor: ECE >0.15. Used for monitoring calibration drift over time. Requires labels (from audit set or active learning samples).\"

---

**Summary Table (Q2):**

| **Baseline** | **File** | **Content** | **Size** | **Regenerate When** |
|-------------|----------|-------------|----------|---------------------|
| **Temperature config** | calibration_config.json | T parameter, val ECE/NLL | ~1 KB | After model retraining, ECE degradation |
| **Training embeddings** | baseline_embeddings.npz | Mean, covariance, sample embeddings | ~2-3 MB | After model retraining, architecture change |
| **Training confidence stats** | training_confidence_stats.json | Entropy/margin/confidence distributions | ~10 KB | After model retraining, data shift |

| **Logging Level** | **What to Log** | **Where** | **Retention** |
|-------------------|-----------------|-----------|---------------|
| **Per-window** | confidence, entropy, margin, Mahalanobis distance, is_uncertain, is_ood | predictions_{run_id}.csv | 30 days |
| **Per-batch** | Mean/std/percentiles of all metrics, uncertain_ratio, ood_ratio | MLflow metrics | Indefinite |
| **Distributions** | Histograms of confidence/entropy/margin | MLflow artifacts (PNG) | Indefinite |

| **Monitoring Layer** | **Metric** | **Alert Trigger** | **Action** |
|---------------------|------------|-------------------|------------|
| **1: Confidence** | Mean confidence drop, entropy increase | >0.15 drop / >0.50 increase | Investigate distribution shift |
| **2: Calibration** | ECE degradation | ECE >0.15 (from <0.05) | Refit temperature T |
| **3: OOD ratio** | Percentage of high Mahalanobis distance | >20% samples with D_M >4 | Consider retraining or data collection |
| **4: Embedding drift** | Mean Mahalanobis distance | >3.0 average distance | Model representations shifting |

---


